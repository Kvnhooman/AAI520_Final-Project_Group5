{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "350330dd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fcac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencis\n",
    "# Attribution: Geek for Geeks tutorial - https://www.geeksforgeeks.org/artificial-intelligence/introduction-to-langchain/\n",
    "\n",
    "# -- CORE LANGCHAIN AND PLUGINS --\n",
    "!pip install -U langchain langchain-openai langchain-community langchain-google-genai\n",
    "\n",
    "# -- LARGE MODEL PROVIDERS/SKILL ADAPTERS --\n",
    "!pip install -U google-generativeai huggingface_hub openai\n",
    "\n",
    "# -- COMMON TOOLING AND UTILITIES --\n",
    "!pip install -U python-dotenv yfinance duckduckgo-search\n",
    "\n",
    "# -- DUCKDUCKGO SEARCH API WRAPPER --\n",
    "!pip install -U ddgs\n",
    "\n",
    "# -- OPTIONAL: For advanced memory (semantic search/vector db) --\n",
    "!pip install -U faiss-cpu  # For in-memory vector DBs (Lightweight)\n",
    "# If you want persistent/production memory, add chromadb or qdrant-client\n",
    "\n",
    "# -- Install LangGraph for advanced agent orchestration --\n",
    "!pip install -U langgraph\n",
    "\n",
    "# -- (OPTIONAL) For running Python tool actions securely --\n",
    "!pip install -U restrictedpython\n",
    "\n",
    "# Restart the runtime after running this cell if prompted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54eef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import key libraries\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts.chat import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents import create_react_agent, AgentExecutor, initialize_agent, Tool, AgentType\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import LLMChain\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.tools import tool  # newer import for @tool decorator\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367447d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MEMORY AGENT CELL 1\n",
    "# ---- Session-scoped memory ----\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Any, Union\n",
    "import re\n",
    "\n",
    "@dataclass\n",
    "class MemoryItem:\n",
    "    symbol: str\n",
    "    question: str\n",
    "    answer: str\n",
    "    created_at: str\n",
    "    meta: Dict[str, Any]\n",
    "\n",
    "class SessionMemory:\n",
    "    def __init__(self, max_items: int = 200, max_per_symbol: int = 10):\n",
    "        self._store: Dict[str, List[MemoryItem]] = {}\n",
    "        self.max_items = max_items\n",
    "        self.max_per_symbol = max_per_symbol\n",
    "\n",
    "    def remember(self, symbol: str, question: str, answer: str, **meta) -> None:\n",
    "        symbol = (symbol or \"GENERIC\").upper().strip()\n",
    "        item = MemoryItem(\n",
    "            symbol=symbol,\n",
    "            question=(question or \"\").strip(),\n",
    "            answer=(answer or \"\").strip(),\n",
    "            created_at=datetime.utcnow().isoformat(timespec=\"seconds\"),\n",
    "            meta=meta or {}\n",
    "        )\n",
    "        bucket = self._store.setdefault(symbol, [])\n",
    "        bucket.append(item)\n",
    "        if len(bucket) > self.max_per_symbol:\n",
    "            del bucket[0 : len(bucket) - self.max_per_symbol]\n",
    "        self._cap_global()\n",
    "\n",
    "    def recall(self, symbol: str, question: Optional[str] = None) -> Optional[str]:\n",
    "        symbol = (symbol or \"GENERIC\").upper().strip()\n",
    "        bucket = self._store.get(symbol, [])\n",
    "        if not bucket:\n",
    "            return None\n",
    "        if not question:\n",
    "            return bucket[-1].answer\n",
    "        q = (question or \"\").strip()\n",
    "        for item in reversed(bucket):\n",
    "            if item.question == q:\n",
    "                return item.answer\n",
    "        return None\n",
    "\n",
    "    def latest(self, symbol: str) -> Optional[MemoryItem]:\n",
    "        symbol = (symbol or \"GENERIC\").upper().strip()\n",
    "        bucket = self._store.get(symbol, [])\n",
    "        return bucket[-1] if bucket else None\n",
    "\n",
    "    def _cap_global(self):\n",
    "        all_items = []\n",
    "        for sym, bucket in self._store.items():\n",
    "            for it in bucket:\n",
    "                all_items.append((it.created_at, sym, it))\n",
    "        if len(all_items) <= self.max_items:\n",
    "            return\n",
    "        all_items.sort(key=lambda x: x[0])  # oldest first\n",
    "        to_drop = len(all_items) - self.max_items\n",
    "        cutoff = set(id(it) for _, _, it in all_items[:to_drop])\n",
    "        for sym in list(self._store.keys()):\n",
    "            self._store[sym] = [it for it in self._store[sym] if id(it) not in cutoff]\n",
    "\n",
    "SESSION_MEMORY = SessionMemory()\n",
    "\n",
    "def extract_symbol(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Grab a likely ticker from the user_input like 'Analyze the SPY stock ticker'.\n",
    "    Simple heuristic: first ALL-CAPS token 1-5 chars (e.g., AAPL, MSFT, SPY).\n",
    "    Falls back to 'GENERIC' if none found.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"GENERIC\"\n",
    "    candidates = re.findall(r\"\\b[A-Z]{1,5}\\b\", text)\n",
    "    # Light filter for common English words\n",
    "    stop = {\"THE\",\"AND\",\"FOR\",\"WITH\",\"FROM\",\"THIS\",\"THAT\",\"YOUR\",\"HAVE\",\"HOLD\"}\n",
    "    for c in candidates:\n",
    "        if c not in stop:\n",
    "            return c\n",
    "    return \"GENERIC\"\n",
    "\n",
    "def as_text(x: Any) -> str:\n",
    "    \"\"\"\n",
    "    Normalize whatever comes back from planner/tools/evaluator/optimizer into a string.\n",
    "    Works with LangChain AgentExecutor outputs (dict), AIMessage, or raw str.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # AIMessage / ChatMessage\n",
    "        if hasattr(x, \"content\"):\n",
    "            return str(x.content)\n",
    "        # Agent-like dicts\n",
    "        if isinstance(x, dict):\n",
    "            if \"output\" in x and isinstance(x[\"output\"], str):\n",
    "                return x[\"output\"]\n",
    "            if \"messages\" in x and isinstance(x[\"messages\"], list):\n",
    "                return \"\\n\\n\".join(\n",
    "                    (m.content if hasattr(m, \"content\") else str(m))\n",
    "                    for m in x[\"messages\"]\n",
    "                )\n",
    "        # plain string\n",
    "        if isinstance(x, str):\n",
    "            return x\n",
    "        return str(x)\n",
    "    except Exception:\n",
    "        return str(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558a3067",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up LLM API Calls\n",
    "from google.colab import userdata\n",
    "\n",
    "gemini_key = userdata.get('GEMINI')\n",
    "hf_key = userdata.get('HF_TOKEN')\n",
    "openai_key = userdata.get('OPENAI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376ebd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Gemini to use in Agents\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm_gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash-latest\",\n",
    "    google_api_key=gemini_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f957543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# Test if Gemini LLM is reachable\n",
    "try:\n",
    "    response = llm_gemini.invoke(\"Say 'GoogleAPI-success-check'\")\n",
    "    print(\"Gemini API Response:\", response)\n",
    "except Exception as e:\n",
    "    print(\"Error reaching Gemini API:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbeca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm_openai = ChatOpenAI(\n",
    "    model=\"gpt-5-mini\",  # or \"gpt-3.5-turbo\", \"gpt-4-mini\", etc.\n",
    "    openai_api_key=openai_key,  # Your OpenAI API key\n",
    "    temperature=0.0             # (optional) set as needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9518d197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "response = llm_openai.invoke(\"Explain how neural networks work in 10 words or less.\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bc2736",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "!pip install langchain-huggingface huggingface_hub\n",
    "\n",
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# For LangChain HuggingFace integration\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "llm_hf = HuggingFaceEndpoint(\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    huggingfacehub_api_token=hf_key   # Hugging Face API key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6e1c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Test\n",
    "# Test if HF LLM is reachable\n",
    "try:\n",
    "    response = llm_hf.invoke(\"Describe a neural network in 10 words of less'\")\n",
    "    print(\"Gemini API Response:\", response)\n",
    "except Exception as e:\n",
    "    print(\"Error reaching HuggingFace API:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca35bdfd",
   "metadata": {},
   "source": [
    "## Planning Agent\n",
    "Plans research steps for stock analysis\n",
    "\n",
    "0. Evaluate user input, infer that they want to know about a stock, or up to three stocks. They may or may not write the stock symbol.\n",
    "1. Ask, what are the top things to know when analyzing any stock? (Shortcut by hitting top 3 analysis compaines? (Morningstar, Moody, Bloomberg?)\n",
    "2. Research those things for a limited amount of tokens (how do we limit web search API counts? We could somehow hit an API like wikipedia or something similar if it exists)\n",
    "3. Ingest news\n",
    "4. Consider second order effects that might impact this stock or industry, serach news for that\n",
    "5. Evaluate, compare and contrast and then summarize.\n",
    "6. Tell tools agent which tools to use based on plan. Pass instructions of where to look and likely tools to use to Tools Agent. Also plan expected output,\n",
    "- Stock Name\n",
    "- Stock Symbol\n",
    "- Value\n",
    "- Category (growth or stock)\n",
    "- Recommended strategy (buy, hold, sell)\n",
    "- Brief pros and cons, including key organization news as well as indsutry trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6674bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Potential Gemini Approach\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "simple_prompt = PromptTemplate.from_template(\n",
    "    \"You are a planning agent. Give a plan for: {input}\"\n",
    ")\n",
    "chain = LLMChain(prompt=simple_prompt, llm=llm_gemini)\n",
    "\n",
    "response = chain.invoke({\"input\": \"Analyze the SPY stock ticker\"})\n",
    "print(\"Simple LLMChain response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7f5436",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Gemini Approach\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Basic chain-of-thought agent, no true ReAct logic:\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an intelligent planner. For the following task, think step by step and devise a thorough plan:\n",
    "    Task: {input}\n",
    "    \"\"\")\n",
    "chain = prompt | llm_gemini\n",
    "\n",
    "result = chain.invoke({\"input\": \"Analyze the SPY stock ticker\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598464b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReAct Planning agent example from Geek for Geeks\n",
    "# Define planning prompt\n",
    "PLANNER_PROMPT = \"\"\"\n",
    "You are a thoughtful agent that reasons and acts in this format:\n",
    "\n",
    "Thought: you should always think before acting\n",
    "Action: create a step-by-step plan to analyze the stock\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "\n",
    "(Repeat Thought/Action/Action Input/Observation as needed)\n",
    "Thought: I now know the best plan\n",
    "Final Answer: the final plan to provide to tool agent is\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "@tool\n",
    "def echo(text: str) -> str:\n",
    "    \"\"\"Return the input text as the output.\"\"\"\n",
    "    return text\n",
    "\n",
    "tools = [echo]\n",
    "\n",
    "# Create planner agent\n",
    "planner_agent = create_react_agent(\n",
    "    model=llm_openai,\n",
    "    tools=tools,\n",
    "    prompt=PLANNER_PROMPT\n",
    ")\n",
    "\n",
    "\n",
    "# --- Test input for the agent ---\n",
    "\n",
    "#print(\"Sending to agent:\", {\"input\": \"Analyze the SPY stock ticker\"})\n",
    "#print(llm_gemini.invoke(\"test: does this call work?\"))\n",
    "\n",
    "response = planner_agent.invoke({\"input\": \"Analyze the SPY stock ticker\"})\n",
    "\n",
    "# Helper to extract the response text (if needed)\n",
    "try:\n",
    "    output_text = response['output']    # Many agents return {'output': ...}\n",
    "except (TypeError, KeyError):\n",
    "    output_text = str(response)         # Fall back to direct string conversion\n",
    "\n",
    "# You can use a tokenizer for exact token count,\n",
    "# but most practical, use whitespace split for demonstration:\n",
    "words = output_text.split()\n",
    "first_100 = \" \".join(words[:100])\n",
    "\n",
    "print(\"First 100 words/tokens:\\n\")\n",
    "print(first_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99abff5c",
   "metadata": {},
   "source": [
    "## Tool Agent\n",
    "Integrates APIs (Yahoo Finance, SEC EDGAR, News APIs)\n",
    "\n",
    "List of Tools:\n",
    "- Web Search\n",
    "- API call - Yahoo Finance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dbf883",
   "metadata": {},
   "source": [
    "### Tool Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c203af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# Web search tool\n",
    "search = DuckDuckGoSearchRun()\n",
    "search_tool = Tool(\n",
    "    name=\"WebSearch\",\n",
    "    func=search.run,\n",
    "    description=\"Search the web and return relevant results\"\n",
    ")\n",
    "\n",
    "# Yahoo Finance tool\n",
    "import yfinance as yf\n",
    "def get_stock_price(ticker: str) -> str:\n",
    "    stock = yf.Ticker(ticker)\n",
    "    price = stock.info['regularMarketPrice']\n",
    "    return f\"{ticker} price is {price}\"\n",
    "\n",
    "yahoo_tool = Tool(\n",
    "    name=\"YahooFinance\",\n",
    "    func=get_stock_price,\n",
    "    description=\"Get the latest stock price for a given ticker symbol\"\n",
    ")\n",
    "\n",
    "# OTHER APIs & TOOLS HERE IN THE FUTURE\n",
    "\n",
    "# ]Python REPL tool\n",
    "def python_eval(code: str) -> str:\n",
    "    try:\n",
    "        return str(eval(code))\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "python_tool = Tool(\n",
    "    name=\"PythonREPL\",\n",
    "    func=python_eval,\n",
    "    description=\"Run Python code and return the result\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62001d47",
   "metadata": {},
   "source": [
    "### Tools Agent Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a409afab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool execution agent\n",
    "\n",
    "REACT_TOOL_PROMPT = \"\"\"\n",
    "You are an expert analyst and researcher. Using ONLY the tools provided, thoughtfully follow these steps:\n",
    "\n",
    "1. Perform any pre-processing or data gathering needed.\n",
    "2. Use all of the tools to answer or analyze the user's request. Answer qustions like \"# Tool execution agent\n",
    "\n",
    "You are an expert analyst and researcher. Using ONLY the tools provided, thoughtfully follow these steps:\n",
    "\n",
    "1. Perform any pre-processing or data gathering needed.\n",
    "2. Use all of the tools to answer or analyze the user's request and answer the following questions\n",
    "\n",
    "* (“What is the current price of stock as of now?”\n",
    "\n",
    "* “Search for news about stock published in the past 24 hours.”\n",
    "\n",
    "“Calculate the latest annualized yield for stock using Yahoo data.”).\n",
    "3. (If analysis requires multiple steps) Chain tools or aggregate intermediate results.\n",
    "4. Summarize your findings in a concise and useful way for the end user.\n",
    "\n",
    "At each stage, state your Thought, Action, Action Input, and Observation as needed.\n",
    "\n",
    "Reminder: Do NOT make up information—use only direct tool outputs.\n",
    "\n",
    "Final Answer: A succinct, well-organized summary of findings appropriate for the user.\n",
    "Question: {input}\n",
    "{agent_scratchpad}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "tools = [search_tool, yahoo_tool, python_tool]  # Pass tools above to pecialist list here\n",
    "\n",
    "# Create tool\n",
    "\n",
    "tools_executor = create_react_agent(\n",
    "    model=llm_openai,\n",
    "    tools=tools,\n",
    "    prompt=REACT_TOOL_PROMPT\n",
    ")\n",
    "\n",
    "tools = [search_tool, yahoo_tool, python_tool]  # Pass tools above to pecialist list here\n",
    "\n",
    "# Create tool\n",
    "\n",
    "tools_executor = create_react_agent(\n",
    "    model=llm_openai,\n",
    "    tools=tools,\n",
    "    prompt=REACT_TOOL_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee31d9fd",
   "metadata": {},
   "source": [
    "## Self Reflection & Evaluation Agent\n",
    "Evaluates output quality and iterates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e24fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Self Evaluation agent\n",
    "\n",
    "EVAL_PROMPT = \"\"\"\n",
    "You are an expert evaluator. Review the analysis/summary below for:\n",
    "\n",
    "- Completeness (all important steps/points covered)\n",
    "- Succinctness (concise, minimal repetition)\n",
    "- Accuracy (supported by real or tool-sourced information)\n",
    "- Clarity (well organized, easy to follow)\n",
    "\n",
    "Give clear, specific suggestions if any improvement is needed.\n",
    "Return the revised summary/answer if changes are warranted.\n",
    "Otherwise, state that the answer is adequate.\n",
    "\n",
    "--- Analysis To Evaluate ---\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "self_evaluator = create_react_agent(\n",
    "    model=llm_openai, # Or gemini_model if that's your variable\n",
    "    tools = [],\n",
    "    prompt=EVAL_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf3320a",
   "metadata": {},
   "source": [
    "## Optimization Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d2262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer agent\n",
    "\n",
    "OPTIMIZER_PROMPT = \"\"\"\n",
    "You are an optimization agent. Given the evaluator's feedback and the initial summary below,\n",
    "produce a revised version that fixes any weaknesses cited (completeness, succinctness, accuracy, clarity).\n",
    "\n",
    "Evaluator Feedback:\n",
    "{feedback}\n",
    "Initial Answer:\n",
    "{answer}\n",
    "---\n",
    "Optimized Revised Answer:\n",
    "\"\"\"\n",
    "\n",
    "optimizer_agent = create_react_agent(\n",
    "    model=llm_openai, # Or other model variable\n",
    "    tools = [],\n",
    "    prompt=OPTIMIZER_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdff54f",
   "metadata": {},
   "source": [
    "## Learning Agent\n",
    "Maintains memory across analysis runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185adb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Learning Agent CELL 2 ----\n",
    "# Maintains memory within the current Colab/kernel session only.\n",
    "\n",
    "def maybe_answer_from_memory(user_input: str) -> Optional[str]:\n",
    "    symbol = extract_symbol(user_input)\n",
    "    # Try exact question match first\n",
    "    ans = SESSION_MEMORY.recall(symbol, question=user_input)\n",
    "    if ans:\n",
    "        print(f\"[Memory hit] Found exact match for {symbol}.\")\n",
    "        return ans\n",
    "    # Fallback to latest for this symbol\n",
    "    ans = SESSION_MEMORY.recall(symbol)\n",
    "    if ans:\n",
    "        print(f\"[Memory hit] Using latest cached answer for {symbol}.\")\n",
    "        return ans\n",
    "    return None\n",
    "\n",
    "def remember_final_answer(user_input: str, final_answer: str, **meta):\n",
    "    symbol = extract_symbol(user_input)\n",
    "    SESSION_MEMORY.remember(\n",
    "        symbol=symbol,\n",
    "        question=user_input,\n",
    "        answer=final_answer,\n",
    "        **meta\n",
    "    )\n",
    "    print(f\"[Memory write] Saved answer for {symbol}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde21f98",
   "metadata": {},
   "source": [
    "## Multiple Agent Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be58530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATED MAIN WORKFLOW WITH MEMORY AGENT\n",
    "\n",
    "# Example user input\n",
    "user_input = \"Analyze the SPY stock ticker\"\n",
    "\n",
    "# 0) Try to answer directly from session memory\n",
    "_cached = maybe_answer_from_memory(user_input)\n",
    "if _cached:\n",
    "    final_answer = _cached\n",
    "    print(\"\\n=== FINAL ANSWER (from session memory) ===\\n\")\n",
    "    print(final_answer)\n",
    "else:\n",
    "    # 1) Planner\n",
    "    print(\"Planner Agent Reasoning...\")\n",
    "    planner_output = planner_agent.invoke({\"input\": user_input})\n",
    "    print(\"Planner Agent Output:\", as_text(planner_output))\n",
    "    print()\n",
    "\n",
    "    # 2) Tools Executor\n",
    "    print(\"Tools Executor Reasoning...\")\n",
    "    try:\n",
    "        tools_output = tools_executor.invoke(\n",
    "            {\"input\": as_text(planner_output)},\n",
    "            config={\"recursion_limit\": 20}\n",
    "        )\n",
    "        print(\"Tools Executor Output:\", as_text(tools_output))\n",
    "    except Exception as e:\n",
    "        tools_output = f\"[Tools Exception] {type(e).__name__}: {e}\"\n",
    "        print(\"Tools Executor Exception:\", type(e), e)\n",
    "    print()\n",
    "\n",
    "    # 3) Evaluator\n",
    "    print(\"Evaluator Agent Reasoning...\")\n",
    "    evaluation_output = self_evaluator.invoke({\"input\": as_text(tools_output)})\n",
    "    print(\"Evaluator Agent Output:\", as_text(evaluation_output))\n",
    "    print()\n",
    "\n",
    "    # 4) Optimizer\n",
    "    print(\"Optimizer Agent Reasoning...\")\n",
    "    # optimizer prompt expects==>> {feedback, answer}\n",
    "    optimizer_input = {\n",
    "        \"feedback\": as_text(evaluation_output),\n",
    "        \"answer\": as_text(tools_output)\n",
    "    }\n",
    "    optimizer_output = optimizer_agent.invoke(optimizer_input)\n",
    "    print(\"Optimizer Agent Output:\", as_text(optimizer_output))\n",
    "    print()\n",
    "\n",
    "    # 5) Normalize final answer text\n",
    "    final_answer = as_text(optimizer_output)\n",
    "\n",
    "    \n",
    "    def print_optimizer_output_text(text_block: str):\n",
    "        print(\"Raw optimizer_output (normalized):\\n\")\n",
    "        for para in str(text_block).split('\\n\\n'):\n",
    "            print(para)\n",
    "            print()\n",
    "\n",
    "    print_optimizer_output_text(final_answer)\n",
    "\n",
    "    # Remember the final answer in session memory\n",
    "    remember_final_answer(\n",
    "        user_input=user_input,\n",
    "        final_answer=final_answer,\n",
    "        planner_preview=as_text(planner_output)[:300],\n",
    "        tools_preview=as_text(tools_output)[:300]\n",
    "    )\n",
    "\n",
    "    print(\"\\n=/\\=\\/= FINAL ANSWER =\\/=/\\=\\n\")\n",
    "    print(final_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742010c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b468ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
