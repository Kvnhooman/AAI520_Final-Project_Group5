{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kvnhooman/AAI520_Final-Project_Group5/blob/kev-dev/Kevin_copy_Financial_Review_AI_Agents_v_10_7_2200.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z34tfGYWXK7X"
      },
      "source": [
        "# Title\n",
        "**Group 5**\n",
        "\n",
        "Final Project\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pss-SJpmYEdY"
      },
      "source": [
        "___\n",
        "## Outline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au9v6SfkXl5p"
      },
      "source": [
        "**Core Agent Functions**\n",
        "\n",
        "\n",
        "* Tool Use Agent: Integrates APIs (Yahoo Finance, SEC EDGAR, News APIs) and Specialists\n",
        "\n",
        "\n",
        "* Self-Reflection Agent: Evaluates output quality and iterates\n",
        "\n",
        "\n",
        "* Memory/Learning Agent: Maintains memory across analysis runs\n",
        "\n",
        "\n",
        "**Workflow Patterns**\n",
        "* Prompt Chaining: News ingestion → preprocessing → classification → extraction → summarization\n",
        "\n",
        "\n",
        "* Routing: Directs content to specialist analyzers (earnings, news, market)\n",
        "\n",
        "\n",
        "* Evaluator-Optimizer: Generates analysis → evaluates quality → refines using feedback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "p_jPSJnIXKke",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "e0432730-7c36-4686-cf3f-26d8eec2a55b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (0.3.35)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.31)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.12/dist-packages (2.1.12)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.78)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.33)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.3.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Collecting google-ai-generativelanguage<1,>=0.7 (from langchain-google-genai)\n",
            "  Using cached google_ai_generativelanguage-0.7.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: filetype<2,>=1.2 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.25.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.75.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (0.6.1)\n",
            "Using cached google_ai_generativelanguage-0.7.0-py3-none-any.whl (1.4 MB)\n",
            "Installing collected packages: google-ai-generativelanguage\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-ai-generativelanguage-0.7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "9beafa6c06a24d5a8f5af40f71651477"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.35.3)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.3.0)\n",
            "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
            "  Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.25.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.184.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.10)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.75.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Using cached google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
            "Installing collected packages: google-ai-generativelanguage\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.7.0\n",
            "    Uninstalling google-ai-generativelanguage-0.7.0:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-google-genai 2.1.12 requires google-ai-generativelanguage<1,>=0.7, but you have google-ai-generativelanguage 0.6.15 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-ai-generativelanguage-0.6.15\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "eb2636b3047c461da2bdd577eb2291ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.12/dist-packages (0.2.66)\n",
            "Requirement already satisfied: duckduckgo-search in /usr/local/lib/python3.12/dist-packages (8.1.1)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.32.5)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.0.12)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.5.0)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.12/dist-packages (from yfinance) (3.18.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.13.5)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.13.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (5.29.5)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (8.3.0)\n",
            "Requirement already satisfied: primp>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (0.15.0)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (5.4.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.15.0)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (2.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
            "Collecting ddgs\n",
            "  Downloading ddgs-9.6.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from ddgs) (8.3.0)\n",
            "Requirement already satisfied: primp>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from ddgs) (0.15.0)\n",
            "Collecting lxml>=6.0.0 (from ddgs)\n",
            "  Downloading lxml-6.0.2-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.16.0)\n",
            "Requirement already satisfied: brotli in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.1.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.3.0)\n",
            "Collecting socksio==1.* (from httpx[brotli,http2,socks]>=0.28.1->ddgs)\n",
            "  Downloading socksio-1.0.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.15.0)\n",
            "Downloading ddgs-9.6.1-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading socksio-1.0.0-py3-none-any.whl (12 kB)\n",
            "Downloading lxml-6.0.2-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m126.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: socksio, lxml, ddgs\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 5.4.0\n",
            "    Uninstalling lxml-5.4.0:\n",
            "      Successfully uninstalled lxml-5.4.0\n",
            "Successfully installed ddgs-9.6.1 lxml-6.0.2 socksio-1.0.0\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.12.0\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.6.10-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.78)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.2-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.11.10)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.4.33)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.1)\n",
            "Downloading langgraph-0.6.10-py3-none-any.whl (155 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed langgraph-0.6.10 langgraph-checkpoint-2.1.2 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.9 ormsgpack-1.11.0\n",
            "Collecting restrictedpython\n",
            "  Downloading RestrictedPython-8.0-py3-none-any.whl.metadata (14 kB)\n",
            "Downloading RestrictedPython-8.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: restrictedpython\n",
            "Successfully installed restrictedpython-8.0\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (4.13.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.10.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->wikipedia) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->wikipedia) (4.15.0)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=cf319b164d88b467c23d529321b8a35a86bad1ae15461af3b79526e269638116\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/47/7c/a9688349aa74d228ce0a9023229c6c0ac52ca2a40fe87679b8\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "# Install dependencis\n",
        "# Attribution: Geek for Geeks tutorial - https://www.geeksforgeeks.org/artificial-intelligence/introduction-to-langchain/\n",
        "\n",
        "# -- CORE LANGCHAIN AND PLUGINS --\n",
        "!pip install -U langchain langchain-openai langchain-community langchain-google-genai\n",
        "\n",
        "# -- LARGE MODEL PROVIDERS/SKILL ADAPTERS --\n",
        "!pip install -U google-generativeai huggingface_hub openai\n",
        "\n",
        "# -- COMMON TOOLING AND UTILITIES --\n",
        "!pip install -U python-dotenv yfinance duckduckgo-search\n",
        "\n",
        "# -- DUCKDUCKGO SEARCH API WRAPPER --\n",
        "!pip install -U ddgs\n",
        "\n",
        "# -- OPTIONAL: For advanced memory (semantic search/vector db) --\n",
        "!pip install -U faiss-cpu  # For in-memory vector DBs (Lightweight)\n",
        "# If you want persistent/production memory, add chromadb or qdrant-client\n",
        "\n",
        "# -- Install LangGraph for advanced agent orchestration --\n",
        "!pip install -U langgraph\n",
        "\n",
        "# -- (OPTIONAL) For running Python tool actions securely --\n",
        "!pip install -U restrictedpython\n",
        "\n",
        "# -- Wikipedia\n",
        "!pip install wikipedia\n",
        "\n",
        "\n",
        "\n",
        "# Restart the runtime after running this cell if prompted!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers==4.44.2 accelerate==0.34.2\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nqLxCiIP7F8Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "zeCeqrXeXdDA"
      },
      "outputs": [],
      "source": [
        "#Import key libraries\n",
        "import os\n",
        "import requests\n",
        "import re\n",
        "import textwrap\n",
        "import torch\n",
        "import json ,textwrap, requests\n",
        "import csv\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain_community.chat_models import ChatPerplexity\n",
        "\n",
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts.chat import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.agents import  AgentExecutor, initialize_agent, Tool, AgentType, create_react_agent\n",
        "#from langchain_core.tools import tool\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.tools import tool  #Newer import for @tool decorator\n",
        "from langchain.llms import OpenAI\n",
        "from wikipedia import summary\n",
        "\n",
        "#Correct import path for YahooFinanceAPI\n",
        "import yfinance as yf\n",
        "from langchain_community.tools.yahoo_finance_news import YahooFinanceNewsTool\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "from transformers import pipeline,AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "#Memory\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Any, Union"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HKJlF0P5EvHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "SVdqL2i_ZTB0"
      },
      "outputs": [],
      "source": [
        "#Set up LLM API Calls\n",
        "\n",
        "gemini_key = userdata.get('GEMINI')\n",
        "hf_key = userdata.get('HF_TOKEN')\n",
        "openai_key = userdata.get('OPENAI')\n",
        "fin_news = userdata.get('FIN_API_KEY')\n",
        "#tavily_key = userdata.get('TAVILY_API_KEY')\n",
        "perplexity_key = userdata.get('PERPLEXITY_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7wXVW8qf2v4M"
      },
      "outputs": [],
      "source": [
        "#gemini_key = os.getenv('GEMINI_API')\n",
        "#openai_key = os.getenv('OPENAI_API')\n",
        "#perplexity_key = os.getenv(\"PERPLEXITY_API\")\n",
        "#fin_news = os.getenv(\"NEWS_API\")\n",
        "#tavily_key = os.getenv('TAVILY_API')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7cDESRfd2v4M"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jlClLEzOAxjy"
      },
      "outputs": [],
      "source": [
        "# Setup Gemini to use in Agents\n",
        "llm_gemini = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    google_api_key=gemini_key\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "YPlYol3q2v4N",
        "outputId": "ce088a9c-f370-486c-e5f2-a2e3d0ba6e92"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'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'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "openai_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "m95A2uEwHt4O"
      },
      "outputs": [],
      "source": [
        "llm_openai = ChatOpenAI(\n",
        "    model=\"gpt-5-mini\",  # \"gpt-3.5-turbo\", \"gpt-4-mini\", \"gpt-5-mini\" ...\n",
        "    openai_api_key=openai_key,  # Your OpenAI API key\n",
        "    temperature=0.0             # (optional) set as needed\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gKbuO_UUH3hO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffd4b5b4-fa55-4319-845b-8d7f20b3c8cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-786221510.py:1: LangChainDeprecationWarning: The class `ChatPerplexity` was deprecated in LangChain 0.3.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-perplexity package and should be used instead. To use it run `pip install -U :class:`~langchain-perplexity` and import as `from :class:`~langchain_perplexity import ChatPerplexity``.\n",
            "  llm_perplexity = ChatPerplexity(\n"
          ]
        }
      ],
      "source": [
        "llm_perplexity = ChatPerplexity(\n",
        "    model=\"sonar-pro\", #Other model options\n",
        "    pplx_api_key = perplexity_key,\n",
        "    temperature=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830
        },
        "collapsed": true,
        "id": "Pd9mMhbYLg6e",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "c01f7b7c-3274-450e-edfc-6ae5a7d99066"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-perplexity\n",
            "  Downloading langchain_perplexity-0.1.2-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.71 in /usr/local/lib/python3.12/dist-packages (from langchain-perplexity) (0.3.78)\n",
            "Collecting openai<2.0.0,>=1.97.1 (from langchain-perplexity)\n",
            "  Downloading openai-1.109.1-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (0.4.33)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (1.33)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (2.11.10)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.97.1->langchain-perplexity) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.97.1->langchain-perplexity) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.97.1->langchain-perplexity) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.97.1->langchain-perplexity) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.97.1->langchain-perplexity) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.97.1->langchain-perplexity) (4.67.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.97.1->langchain-perplexity) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.97.1->langchain-perplexity) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.97.1->langchain-perplexity) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.97.1->langchain-perplexity) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (2.5.0)\n",
            "Downloading langchain_perplexity-0.1.2-py3-none-any.whl (8.7 kB)\n",
            "Downloading openai-1.109.1-py3-none-any.whl (948 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai, langchain-perplexity\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 2.3.0\n",
            "    Uninstalling openai-2.3.0:\n",
            "      Successfully uninstalled openai-2.3.0\n",
            "Successfully installed langchain-perplexity-0.1.2 openai-1.109.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "openai"
                ]
              },
              "id": "576987a4167a4dcf91cdec755fcf7232"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -U langchain-perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu6ALnCIHxr9",
        "outputId": "b17df5e3-2a49-43d5-cc84-0a9086242458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural networks learn patterns from data to make predictions.\n",
            "Learn patterns from data to make predictions.\n",
            "Neural networks process data through interconnected nodes, adjusting weights to learn[4][5][12].\n"
          ]
        }
      ],
      "source": [
        "#Test OpenAI API\n",
        "response1 = llm_openai.invoke(\"Explain how neural networks work in 10 words or less.\")\n",
        "response2 = llm_gemini.invoke(\"Explain how neural networks work in 10 words or less.\")\n",
        "response3 = llm_perplexity.invoke(\"Explain how neural networks work in 10 words or less.\")\n",
        "print(response1.content)\n",
        "print(response2.content)\n",
        "print(response3.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iVDzqg0gaPz"
      },
      "source": [
        "## User Input Context Extractor Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XlJHg7QSJIhS"
      },
      "outputs": [],
      "source": [
        "# Simply outlining this here, we can build an ingestion agent or function if we have time\n",
        "user_input = \"\"\"\n",
        "    \"input\": (str) Analyze the Apple stock.\n",
        "    \"symbol\": (str) AAPL\n",
        "    \"company_name\": (str) Apple\n",
        "    \"date\": (str) 2025/10/07\n",
        "    \"agent_scratchpad\": (str) \"\",\n",
        "    \"research_notes\": (str) \"\"\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### -->> getting company name\n",
        "def get_company_name(ticker: str) -> str:\n",
        "    \"\"\"Return a best-guess company name for a ticker using yfinance, with safe fallbacks.\"\"\"\n",
        "    try:\n",
        "        info = yf.Ticker(ticker).info\n",
        "        return info.get(\"longName\") or info.get(\"shortName\") or ticker\n",
        "    except Exception:\n",
        "        return ticker  # fallback to the ticker itself"
      ],
      "metadata": {
        "id": "UiEacyjg8EIb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AfRwYIsOghzz"
      },
      "outputs": [],
      "source": [
        "#Function on input\n",
        "def extract_context(user_input):\n",
        "    \"\"\"\n",
        "    Extracts key context fields from user input for agent API use.\n",
        "\n",
        "    Fields:\n",
        "      - input (str): raw user text\n",
        "      - symbol (str): detected uppercase ticker (2-5 chars)\n",
        "      - company_name (str): placeholder for future NLP extraction\n",
        "      - date (str): today's date in ISO format (YYYY-MM-DD)\n",
        "      - agent_scratchpad (str): empty, reserved for intermediate agent notes\n",
        "      - research_notes (str): empty, reserved for additional notes\n",
        "    \"\"\"\n",
        "    # Extract uppercase ticker symbol (2-5 chars)\n",
        "    match = re.search(r\"\\b([A-Z]{2,5})\\b\", user_input)\n",
        "    symbol = match.group(1) if match else \"\"\n",
        "\n",
        "    # Fill company_name if we detected a ticker above\n",
        "    company_name = get_company_name(symbol) if symbol else \"\"\n",
        "\n",
        "    # Today's date in ISO format\n",
        "    today = datetime.now().date().isoformat()\n",
        "\n",
        "    # Build context dictionary\n",
        "    return {\n",
        "        'input': user_input,\n",
        "        'symbol': symbol,\n",
        "        'company_name': company_name,\n",
        "        'date': today,\n",
        "        'agent_scratchpad': '',\n",
        "        'research_notes': ''\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJQEJBhC3M3E",
        "outputId": "9f89aba4-50e6-4171-ff88-194b95b1846e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': '\\n    \"input\": (str) Analyze the Apple stock.\\n    \"symbol\": (str) AAPL\\n    \"company_name\": (str) Apple\\n    \"date\": (str) 2025/10/07\\n    \"agent_scratchpad\": (str) \"\",\\n    \"research_notes\": (str) \"\"\\n    ', 'symbol': 'AAPL', 'company_name': 'Apple Inc.', 'date': '2025-10-14', 'agent_scratchpad': '', 'research_notes': ''}\n"
          ]
        }
      ],
      "source": [
        "#Input extraction output\n",
        "context = extract_context(user_input)\n",
        "print(context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7IzZtQ0i_p0"
      },
      "source": [
        "## Tool Agent\n",
        "Integrates APIs (Yahoo Finance, SEC EDGAR, News APIs)\n",
        "\n",
        "List of Tools:\n",
        "- Web Search\n",
        "- API call - Yahoo Finance\n",
        "- to be updated..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz8YEnt9jfua"
      },
      "source": [
        "### Tool Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFCPTCNfkIit",
        "outputId": "e94cfd97-45c6-4797-b075-0e8f6444ec02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        }
      ],
      "source": [
        "#Yahoo Finance Tool\n",
        "def get_stock_price(ticker: str) -> str:\n",
        "    try:\n",
        "        price = yf.Ticker(ticker).info['regularMarketPrice']\n",
        "        return f\"{ticker} price is {price}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching price for {ticker}: {e}\"\n",
        "\n",
        "\n",
        "yahoo_api_tool = Tool(\n",
        "    name=\"YahooFinanceAPI\",\n",
        "    func=get_stock_price,\n",
        "    description=\"Queries Yahoo Finance for stock price and financials\"\n",
        ")\n",
        "\n",
        "\n",
        "#SEC Filings\n",
        "def get_sec_filings(ticker: str) -> str:\n",
        "    cik_url = \"https://www.sec.gov/files/company_tickers.json\"\n",
        "    headers = {\"User-Agent\": \"tpool@sandiego.edu\"}  # Use your real email!\n",
        "    cik_resp = requests.get(cik_url, headers=headers)\n",
        "    print(f\"cik_resp: {cik_resp.status_code}, {cik_resp.text[:100]}\")\n",
        "\n",
        "\n",
        "    if cik_resp.status_code != 200:\n",
        "        return f\"SEC.gov rejected our request: {cik_resp.status_code}\\n{cik_resp.text[:200]}\"\n",
        "\n",
        "\n",
        "    try:\n",
        "        cik_data = cik_resp.json()\n",
        "        cik_lookup = {item['ticker']: item['cik_str'] for item in cik_data.values()}\n",
        "        cik = cik_lookup.get(ticker.upper())\n",
        "    except Exception as e:\n",
        "        return f\"Error parsing CIK data: {e}\"\n",
        "\n",
        "\n",
        "    if not cik:\n",
        "        return f\"CIK for {ticker} not found.\"\n",
        "\n",
        "\n",
        "    filings_url = f\"https://data.sec.gov/submissions/CIK{cik:0>10}.json\"\n",
        "    filings_resp = requests.get(filings_url, headers=headers)\n",
        "\n",
        "\n",
        "    try:\n",
        "        data = filings_resp.json() if filings_resp.status_code == 200 else {}\n",
        "        filings = data.get('filings', {}).get('recent', {})\n",
        "        if filings:\n",
        "            forms = filings.get('form', [])[:3]\n",
        "            filing_dates = filings.get('filingDate', [])[:3] #If successful, get filing dates\n",
        "            filing_links = filings.get('primaryDocument', [])[:3] #Also get three primary documents for agent\n",
        "            result = []\n",
        "            for f, d, l in zip(forms, filing_dates, filing_links):\n",
        "                result.append(f\"{f} on {d}: {l}\")\n",
        "            return f\"Latest filings for {ticker}:\\n\" + \"\\n\".join(result)\n",
        "        else:\n",
        "            return f\"No filings found for {ticker}.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error loading filings for {ticker}: {e}\"\n",
        "\n",
        "\n",
        "\n",
        "sec_api_tool = Tool(\n",
        "    name=\"SECEDGARAPI\",\n",
        "    func=get_sec_filings,\n",
        "    description=\"Retrieves SEC filings on stock symbol\"\n",
        ")\n",
        "\n",
        "#Finanical news lookup\n",
        "def get_fin_news(symbol: str) -> str:\n",
        "    api_key = fin_news\n",
        "    url = \"https://newsapi.org/v2/everything\"\n",
        "    params = {\n",
        "        \"q\": symbol,\n",
        "        \"apiKey\": api_key,\n",
        "        \"sortBy\": \"publishedAt\",\n",
        "        \"language\": \"en\"\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    if response.status_code != 200:\n",
        "        return f\"API error {response.status_code}: {response.text[:200]}\"\n",
        "    data = response.json()\n",
        "    articles = data.get(\"articles\", [])\n",
        "    if not articles:\n",
        "        return f\"No news found for {symbol}. Full message: {data.get('message', '')}\"\n",
        "    return \"\\n\".join([a[\"description\"] or a[\"title\"] for a in articles[:3]])\n",
        "\n",
        "\n",
        "\n",
        "news_api_tool = Tool(\n",
        "    name=\"NewsAPI\",\n",
        "    func=get_fin_news,  #Assumes you've defined this class\n",
        "    description=\"Finds recent financial news on stock symbol\"\n",
        ")\n",
        "\n",
        "#Tavily for websearching\n",
        "def get_fin_news_tavily(symbol: str) -> str:\n",
        "    api_key = tavily_key\n",
        "    url = \"https://api.tavily.com/search\"\n",
        "    # You can optimize the query for financial news by including keywords:\n",
        "    query = f\"{symbol} financial news\"\n",
        "    payload = {\n",
        "        \"query\": query,\n",
        "        \"api_key\": api_key,\n",
        "        \"max_results\": 3,  # You can set number of results as you prefer\n",
        "    }\n",
        "    response = requests.post(url, json=payload)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        results = data.get(\"results\", [])\n",
        "        # Each result contains title, link, snippet, etc.\n",
        "        return \"\\n\".join([r.get(\"title\", \"No title\") for r in results]) if results else \"No news found.\"\n",
        "    else:\n",
        "        return f\"Error from Tavily: {response.status_code} {response.text}\"\n",
        "\n",
        "\n",
        "tavily_news_tool = Tool(\n",
        "    name=\"TavilyNewsSearch\",\n",
        "    func=get_fin_news_tavily,\n",
        "    description=\"Searches web for recent financial news about a stock symbol using Tavily API.\"\n",
        ")\n",
        "\n",
        "\n",
        "#Wikipedia Search Tool\n",
        "\n",
        "\n",
        "\n",
        "def search_wikipedia(query: str) -> str:\n",
        "    # If query looks like a ticker (e.g., all caps, 2-5 chars), get company name\n",
        "    # if query.isupper() and 2 <= len(query) <= 5:\n",
        "    #     company_name = get_company_name(query)\n",
        "    #     search_term = f\"{company_name} stock\"\n",
        "    # else:\n",
        "    search_term = query\n",
        "\n",
        "\n",
        "    try:\n",
        "        return summary(search_term, sentences=2)\n",
        "    except Exception:\n",
        "        # Fallback, try just company name (without \"stock\")\n",
        "        if search_term != query:\n",
        "            try:\n",
        "                return summary(company_name, sentences=2)\n",
        "            except Exception:\n",
        "                pass\n",
        "        return \"I couldn't find any information on that.\"\n",
        "\n",
        "\n",
        "wikipedia_tool = Tool(\n",
        "    name=\"WikipediaSearch\",\n",
        "    func=search_wikipedia,\n",
        "    description=\"Searches Wikipedia and returns a summary for a stock symbol or company name.\"\n",
        ")\n",
        "\n",
        "\n",
        "#Sentiment analysis tool\n",
        "class HuggingFaceSentimentTool:\n",
        "    def __init__(self):\n",
        "        self.classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "\n",
        "    def analyze(self, text):\n",
        "        result = self.classifier(text)[0]\n",
        "        # result['label'] is 'POSITIVE' or 'NEGATIVE'\n",
        "        return 1 if result['label'] == \"POSITIVE\" else -1\n",
        "\n",
        "\n",
        "sentiment_api_tool = Tool (\n",
        "    name=\"HuggingFaceSentiment\",\n",
        "    func=HuggingFaceSentimentTool().analyze,\n",
        "    description=\"Analyzes sentiment of text using HuggingFace\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "#Analyzer specialist tool\n",
        "class EarningsAnalyzer:\n",
        "    def __init__(self, yahoo_tool, sec_tool):\n",
        "        self.yahoo_tool = yahoo_tool\n",
        "        self.sec_tool = sec_tool\n",
        "\n",
        "\n",
        "    def analyze(self, symbol, messages=None):\n",
        "        #Try to find earnings info in past messages\n",
        "        yahoo_data = None\n",
        "        if messages:\n",
        "            for msg in reversed(messages):\n",
        "                # ToolMessage expected structure\n",
        "                # Check by tool name and relevant symbol\n",
        "                if (\n",
        "                    hasattr(msg, \"name\") and msg.name == \"YahooFinanceAPI\"\n",
        "                    and hasattr(msg, \"content\") and symbol in msg.content\n",
        "                ):\n",
        "                    yahoo_data = msg.content  # or parse accordingly\n",
        "                    break\n",
        "        #If not found, call API\n",
        "        if yahoo_data is None:\n",
        "            yahoo_data = self.yahoo_tool.func(symbol)\n",
        "\n",
        "        return yahoo_data\n",
        "\n",
        "earnings_specialist = Tool (\n",
        "    name=\"EarningsAnalyzer\",\n",
        "    func=EarningsAnalyzer(yahoo_api_tool, sec_api_tool).analyze,\n",
        "    description=\"Analyzes earnings of a stock symbol\"\n",
        ")\n",
        "\n",
        "\n",
        "class SentimentAnalyzer:\n",
        "    def __init__(self, sentiment_tool, news_tool):\n",
        "        self.sentiment_tool = sentiment_tool  # expects .analyze(text)\n",
        "        self.news_tool = news_tool            # expects .get_news(symbol)\n",
        "\n",
        "\n",
        "    def analyze(self, symbol, messages=None):\n",
        "        #Find any recent news/sentiment in agent messages\n",
        "        headlines = []\n",
        "        if messages:\n",
        "            for msg in reversed(messages):\n",
        "                # If you have ToolMessage for news and symbol in content\n",
        "                if hasattr(msg, \"name\") and msg.name in [\"NewsAPI\", \"TavilyNewsSearch\"]:\n",
        "                    if hasattr(msg, \"content\") and symbol in msg.content:\n",
        "                        # Try splitting headlines (change this parsing as needed)\n",
        "                        headlines += msg.content.split(\"\\n\")\n",
        "                # If you cache sentiment scores/results, you can parse those too!\n",
        "\n",
        "\n",
        "        #If no headlines found, call news API/tool\n",
        "        if not headlines:\n",
        "            news_items = self.news_tool.get_news(symbol)\n",
        "            headlines = [item['headline'] for item in news_items]\n",
        "\n",
        "\n",
        "        #Get sentiment scores for each headline\n",
        "        if headlines:\n",
        "            scores = [self.sentiment_tool.analyze(h) for h in headlines]\n",
        "            if scores:\n",
        "                mean_score = sum(scores) / len(scores)\n",
        "                recommendation = self._interpret_score(mean_score)\n",
        "                return (f\"Average news sentiment for {symbol}: {mean_score:.2f}\\n\"\n",
        "                        f\"Recommendation: {recommendation}\")\n",
        "            else:\n",
        "                return \"No sentiment scores could be computed.\"\n",
        "        else:\n",
        "            return \"No recent news headlines found.\"\n",
        "\n",
        "\n",
        "    def _interpret_score(self, score):\n",
        "        #Customize thresholds to your model's scoring scale\n",
        "        if score > 0.2:\n",
        "            return \"Buy\"\n",
        "        elif score > -0.2:\n",
        "            return \"Hold\"\n",
        "        else:\n",
        "            return \"Sell\"\n",
        "\n",
        "\n",
        "sentiment_anaylsis_specialist = Tool(\n",
        "    name=\"SentimentAnalyzer\",\n",
        "    func=lambda symbol, messages: SentimentAnalyzer(sentiment_api_tool, news_api_tool).analyze(symbol, messages=messages),\n",
        "    description=\"Aggregates news sentiment for a stock and suggests buy/hold/sell.\"\n",
        ")\n",
        "\n",
        "\n",
        "#Perplexity search tool\n",
        "def perplexity_search(query: str) -> str:\n",
        "    api_key = perplexity_key\n",
        "    url = \"https://api.perplexity.ai/v1/search\"\n",
        "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "    params = {\"q\": query, \"num_results\": 3}\n",
        "    response = requests.get(url, headers=headers, params=params)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        return \"\\n\".join([item.get(\"snippet\", \"No snippet\") for item in data.get(\"results\", [])])\n",
        "    else:\n",
        "        return f\"Error from Perplexity: {response.status_code} {response.text}\"\n",
        "\n",
        "perplexity_search_tool = Tool(\n",
        "    name=\"PerplexitySearch\",\n",
        "    func=perplexity_search,\n",
        "    description=\"Searches the web using Perplexity AI engine.\"\n",
        ")\n",
        "\n",
        "#FRED Scrape\n",
        "def get_fred_series_web(series_id: str) -> str:\n",
        "    url = f\"https://fred.stlouisfed.org/series/{series_id}\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code != 200:\n",
        "            return f\"Failed to load FRED page [{response.status_code}]\"\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        # Try to extract the latest value from the series chart metadata\n",
        "        latest = soup.select_one(\".series-meta-observation-value\")\n",
        "        date = soup.select_one(\".series-meta-observation-date\")\n",
        "        if latest and date:\n",
        "            latest_val = latest.text.strip()\n",
        "            latest_date = date.text.strip()\n",
        "            return f\"{series_id} latest value: {latest_val} ({latest_date})\"\n",
        "        # Fallback: Try alternative selectors or extraction\n",
        "        # For some series, the value is in an element with class=\"meta-value\"\n",
        "        alt_latest = soup.find(\"span\", class_=\"meta-value\")\n",
        "        if alt_latest:\n",
        "            return f\"{series_id} recent value: {alt_latest.text.strip()}\"\n",
        "        return f\"Could not find observation value for series '{series_id}' on FRED site.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching FRED data: {e}\"\n",
        "\n",
        "fred_api_tool = Tool(\n",
        "    name=\"FREDScrapeTool\",\n",
        "    func=get_fred_series_web,\n",
        "    description=\"Scrapes the open FRED site for latest data value of a given series (e.g., GDP, CPI, etc.)\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Company name -->> Ticker"
      ],
      "metadata": {
        "id": "Xv9CAZ7AGTPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Company/name/sentence -> TICKER (no hardcoding, no yfinance validation) ---\n",
        "import requests, re, difflib\n",
        "\n",
        "TICKER_RE = re.compile(r\"^[A-Z]{1,5}(\\.[A-Z]{1,2})?$\")\n",
        "STOPWORDS = {\n",
        "    \"analyze\",\"analysis\",\"check\",\"stock\",\"within\",\"last\",\"two\",\"2\",\"weeks\",\"week\",\n",
        "    \"for\",\"the\",\"a\",\"of\",\"and\",\"please\",\"do\",\"on\",\"about\",\"company\",\"ticker\",\"in\",\"to\"\n",
        "}\n",
        "\n",
        "def _clean_text(s: str) -> str:\n",
        "    s = re.sub(r\"[^A-Za-z0-9\\s\\.\\-&]\", \" \", s)\n",
        "    toks = [t for t in s.split() if t.lower() not in STOPWORDS]\n",
        "    return \" \".join(toks).strip()\n",
        "\n",
        "def _prefer_equities(quotes):\n",
        "    \"\"\"Pick a common stock; prefer US exchanges if possible.\"\"\"\n",
        "    preferred_exchanges = {\"NMS\",\"NYQ\",\"NCM\"}  # Nasdaq/NYSE/NYSE MKT\n",
        "    # 1) US common equities on major exchanges\n",
        "    for q in quotes:\n",
        "        sym = q.get(\"symbol\"); qt = (q.get(\"quoteType\") or \"\").upper(); ex = (q.get(\"exchange\") or \"\").upper()\n",
        "        if sym and TICKER_RE.match(sym) and qt in {\"EQUITY\",\"COMMONSTOCK\"} and (ex in preferred_exchanges):\n",
        "            return sym.upper()\n",
        "    # 2) any common equity\n",
        "    for q in quotes:\n",
        "        sym = q.get(\"symbol\"); qt = (q.get(\"quoteType\") or \"\").upper()\n",
        "        if sym and TICKER_RE.match(sym) and qt in {\"EQUITY\",\"COMMONSTOCK\"}:\n",
        "            return sym.upper()\n",
        "    # 3) first symbol-ish\n",
        "    if quotes and quotes[0].get(\"symbol\") and TICKER_RE.match(quotes[0][\"symbol\"]):\n",
        "        return quotes[0][\"symbol\"].upper()\n",
        "    return None\n",
        "\n",
        "def _yahoo_autocomplete(query: str) -> str | None:\n",
        "    url = \"https://query1.finance.yahoo.com/v1/finance/search\"\n",
        "    params = {\"q\": query, \"quotesCount\": 10, \"newsCount\": 0, \"lang\": \"en-US\", \"region\": \"US\"}\n",
        "    r = requests.get(url, params=params, timeout=10)\n",
        "    r.raise_for_status()\n",
        "    return _prefer_equities(r.json().get(\"quotes\", []) or [])\n",
        "\n",
        "def _sec_fuzzy_match(name: str) -> str | None:\n",
        "    \"\"\"Fetch SEC company list and fuzzy-match by title (no key required).\"\"\"\n",
        "    headers = {\"User-Agent\": \"your.email@example.com\"}  # polite header\n",
        "    resp = requests.get(\"https://www.sec.gov/files/company_tickers.json\", headers=headers, timeout=20)\n",
        "    resp.raise_for_status()\n",
        "    data = resp.json()\n",
        "    rows = [(v.get(\"ticker\",\"\").upper(), (v.get(\"title\",\"\") or \"\").upper()) for v in data.values() if v.get(\"ticker\")]\n",
        "    titles = [t for _, t in rows]\n",
        "    # exact or startswith\n",
        "    name_up = name.upper()\n",
        "    for sym, title in rows:\n",
        "        if title == name_up or title.startswith(name_up):\n",
        "            return sym\n",
        "    # fuzzy\n",
        "    best = difflib.get_close_matches(name_up, titles, n=3, cutoff=0.65)\n",
        "    for t in best:\n",
        "        for sym, title in rows:\n",
        "            if title == t:\n",
        "                return sym\n",
        "    return None\n",
        "\n",
        "def _wiki_guess(name: str) -> str | None:\n",
        "    \"\"\"Try Wikipedia summary and extract patterns like (NASDAQ: NVDA) or ticker in infobox.\"\"\"\n",
        "    try:\n",
        "        import wikipedia\n",
        "        txt = wikipedia.summary(name, sentences=2, auto_suggest=True, redirect=True)\n",
        "        # patterns like: NASDAQ: NVDA / NYSE: IBM / ticker: NVDA\n",
        "        m = re.search(r\"(NASDAQ|NYSE|AMEX)\\s*:\\s*([A-Z]{1,5}(?:\\.[A-Z]{1,2})?)\", txt, re.I)\n",
        "        if m: return m.group(2).upper()\n",
        "        m = re.search(r\"\\bticker\\s*[:\\-]\\s*([A-Z]{1,5}(?:\\.[A-Z]{1,2})?)\\b\", txt)\n",
        "        if m: return m.group(1).upper()\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "def resolve_ticker(user_text: str) -> str:\n",
        "    txt = user_text.strip()\n",
        "\n",
        "    # 1) exact ticker?\n",
        "    cand = txt.upper()\n",
        "    if TICKER_RE.match(cand):\n",
        "        return cand\n",
        "\n",
        "    # 2) ticker embedded in sentence?\n",
        "    m = re.search(r\"\\b([A-Z]{1,5})(?:\\.[A-Z]{1,2})?\\b\", txt)\n",
        "    if m and TICKER_RE.match(m.group(0)):\n",
        "        return m.group(0).upper()\n",
        "\n",
        "    # 3) Yahoo autocomplete on cleaned text\n",
        "    cleaned = _clean_text(txt)\n",
        "    if cleaned:\n",
        "        try:\n",
        "            sym = _yahoo_autocomplete(cleaned)\n",
        "            if sym: return sym\n",
        "        except Exception:\n",
        "            pass\n",
        "        # also try first 1–3 tokens (helps with long requests)\n",
        "        toks = cleaned.split()\n",
        "        for span in (1, 2, 3):\n",
        "            if len(toks) >= span:\n",
        "                q = \" \".join(toks[:span])\n",
        "                try:\n",
        "                    sym = _yahoo_autocomplete(q)\n",
        "                    if sym: return sym\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "    # 4) SEC fuzzy fallback\n",
        "    if cleaned:\n",
        "        sym = _sec_fuzzy_match(cleaned)\n",
        "        if sym: return sym\n",
        "\n",
        "    # 5) Wikipedia parse as last resort\n",
        "    if cleaned:\n",
        "        sym = _wiki_guess(cleaned)\n",
        "        if sym: return sym\n",
        "\n",
        "    # 6) no match\n",
        "    return \"\"\n"
      ],
      "metadata": {
        "id": "msHaylyBGSu9"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SQLite Memory for consistent memory between sessions\n"
      ],
      "metadata": {
        "id": "8753L8yEHw3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  SQLite memory\n",
        "import sqlite3\n",
        "\n",
        "MEM_DB = \"memory.db\"\n",
        "\n",
        "def _init_mem():\n",
        "    with sqlite3.connect(MEM_DB) as con:\n",
        "        c = con.cursor()\n",
        "        c.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS notes (\n",
        "          id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "          ticker TEXT,\n",
        "          note TEXT,\n",
        "          created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
        "        )\"\"\")\n",
        "        con.commit()\n",
        "_init_mem()\n",
        "\n",
        "def mem_get_notes(ticker: str, k: int = 5) -> list[str]:\n",
        "    with sqlite3.connect(MEM_DB) as con:\n",
        "        c = con.cursor()\n",
        "        c.execute(\"SELECT note FROM notes WHERE ticker=? ORDER BY id DESC LIMIT ?\", (ticker.upper(), k))\n",
        "        return [row[0] for row in c.fetchall()]\n",
        "\n",
        "def mem_remember(ticker: str, note: str):\n",
        "    with sqlite3.connect(MEM_DB) as con:\n",
        "        c = con.cursor()\n",
        "        c.execute(\"INSERT INTO notes (ticker, note) VALUES (?, ?)\", (ticker.upper(), note))\n",
        "        con.commit()\n",
        "\n",
        "# Hook the planner to SQLite-based memory\n",
        "def _get_recent_notes_for_planner(ticker: str) -> str:\n",
        "    notes = mem_get_notes(ticker, k=5)\n",
        "    return \"\\n\".join(notes) if notes else \"\"\n",
        "\n",
        "def remember_takeaway(ticker: str, brief: str):\n",
        "    import re\n",
        "    m = re.search(r\"Takeaway\\s*:\\s*(.*)\", brief)\n",
        "    note = (m.group(1) if m else brief)[:240]\n",
        "    mem_remember(ticker, note)\n"
      ],
      "metadata": {
        "id": "ZIj5X2n4H3bD"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Planner Agent"
      ],
      "metadata": {
        "id": "oGA9kwA69aJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  PLANNER (HF FLAN-T5)\n",
        "# Purpose: Given a ticker, produce a compact JSON \"Plan\".\n",
        "\n",
        "# ---------- helper: ticker -> company name ----------\n",
        "def get_company_name(ticker: str) -> str:\n",
        "    \"\"\"Best-effort company name lookup using yfinance; safe fallback to the ticker.\"\"\"\n",
        "    try:\n",
        "        info = yf.Ticker(ticker).info\n",
        "        return info.get(\"longName\") or info.get(\"shortName\") or ticker\n",
        "    except Exception:\n",
        "        return ticker\n",
        "\n",
        "# ---------- model init (robust across transformers versions) ----------\n",
        "PLANNER_MODEL_ID = \"google/flan-t5-base\"   # if RAM is tight, try: \"google/flan-t5-small\"\n",
        "planner_tok = AutoTokenizer.from_pretrained(PLANNER_MODEL_ID)\n",
        "planner_mod = AutoModelForSeq2SeqLM.from_pretrained(PLANNER_MODEL_ID)\n",
        "\n",
        "_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "planner_mod.to(_device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def _t5_greedy_decode_no_generate(model, tokenizer, prompt: str, max_new_tokens: int = 256) -> str:\n",
        "    \"\"\"\n",
        "    Manual greedy decode for T5 when .generate() isn't available (version edge cases).\n",
        "    \"\"\"\n",
        "    enc = tokenizer(prompt, return_tensors=\"pt\").to(_device)\n",
        "\n",
        "    # Encode once\n",
        "    enc_out = model.get_encoder()(input_ids=enc[\"input_ids\"],\n",
        "                                  attention_mask=enc.get(\"attention_mask\"),\n",
        "                                  return_dict=True)\n",
        "\n",
        "    # Start with decoder_start_token_id then grow one token at a time\n",
        "    start_id = model.config.decoder_start_token_id\n",
        "    eos_id = model.config.eos_token_id\n",
        "    cur = torch.tensor([[start_id]], device=_device, dtype=torch.long)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        out = model(encoder_outputs=enc_out, decoder_input_ids=cur, return_dict=True)\n",
        "        next_id = out.logits[:, -1, :].argmax(dim=-1, keepdim=True)  # greedy\n",
        "        cur = torch.cat([cur, next_id], dim=1)\n",
        "        if eos_id is not None and int(next_id[0, 0]) == eos_id:\n",
        "            break\n",
        "\n",
        "    # Drop the start token and decode to text\n",
        "    return tokenizer.decode(cur[0, 1:], skip_special_tokens=True)\n",
        "\n",
        "@torch.no_grad()\n",
        "def _flan_generate(prompt: str, max_new_tokens: int = 256) -> str:\n",
        "    \"\"\"\n",
        "    Use native .generate() if present; otherwise use the manual greedy decoder.\n",
        "    \"\"\"\n",
        "    if hasattr(planner_mod, \"generate\"):\n",
        "        enc = planner_tok(prompt, return_tensors=\"pt\").to(_device)\n",
        "        out = planner_mod.generate(\n",
        "            **enc,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            num_beams=1\n",
        "        )\n",
        "        return planner_tok.decode(out[0], skip_special_tokens=True)\n",
        "    else:\n",
        "        return _t5_greedy_decode_no_generate(planner_mod, planner_tok, prompt, max_new_tokens=max_new_tokens)\n",
        "\n",
        "# ---------- prompt template ----------\n",
        "def _planner_prompt(ticker: str, company: str, notes: str) -> str:\n",
        "    return f\"\"\"\n",
        "You are a senior investment research planner. Produce a compact JSON plan to research the stock.\n",
        "\n",
        "Ticker: {ticker}\n",
        "Company: {company}\n",
        "Recent memory notes:\n",
        "{notes if notes.strip() else \"- none -\"}\n",
        "\n",
        "Rules:\n",
        "- Output VALID JSON only. No comments.\n",
        "- Include keys: ticker, time_window_days, data_sources, workflow, routing_rules, evaluation_rubric, optimize_if, deliverables.\n",
        "- data_sources: choose from [\"price\",\"sec\",\"news\",\"wikipedia\",\"fred\"]\n",
        "- workflow: list of steps from [\"ingest\",\"preprocess\",\"classify\",\"route\",\"analyze\",\"extract\",\"summarize\",\"evaluate\",\"optimize\",\"remember\"]\n",
        "- routing_rules: map labels to keywords, labels in [\"earnings\",\"news\",\"market\",\"other\"]\n",
        "- evaluation_rubric: 5 short criteria\n",
        "- optimize_if: use {{\"score_lt\": 4.0, \"missing_sections\": []}}\n",
        "- deliverables: [\"analyst_brief.md\",\"facts.json\",\"kpis.csv\"]\n",
        "\n",
        "Return JSON only.\n",
        "\"\"\".strip()\n",
        "\n",
        "# ---------- public API ----------\n",
        "def plan_research(ticker: str) -> dict:\n",
        "    \"\"\"\n",
        "    Return a JSON plan dict. Reads last notes via `_get_recent_notes_for_planner` (from SQLite memory cell).\n",
        "    If the model emits non-JSON, we fall back to a safe default plan that still meets the rubric.\n",
        "    \"\"\"\n",
        "    company = get_company_name(ticker)\n",
        "\n",
        "    # Use the hook defined in the SQLite memory cell; fall back to \"\" if not present.\n",
        "    try:\n",
        "        notes = _get_recent_notes_for_planner(ticker)  # defined in the SQLite memory cell\n",
        "    except Exception:\n",
        "        try:\n",
        "            # direct call if you didn't create the hook (mem_get_notes only)\n",
        "            notes = \"\\n\".join(mem_get_notes(ticker, k=5))\n",
        "        except Exception:\n",
        "            notes = \"\"\n",
        "\n",
        "    prompt = _planner_prompt(ticker, company, notes)\n",
        "    raw = _flan_generate(prompt, max_new_tokens=256)\n",
        "\n",
        "    try:\n",
        "        return json.loads(raw)\n",
        "    except Exception:\n",
        "        # Fallback plan if the model returns non-JSON\n",
        "        return {\n",
        "            \"ticker\": ticker,\n",
        "            \"time_window_days\": 90,\n",
        "            \"data_sources\": [\"price\",\"sec\",\"news\",\"wikipedia\",\"fred\"],\n",
        "            \"workflow\": [\"ingest\",\"preprocess\",\"classify\",\"route\",\"analyze\",\"extract\",\"summarize\",\"evaluate\",\"optimize\",\"remember\"],\n",
        "            \"routing_rules\": {\n",
        "                \"earnings\": [\"10-k\",\"10-q\",\"8-k\",\"transcript\",\"guidance\"],\n",
        "                \"news\":     [\"headline\",\"press\",\"acquires\",\"launches\",\"downgrade\",\"upgrade\"],\n",
        "                \"market\":   [\"price\",\"volume\",\"yield\",\"rate\",\"macro\"],\n",
        "                \"other\":    []\n",
        "            },\n",
        "            \"evaluation_rubric\": [\"accuracy\",\"evidence\",\"coverage\",\"clarity\",\"actionability\"],\n",
        "            \"optimize_if\": {\"score_lt\": 4.0, \"missing_sections\": []},\n",
        "            \"deliverables\": [\"analyst_brief.md\",\"facts.json\",\"kpis.csv\"]\n",
        "        }\n"
      ],
      "metadata": {
        "id": "Lj2AA-1T9ZZK"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test call for the planner"
      ],
      "metadata": {
        "id": "mpFDhLqnA2MG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ticker = \"AAPL\"  # or from your extract_context()\n",
        "#plan = plan_research(ticker)\n",
        "#print(json.dumps(plan, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9i1P0BLjA6C9",
        "outputId": "f9a8885e-01fc-468d-8ede-c1dd85bd4c2c"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"ticker\": \"AAPL\",\n",
            "  \"time_window_days\": 90,\n",
            "  \"data_sources\": [\n",
            "    \"price\",\n",
            "    \"sec\",\n",
            "    \"news\",\n",
            "    \"wikipedia\",\n",
            "    \"fred\"\n",
            "  ],\n",
            "  \"workflow\": [\n",
            "    \"ingest\",\n",
            "    \"preprocess\",\n",
            "    \"classify\",\n",
            "    \"route\",\n",
            "    \"analyze\",\n",
            "    \"extract\",\n",
            "    \"summarize\",\n",
            "    \"evaluate\",\n",
            "    \"optimize\",\n",
            "    \"remember\"\n",
            "  ],\n",
            "  \"routing_rules\": {\n",
            "    \"earnings\": [\n",
            "      \"10-k\",\n",
            "      \"10-q\",\n",
            "      \"8-k\",\n",
            "      \"transcript\",\n",
            "      \"guidance\"\n",
            "    ],\n",
            "    \"news\": [\n",
            "      \"headline\",\n",
            "      \"press\",\n",
            "      \"acquires\",\n",
            "      \"launches\",\n",
            "      \"downgrade\",\n",
            "      \"upgrade\"\n",
            "    ],\n",
            "    \"market\": [\n",
            "      \"price\",\n",
            "      \"volume\",\n",
            "      \"yield\",\n",
            "      \"rate\",\n",
            "      \"macro\"\n",
            "    ],\n",
            "    \"other\": []\n",
            "  },\n",
            "  \"evaluation_rubric\": [\n",
            "    \"accuracy\",\n",
            "    \"evidence\",\n",
            "    \"coverage\",\n",
            "    \"clarity\",\n",
            "    \"actionability\"\n",
            "  ],\n",
            "  \"optimize_if\": {\n",
            "    \"score_lt\": 4.0,\n",
            "    \"missing_sections\": []\n",
            "  },\n",
            "  \"deliverables\": [\n",
            "    \"analyst_brief.md\",\n",
            "    \"facts.json\",\n",
            "    \"kpis.csv\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ingest"
      ],
      "metadata": {
        "id": "GxKnV6sxDHVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Ingest ---\n",
        "\n",
        "@dataclass\n",
        "class Doc:\n",
        "    text: str\n",
        "    meta: dict\n",
        "\n",
        "def _call_tool(fn_or_tool, *args, **kwargs):\n",
        "    \"\"\"Call either a raw function or a langchain Tool wrapper.\"\"\"\n",
        "    try:\n",
        "        if hasattr(fn_or_tool, \"func\"):\n",
        "            return fn_or_tool.func(*args, **kwargs)\n",
        "        return fn_or_tool(*args, **kwargs)\n",
        "    except Exception as e:\n",
        "        return f\"Tool error: {e}\"\n",
        "\n",
        "def ingest(plan: dict) -> list[Doc]:\n",
        "    \"\"\"\n",
        "    Pull raw texts from sources selected by the planner.\n",
        "    - Threads planner's time window into news.\n",
        "    - Tags provider in news meta (newsapi/tavily/unknown).\n",
        "    - Falls back gracefully if a tool is missing.\n",
        "    \"\"\"\n",
        "    tkr = plan[\"ticker\"]\n",
        "    window = int(plan.get(\"time_window_days\", 30))  # e.g., 14/30/90\n",
        "    docs: list[Doc] = []\n",
        "\n",
        "    # --- PRICE ---\n",
        "    if \"price\" in plan[\"data_sources\"]:\n",
        "        try:\n",
        "            if \"get_stock_price\" in globals():\n",
        "                txt = _call_tool(get_stock_price, tkr)\n",
        "            elif \"yahoo_api_tool\" in globals():\n",
        "                txt = _call_tool(yahoo_api_tool, tkr)\n",
        "            else:\n",
        "                txt = f\"{tkr} price unavailable (no price tool found).\"\n",
        "        except Exception as e:\n",
        "            txt = f\"Price error: {e}\"\n",
        "        docs.append(Doc(text=str(txt), meta={\"source\": \"price\"}))\n",
        "\n",
        "    # --- SEC FILINGS ---\n",
        "    if \"sec\" in plan[\"data_sources\"]:\n",
        "        try:\n",
        "            if \"get_sec_filings\" in globals():\n",
        "                txt = _call_tool(get_sec_filings, tkr)\n",
        "            elif \"sec_api_tool\" in globals():\n",
        "                txt = _call_tool(sec_api_tool, tkr)\n",
        "            else:\n",
        "                txt = \"SEC filings unavailable (no SEC tool found).\"\n",
        "        except Exception as e:\n",
        "            txt = f\"SEC error: {e}\"\n",
        "        docs.append(Doc(text=str(txt), meta={\"source\": \"sec\"}))\n",
        "\n",
        "    # --- NEWS (NewsAPI preferred; else Tavily; else generic message) ---\n",
        "    if \"news\" in plan[\"data_sources\"]:\n",
        "        provider = None\n",
        "        txt = \"No news tool configured.\"\n",
        "        try:\n",
        "            # Prefer direct function (lets us pass days)\n",
        "            if \"get_fin_news\" in globals():\n",
        "                import inspect\n",
        "                sig = inspect.signature(get_fin_news)\n",
        "                if \"days\" in sig.parameters:\n",
        "                    txt = get_fin_news(tkr, days=window)\n",
        "                else:\n",
        "                    txt = get_fin_news(tkr)\n",
        "                provider = \"newsapi\"\n",
        "\n",
        "            elif \"get_fin_news_tavily\" in globals():\n",
        "                txt = get_fin_news_tavily(tkr)\n",
        "                provider = \"tavily\"\n",
        "\n",
        "            # If only Tool-wrapped versions exist\n",
        "            elif \"news_api_tool\" in globals():\n",
        "                txt = _call_tool(news_api_tool, tkr)  # may ignore window internally\n",
        "                provider = \"newsapi\"\n",
        "\n",
        "            elif \"tavily_news_tool\" in globals():\n",
        "                txt = _call_tool(tavily_news_tool, tkr)\n",
        "                provider = \"tavily\"\n",
        "\n",
        "        except Exception as e:\n",
        "            txt = f\"News error: {e}\"\n",
        "\n",
        "        docs.append(Doc(text=str(txt), meta={\"source\": \"news\", \"provider\": provider or \"unknown\", \"window_days\": window}))\n",
        "\n",
        "    # --- WIKIPEDIA ---\n",
        "    if \"wikipedia\" in plan[\"data_sources\"]:\n",
        "        try:\n",
        "            if \"search_wikipedia\" in globals():\n",
        "                txt = _call_tool(search_wikipedia, tkr)\n",
        "            elif \"wikipedia_tool\" in globals():\n",
        "                txt = _call_tool(wikipedia_tool, tkr)\n",
        "            else:\n",
        "                txt = \"Wikipedia summary unavailable.\"\n",
        "        except Exception as e:\n",
        "            txt = f\"Wikipedia error: {e}\"\n",
        "        docs.append(Doc(text=str(txt), meta={\"source\": \"wikipedia\"}))\n",
        "\n",
        "    # --- FRED MACRO (example: 10Y Treasury, DGS10) ---\n",
        "    if \"fred\" in plan[\"data_sources\"]:\n",
        "        series = plan.get(\"fred_series\", \"DGS10\")  # allow planner override\n",
        "        try:\n",
        "            if \"get_fred_series_web\" in globals():\n",
        "                txt = _call_tool(get_fred_series_web, series)\n",
        "            elif \"fred_api_tool\" in globals():\n",
        "                txt = _call_tool(fred_api_tool, series)\n",
        "            else:\n",
        "                txt = \"FRED value unavailable.\"\n",
        "        except Exception as e:\n",
        "            txt = f\"FRED error: {e}\"\n",
        "        docs.append(Doc(text=str(txt), meta={\"source\": \"fred\", \"series\": series}))\n",
        "\n",
        "    # debug:\n",
        "    # print(\"INGEST sources:\", [d.meta for d in docs])\n",
        "\n",
        "    return docs\n",
        "\n",
        "\n",
        "# quick test\n",
        "_raw_docs = ingest(plan)\n",
        "print(f\"Ingested {len(_raw_docs)} docs\")\n",
        "for d in _raw_docs:\n",
        "    print(\"-\", d.meta[\"source\"], \":\", d.text[:100].replace(\"\\n\", \" \") + (\"...\" if len(d.text) > 100 else \"\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k_8heYqDFmP",
        "outputId": "097a71db-46b0-48c3-c5ea-dc4a94716163"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cik_resp: 200, {\"0\":{\"cik_str\":1045810,\"ticker\":\"NVDA\",\"title\":\"NVIDIA CORP\"},\"1\":{\"cik_str\":789019,\"ticker\":\"MSFT\"\n",
            "Ingested 5 docs\n",
            "- price : AAPL price is 247.77\n",
            "- sec : Latest filings for AAPL: 4 on 2025-10-03: xslF345X05/wk-form4_1759530830.xml 4 on 2025-10-03: xslF34...\n",
            "- news : China upped the ante in its trade spat with the US, rattling investors' nerves as big Wall Street ba...\n",
            "- wikipedia : I couldn't find any information on that.\n",
            "- fred : Could not find observation value for series 'DGS10' on FRED site.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess + Classify"
      ],
      "metadata": {
        "id": "p5EyGooNDN5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunk+ HF zero shot\n",
        "# Zero shot classifier (HF)\n",
        "zshot = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "def preprocess(docs: list[Doc], chunk_size: int = 500) -> list[Doc]:\n",
        "    chunks: list[Doc] = []\n",
        "    for d in docs:\n",
        "        text = re.sub(r\"\\s+\", \" \", d.text).strip()\n",
        "        if not text:\n",
        "            continue\n",
        "        for i in range(0, len(text), chunk_size):\n",
        "            piece = text[i:i+chunk_size]\n",
        "            if piece:\n",
        "                chunks.append(Doc(text=piece, meta=d.meta.copy()))\n",
        "    return chunks\n",
        "\n",
        "def classify(chunks: list[Doc]) -> list[str]:\n",
        "    labels = []\n",
        "    candidates = [\"earnings\", \"news\", \"market\", \"other\"]\n",
        "    for c in chunks:\n",
        "        try:\n",
        "            out = zshot(c.text, candidate_labels=candidates)\n",
        "            labels.append(out[\"labels\"][0])\n",
        "        except Exception:\n",
        "            labels.append(\"other\")\n",
        "    return labels\n",
        "\n",
        "# test\n",
        "_chunks = preprocess(_raw_docs, chunk_size=500)\n",
        "_labels = classify(_chunks)\n",
        "print(\"Chunks:\", len(_chunks))\n",
        "print(\"Label counts:\", {lab: _labels.count(lab) for lab in set(_labels)})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLiIqc-GDM6d",
        "outputId": "cf22e5ae-7ce8-4a87-e52d-763acd765f50"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunks: 5\n",
            "Label counts: {'other': 2, 'news': 2, 'market': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extraction"
      ],
      "metadata": {
        "id": "3nw_B0CAD7PV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Extract facts  ---\n",
        "\n",
        "qa = pipeline(\"question-answering\", model=\"deepset/tinyroberta-squad2\")\n",
        "\n",
        "def _ask_qa(question: str, context: str):\n",
        "    try:\n",
        "        out = qa(question=question, context=context, truncation=True)\n",
        "        if isinstance(out, list) and out:\n",
        "            out = out[0]\n",
        "        return out.get(\"answer\", \"\").strip(), float(out.get(\"score\", 0.0))\n",
        "    except Exception:\n",
        "        return \"\", 0.0\n",
        "\n",
        "def extract_facts(chunks: list[Doc], labels: list[str], min_score: float = 0.25) -> dict:\n",
        "    \"\"\"\n",
        "    Ask targeted QA over 'earnings'/'news' chunks and keep answers with score >= min_score.\n",
        "    Returns dict of lists with provenance:\n",
        "      facts[\"risks\"/\"events\"/\"guidance\"] = [{text, score, source:{label,meta}}]\n",
        "    \"\"\"\n",
        "    facts = {\"risks\": [], \"events\": [], \"guidance\": [], \"snippets_used\": 0}\n",
        "\n",
        "    questions = {\n",
        "        \"risks\":    \"Name one risk or concern mentioned.\",\n",
        "        \"events\":   \"What important event or catalyst is mentioned?\",\n",
        "        \"guidance\": \"What forward guidance (if any) is mentioned?\"\n",
        "    }\n",
        "\n",
        "    for c, lab in zip(chunks, labels):\n",
        "        if lab not in (\"earnings\", \"news\"):\n",
        "            continue\n",
        "\n",
        "        for key, q in questions.items():\n",
        "            ans, score = _ask_qa(q, c.text)\n",
        "            if ans and ans.lower() != \"unknown\" and score >= min_score:\n",
        "                facts[key].append({\n",
        "                    \"text\": ans[:200],\n",
        "                    \"score\": round(score, 3),\n",
        "                    \"source\": {\"label\": lab, \"meta\": c.meta}  # captures e.g., {'source': 'sec'|'news'|'wikipedia'...}\n",
        "                })\n",
        "        facts[\"snippets_used\"] += 1\n",
        "\n",
        "    # de-dup by text while preserving highest score\n",
        "    for key in (\"risks\", \"events\", \"guidance\"):\n",
        "        uniq = {}\n",
        "        for item in facts[key]:\n",
        "            t = item[\"text\"]\n",
        "            if t not in uniq or item[\"score\"] > uniq[t][\"score\"]:\n",
        "                uniq[t] = item\n",
        "        facts[key] = list(uniq.values())\n",
        "\n",
        "    return facts\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2HMIh5EDMy6",
        "outputId": "d9086306-dc12-4979-a489-d193d9772c41"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Routing to the specialist"
      ],
      "metadata": {
        "id": "FJAKcwNiEW0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Routing specialists ---\n",
        "#send chunks to the right analyzer based on labels we have.\n",
        "# reuse the same T5 generator helper from the planner cell\n",
        "\n",
        "sentiment = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
        ")\n",
        "def earnings_analyzer(chunks: list[Doc], labels: list[str]) -> str:\n",
        "    \"\"\"\n",
        "    Summarize earnings-related content (filings, transcripts).\n",
        "    \"\"\"\n",
        "    earnings_texts = [c.text for c, lab in zip(chunks, labels) if lab == \"earnings\"]\n",
        "    if not earnings_texts:\n",
        "        return \"No earnings-specific content found.\"\n",
        "    body = \" \".join(earnings_texts[:4])\n",
        "    prompt = f\"Summarize the earnings-related information below into 3 short bullets:\\n{body}\\n\\nBullets:\"\n",
        "    return _flan_generate(prompt, max_new_tokens=160)\n",
        "\n",
        "def news_analyzer(chunks: list[Doc], labels: list[str]) -> str:\n",
        "    \"\"\"\n",
        "    Summarize news tone + highlights. We compute a rough avg sentiment for context.\n",
        "    \"\"\"\n",
        "    news_texts = [c.text for c, lab in zip(chunks, labels) if lab == \"news\"]\n",
        "    if not news_texts:\n",
        "        return \"No news items found.\"\n",
        "    # quick sentiment averaging over first few items\n",
        "    scores = []\n",
        "    for t in news_texts[:6]:\n",
        "        try:\n",
        "            lab = sentiment(t)[0][\"label\"].upper()\n",
        "            scores.append(1 if \"POS\" in lab else (-1 if \"NEG\" in lab else 0))\n",
        "        except Exception:\n",
        "            pass\n",
        "    mean = sum(scores) / len(scores) if scores else 0.0\n",
        "    body = \" \".join(news_texts[:4])\n",
        "    prompt = (\n",
        "        f\"News signal summary (avg tone ~{mean:.2f}). \"\n",
        "        f\"Write 3 short bullets with what matters for investors:\\n{body}\\n\\nBullets:\"\n",
        "    )\n",
        "    return _flan_generate(prompt, max_new_tokens=160)\n",
        "\n",
        "def market_analyzer(chunks: list[Doc], labels: list[str], ticker: str) -> str:\n",
        "    \"\"\"\n",
        "    Provide short market context using any 'market' snippets + current price line we ingested.\n",
        "    \"\"\"\n",
        "    market_texts = [c.text for c, lab in zip(chunks, labels) if lab == \"market\"]\n",
        "    # If we didn't label anything as 'market', we can still use price line from ingest\n",
        "    if not market_texts:\n",
        "        market_texts = [c.text for c in chunks if c.meta.get(\"source\") == \"price\"]\n",
        "    body = \" \".join(market_texts[:2]) if market_texts else f\"{ticker} market context not available.\"\n",
        "    prompt = f\"Given the market snippets:\\n{body}\\n\\nWrite 2 concise bullets of market context:\"\n",
        "    return _flan_generate(prompt, max_new_tokens=120)\n",
        "\n",
        "def route_and_analyze(chunks: list[Doc], labels: list[str], ticker: str) -> dict:\n",
        "    \"\"\"\n",
        "    Main router that calls each specialist and returns a small dict.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"earnings\": earnings_analyzer(chunks, labels),\n",
        "        \"news\": news_analyzer(chunks, labels),\n",
        "        \"market\": market_analyzer(chunks, labels, ticker)\n",
        "    }\n",
        "\n",
        "# quick test\n",
        "_findings = route_and_analyze(_chunks, _labels, plan[\"ticker\"])\n",
        "print(\"=== ROUTED FINDINGS ===\")\n",
        "for k, v in _findings.items():\n",
        "    print(f\"\\n[{k.upper()}]\\n{v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-smbKxHnEac-",
        "outputId": "6604b47f-7e5d-401d-cb11-656d74b4da2e"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ROUTED FINDINGS ===\n",
            "\n",
            "[EARNINGS]\n",
            "No earnings-specific content found.\n",
            "\n",
            "[NEWS]\n",
            "Latest filings for AAPL: 4 on 2025-10-03: xslF345X05/wk-form4_1759530830.xml 4 on 2025-10-03: xslF345X05/wk-form4_1759530744.xml 4 on 2025-10-03: xslF345X05/wk-form4_1759530690.xml China upped the ante in its trade spat with the US, rattling investors' nerves as big Wall Street banks kicked off earnings season.\n",
            "\n",
            "[MARKET]\n",
            "AAPL price is 247.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## External LLM Analysis"
      ],
      "metadata": {
        "id": "KqspX0yt7O7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === External LLM analysis adapter (OpenAI or Gemini) ===\n",
        "# Uses ingested context and asks LLM to write a structured analysis.\n",
        "\n",
        "def _build_context_for_llm(ticker, findings, facts, chunks, max_chars=8000):\n",
        "    def pick_texts(key):\n",
        "        return [it.get(\"text\",\"\") for it in facts.get(key, []) if it.get(\"text\")]\n",
        "\n",
        "    def pick_sources(key):\n",
        "        s = []\n",
        "        for it in facts.get(key, []):\n",
        "            lab = it.get(\"source\", {}).get(\"label\", \"?\")\n",
        "            name = it.get(\"source\", {}).get(\"meta\", {}).get(\"source\", \"?\")\n",
        "            s.append(f\"{lab}->{name}\")\n",
        "        return s\n",
        "\n",
        "    risks_txt    = pick_texts(\"risks\")\n",
        "    events_txt   = pick_texts(\"events\")\n",
        "    guidance_txt = pick_texts(\"guidance\")\n",
        "\n",
        "    risks_src    = pick_sources(\"risks\")\n",
        "    events_src   = pick_sources(\"events\")\n",
        "    guidance_src = pick_sources(\"guidance\")\n",
        "\n",
        "    header = f\"Ticker: {ticker}\\n\"\n",
        "    fx = (\n",
        "        \"Findings:\\n\"\n",
        "        f\"[EARNINGS]\\n{findings.get('earnings','')}\\n\"\n",
        "        f\"[NEWS]\\n{findings.get('news','')}\\n\"\n",
        "        f\"[MARKET]\\n{findings.get('market','')}\\n\\n\"\n",
        "        \"Extracted facts:\\n\"\n",
        "        f\"- Risks: {risks_txt} | sources: {risks_src}\\n\"\n",
        "        f\"- Events: {events_txt} | sources: {events_src}\\n\"\n",
        "        f\"- Guidance: {guidance_txt} | sources: {guidance_src}\\n\"\n",
        "    )\n",
        "\n",
        "    snippets = \" \".join([c.text for c in chunks[:6]])\n",
        "    ctx = f\"{header}\\n{fx}\\nSnippets:\\n{snippets}\"\n",
        "    return ctx[:max_chars]\n",
        "\n",
        "\n",
        "def _analysis_prompt(context, company=None):\n",
        "    # Schema for strict JSON so you can parse the result\n",
        "    return f\"\"\"\n",
        "You are a buy-side equity analyst. Using ONLY the provided context, produce a concise, structured analysis.\n",
        "\n",
        "Return STRICT JSON with this schema:\n",
        "{{\n",
        "  \"summary\": str,                // 4-6 sentences; what matters now\n",
        "  \"thesis\": str,                 // your current investment thesis (neutral is ok)\n",
        "  \"risks\": [str],                // 2-5 specific risks\n",
        "  \"catalysts\": [str],            // 2-5 potential near-term catalysts\n",
        "  \"data_points\": [str],          // concrete evidence you used (price, filings, headlines, macro)\n",
        "  \"sentiment\": \"bullish|neutral|bearish\",\n",
        "  \"confidence\": float,           // 1.0–5.0\n",
        "  \"next_actions\": [str]          // what to do next in research\n",
        "}}\n",
        "\n",
        "Rules:\n",
        "- Cite facts only from the context.\n",
        "- Be specific and investor-focused.\n",
        "- Keep lists short.\n",
        "- Output ONLY valid JSON (no markdown, no commentary).\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\".strip()\n",
        "\n",
        "# -------- Provider: OpenAI ---------\n",
        "def _call_openai_json(prompt, model=\"gpt-4o-mini\", temperature=0.2, timeout=60, api_key=None):\n",
        "    api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n",
        "    if not api_key:\n",
        "        try:\n",
        "            from google.colab import userdata\n",
        "            api_key = userdata.get(\"OPENAI\") or userdata.get(\"OPENAI_API_KEY\") or userdata.get(\"OPENAI_KEY\")\n",
        "        except Exception:\n",
        "            api_key = None\n",
        "    if not api_key:\n",
        "        raise RuntimeError(\"Missing OpenAI API key. Set `openai_key = userdata.get('OPENAI')` or export OPENAI_API_KEY.\")\n",
        "\n",
        "    headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
        "    body = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a precise financial analyst. Return ONLY valid JSON.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        \"response_format\": {\"type\": \"json_object\"},\n",
        "        \"temperature\": temperature\n",
        "    }\n",
        "    r = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=body, timeout=timeout)\n",
        "    r.raise_for_status()\n",
        "    content = r.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return json.loads(content)\n",
        "\n",
        "\n",
        "# --------- Provider: Gemini ---------\n",
        "def _call_gemini_json(prompt, model=\"gemini-1.5-flash\", temperature=0.2):\n",
        "    import google.generativeai as genai\n",
        "    api_key = os.getenv(\"GEMINI_API_KEY\", \"\")\n",
        "    if not api_key:\n",
        "        raise RuntimeError(\"Missing GEMINI_API_KEY\")\n",
        "    genai.configure(api_key=api_key)\n",
        "\n",
        "    generation_config = {\n",
        "        \"temperature\": temperature,\n",
        "        \"max_output_tokens\": 1024,\n",
        "        \"response_mime_type\": \"application/json\"\n",
        "    }\n",
        "    model = genai.GenerativeModel(model_name=model, generation_config=generation_config)\n",
        "    resp = model.generate_content(prompt)\n",
        "    txt = getattr(resp, \"text\", None) or (resp.candidates[0].content.parts[0].text if resp.candidates else \"\")\n",
        "    try:\n",
        "        return json.loads(txt)\n",
        "    except Exception:\n",
        "        # Try to extract JSON block if the model wrapped it in text\n",
        "        m = re.search(r\"\\{.*\\}\", txt, flags=re.S)\n",
        "        if m:\n",
        "            return json.loads(m.group(0))\n",
        "        raise RuntimeError(f\"Gemini did not return valid JSON.\\n---\\n{txt[:400]}\")\n",
        "\n",
        "# --------- Public API ---------\n",
        "def llm_investment_analysis(ticker, findings, facts, chunks, provider=\"openai\", model=None, api_key=None):\n",
        "    context = _build_context_for_llm(ticker, findings, facts, chunks)\n",
        "    prompt  = _analysis_prompt(context)\n",
        "    if provider == \"openai\":\n",
        "        model = model or \"gpt-5-mini\"\n",
        "        return _call_openai_json(prompt, model=model, api_key=api_key)\n",
        "    elif provider == \"gemini\":\n",
        "        model = model or \"gemini-1.5-flash\"\n",
        "        return _call_gemini_json(prompt, model=model)\n",
        "    else:\n",
        "        raise ValueError(\"provider must be 'openai' or 'gemini'\")\n"
      ],
      "metadata": {
        "id": "lmvLQOZZ7M94"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarize --> Evaluate --> optimize"
      ],
      "metadata": {
        "id": "_QRjL9HoFF2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Summarize → Evaluate → Optimize → Remember ---\n",
        "# Uses the same _flan_generate() helper from the planner cell\n",
        "\n",
        "def summarize_report(ticker: str, findings: dict, facts: dict, chunks: list[Doc]) -> str:\n",
        "    \"\"\"\n",
        "    Builds an analyst-style brief. We include routed findings + provenance-aware extracted facts.\n",
        "    \"\"\"\n",
        "    # Lightweight context (first few chunks) to ground the summary\n",
        "    context_text = \" \".join([c.text for c in chunks[:6]])\n",
        "\n",
        "    # Build a compact preview from provenance-aware facts\n",
        "    def _facts_preview(facts: dict, max_items=4) -> str:\n",
        "        def _fmt(items):\n",
        "            out = []\n",
        "            for it in items[:max_items]:\n",
        "                src_label = it.get(\"source\", {}).get(\"label\", \"?\")\n",
        "                src_name  = it.get(\"source\", {}).get(\"meta\", {}).get(\"source\", \"?\")\n",
        "                out.append(f\"- {it.get('text','')} (src: {src_label}->{src_name}, score={it.get('score',0):.2f})\")\n",
        "            return \"\\n\".join(out) if out else \"- none\"\n",
        "\n",
        "        return (\n",
        "            \"Risks:\\n\"    + _fmt(facts.get(\"risks\", []))    + \"\\n\"\n",
        "            \"Events:\\n\"   + _fmt(facts.get(\"events\", []))   + \"\\n\"\n",
        "            \"Guidance:\\n\" + _fmt(facts.get(\"guidance\", []))\n",
        "        )\n",
        "\n",
        "    facts_preview = _facts_preview(facts)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an equity research analyst. Write a concise, structured brief for {ticker}.\n",
        "Include:\n",
        "1) Snapshot (what the company/stock is doing),\n",
        "2) Highlights from filings/news,\n",
        "3) Macro context if present,\n",
        "4) 2–3 risks/catalysts,\n",
        "5) A clear investor takeaway.\n",
        "\n",
        "Context (snippets):\n",
        "{context_text}\n",
        "\n",
        "Findings (specialists):\n",
        "[EARNINGS]\n",
        "{findings.get(\"earnings\",\"\")}\n",
        "\n",
        "[NEWS]\n",
        "{findings.get(\"news\",\"\")}\n",
        "\n",
        "[MARKET]\n",
        "{findings.get(\"market\",\"\")}\n",
        "\n",
        "Extracted facts (with sources):\n",
        "{facts_preview}\n",
        "\n",
        "Return in this format:\n",
        "\n",
        "- Snapshot:\n",
        "- Highlights:\n",
        "- Risks/Catalysts:\n",
        "- Takeaway:\n",
        "\"\"\".strip()\n",
        "\n",
        "    return _flan_generate(prompt, max_new_tokens=320)\n",
        "\n",
        "\n",
        "RUBRIC = [\n",
        "    \"Accuracy (facts look correct)\",\n",
        "    \"Evidence use (filings/news referenced)\",\n",
        "    \"Coverage (price/news/filings/macro present)\",\n",
        "    \"Clarity (well-structured, concise)\",\n",
        "    \"Actionability (clear takeaway)\"\n",
        "]\n",
        "\n",
        "def evaluate_brief(brief: str) -> tuple[float, str]:\n",
        "    \"\"\"\n",
        "    Get a 1–5 score and short feedback using an LLM grader prompt.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "Rate the following brief from 1 (poor) to 5 (excellent) on these criteria:\n",
        "{RUBRIC}\n",
        "\n",
        "Brief:\n",
        "{brief}\n",
        "\n",
        "Respond with: \"score: X.Y\" and then one short sentence of feedback.\n",
        "\"\"\".strip()\n",
        "\n",
        "    txt = _flan_generate(prompt, max_new_tokens=128)\n",
        "    m = re.search(r\"score\\s*:\\s*([0-5](?:\\.\\d)?)\", txt.lower())\n",
        "    score = float(m.group(1)) if m else 3.0\n",
        "    return score, txt\n",
        "\n",
        "def optimize_brief_if_needed(brief: str, score: float, feedback: str, threshold: float = 4.0) -> str:\n",
        "    \"\"\"\n",
        "    If score < threshold, ask the model to improve the brief using the feedback.\n",
        "    \"\"\"\n",
        "    if score >= threshold:\n",
        "        return brief\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an editor. Improve the brief given the feedback. Keep it concise and structured.\n",
        "Feedback:\n",
        "{feedback}\n",
        "\n",
        "Brief:\n",
        "{brief}\n",
        "\n",
        "Return the improved brief only.\n",
        "\"\"\".strip()\n",
        "\n",
        "    return _flan_generate(prompt, max_new_tokens=260)\n",
        "\n",
        "# --- Remember (write a short note for the next run) ---\n",
        "def remember_takeaway(ticker: str, brief: str):\n",
        "    \"\"\"\n",
        "    Save a short takeaway for future planning. If you already have SessionMemory, write there.\n",
        "    Otherwise, this is a safe no-op that you can later wire to SQLite or your memory class.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Try to extract the Takeaway line\n",
        "        m = re.search(r\"Takeaway\\s*:\\s*(.*)\", brief)\n",
        "        note = (m.group(1) if m else brief)[:240]\n",
        "        # SESSION_MEMORY.remember(ticker, note)\n",
        "        print(\"Saved memory note.\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# --- Export helpers ---\n",
        "def export_outputs(ticker: str, brief: str, facts: dict, out_dir: str = \"./outputs\"):\n",
        "    p = Path(out_dir); p.mkdir(exist_ok=True)\n",
        "    (p / f\"{ticker}_analyst_brief.md\").write_text(brief)\n",
        "    (p / f\"{ticker}_facts.json\").write_text(json.dumps(facts, indent=2))\n"
      ],
      "metadata": {
        "id": "J2Na73VcFMHo"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KPIs export\n",
        "\n",
        "def _avg_news_sentiment(findings: dict) -> float:\n",
        "    m = re.search(r\"avg tone ~(-?\\d+\\.\\d+)\", findings.get(\"news\",\"\"))\n",
        "    return float(m.group(1)) if m else 0.0\n",
        "\n",
        "def _last_price_from_chunks(chunks) -> float | None:\n",
        "    for c in chunks:\n",
        "        if c.meta.get(\"source\") == \"price\":\n",
        "            m = re.search(r\"price is ([\\d\\.]+)\", c.text)\n",
        "            if m: return float(m.group(1))\n",
        "    return None\n",
        "\n",
        "def export_kpis_csv(ticker: str, facts: dict, findings: dict, chunks, out_dir=\"./outputs\"):\n",
        "    Path(out_dir).mkdir(exist_ok=True)\n",
        "    rows = [\n",
        "        {\"ticker\": ticker, \"kpi\": \"num_risks\", \"value\": len(facts.get(\"risks\", []))},\n",
        "        {\"ticker\": ticker, \"kpi\": \"num_events\", \"value\": len(facts.get(\"events\", []))},\n",
        "        {\"ticker\": ticker, \"kpi\": \"num_guidance_items\", \"value\": len(facts.get(\"guidance\", []))},\n",
        "        {\"ticker\": ticker, \"kpi\": \"avg_news_sentiment\", \"value\": _avg_news_sentiment(findings)},\n",
        "    ]\n",
        "    lp = _last_price_from_chunks(chunks)\n",
        "    if lp is not None:\n",
        "        rows.append({\"ticker\": ticker, \"kpi\": \"last_price\", \"value\": lp})\n",
        "\n",
        "    fp = Path(out_dir) / f\"{ticker}_kpis.csv\"\n",
        "    with open(fp, \"w\", newline=\"\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=[\"ticker\",\"kpi\",\"value\"])\n",
        "        w.writeheader(); w.writerows(rows)\n",
        "    print(f\"Saved KPIs -> {fp}\")\n",
        "\n",
        "export_kpis_csv(plan[\"ticker\"], _facts, _findings, _chunks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giXt-uuaLjX8",
        "outputId": "e0620531-bd75-4e75-f994-497096a2ab92"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved KPIs -> outputs/AAPL_kpis.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test cell"
      ],
      "metadata": {
        "id": "KhEHolylGRkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##final_brief = summarize_report(plan[\"ticker\"], _findings, _facts, _chunks)\n",
        "#score, feedback = evaluate_brief(final_brief)\n",
        "#improved_brief = optimize_brief_if_needed(final_brief, score, feedback, threshold=plan[\"optimize_if\"][\"score_lt\"])\n",
        "\n",
        "#print(\"=== SCORE ===\", score)\n",
        "#print(\"=== FEEDBACK ===\", feedback[:300])\n",
        "#print(\"=== FINAL BRIEF ===\\n\", improved_brief)\n",
        "\n",
        "# remember + export\n",
        "#remember_takeaway(plan[\"ticker\"], improved_brief)\n",
        "#export_outputs(plan[\"ticker\"], improved_brief, _facts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQxzJcWiGOXj",
        "outputId": "3e6d238d-b89b-4c5e-e601-9e2e79e7760b"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== SCORE === 3.0\n",
            "=== FEEDBACK === 5\n",
            "=== FINAL BRIEF ===\n",
            " TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA: TSLA:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0P_o12beGN4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##ticker = \"AAPL\"\n",
        "#plan = plan_research(ticker)\n",
        "#_raw_docs = ingest(plan)\n",
        "#_chunks = preprocess(_raw_docs)\n",
        "#_labels = classify(_chunks)\n",
        "#_facts = extract_facts(_chunks, _labels)\n",
        "#_findings = route_and_analyze(_chunks, _labels, plan[\"ticker\"])\n",
        "#final_brief = summarize_report(plan[\"ticker\"], _findings, _facts, _chunks)\n",
        "#score, feedback = evaluate_brief(final_brief)\n",
        "#final_brief = optimize_brief_if_needed(final_brief, score, feedback, threshold=plan[\"optimize_if\"][\"score_lt\"])\n",
        "#remember_takeaway(plan[\"ticker\"], final_brief)\n",
        "#export_kpis_csv(plan[\"ticker\"], _facts, _findings, _chunks)\n",
        "#export_outputs(plan[\"ticker\"], final_brief, _facts)\n",
        "#export_kpis_csv(plan[\"ticker\"], _facts, _findings, _chunks)\n",
        "\n",
        "#print(\"OK:\", len(_raw_docs), \"docs,\", len(_chunks), \"chunks,\", \"score\", score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IH_pRDFbKp65",
        "outputId": "136d43a5-2e3e-4231-9e0e-3229c9726fa0"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cik_resp: 200, {\"0\":{\"cik_str\":1045810,\"ticker\":\"NVDA\",\"title\":\"NVIDIA CORP\"},\"1\":{\"cik_str\":789019,\"ticker\":\"MSFT\"\n",
            "Saved KPIs -> outputs/AAPL_kpis.csv\n",
            "Saved KPIs -> outputs/AAPL_kpis.csv\n",
            "OK: 5 docs, 5 chunks, score 3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Commented out this part as it's integrated with the RUN_AGENT function\n",
        "\n",
        "\n",
        "#analysis_json = llm_investment_analysis(\n",
        "    #ticker=plan[\"ticker\"],\n",
        "    #findings=_findings,\n",
        "    #facts=_facts,\n",
        "    #chunks=_chunks,\n",
        "    #provider=\"openai\",\n",
        "    #model=\"gpt-4o-mini\",\n",
        "    #api_key=openai_key\n",
        "#)\n",
        "#print(json.dumps(analysis_json, indent=2))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1r3c-j38y6Q",
        "outputId": "9b7b1922-b0f7-48c6-965a-2e5ded2fed4a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"summary\": \"Tesla's current stock price is $429.24, reflecting ongoing investor interest. Recent filings indicate potential developments that could impact the company's future performance. However, the lack of earnings-specific content raises questions about immediate financial health. Investors should monitor upcoming filings for insights into operational strategies and market positioning.\",\n",
            "  \"thesis\": \"Neutral; while TSLA shows potential for growth, the absence of earnings data and reliance on filings creates uncertainty.\",\n",
            "  \"risks\": [\n",
            "    \"Dependence on recent filings for insights\",\n",
            "    \"Market volatility affecting stock price\",\n",
            "    \"Potential negative news from upcoming filings\"\n",
            "  ],\n",
            "  \"catalysts\": [\n",
            "    \"Upcoming filings may provide new insights\",\n",
            "    \"Market reactions to broader economic conditions\"\n",
            "  ],\n",
            "  \"data_points\": [\n",
            "    \"TSLA price is 429.24\",\n",
            "    \"Latest filings for TSLA: DEFA14A on 2025-10-14\"\n",
            "  ],\n",
            "  \"sentiment\": \"neutral\",\n",
            "  \"confidence\": 3.0,\n",
            "  \"next_actions\": [\n",
            "    \"Monitor upcoming filings for earnings updates\",\n",
            "    \"Analyze market trends affecting TSLA\",\n",
            "    \"Review competitor performance for comparative analysis\"\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the agent"
      ],
      "metadata": {
        "id": "dOmeK-NYGxe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Run the agent on a single request ===\n",
        "# Call with:  _ = run_agent(\"TSLA\")  or  _ = run_agent(\"analyze Tesla\")\n",
        "\n",
        "def _pick_symbol(text: str) -> str:\n",
        "    # Prefer the smart resolver (supports company names)\n",
        "    return resolve_ticker(text)\n",
        "\n",
        "# === Run the agent (integrated external LLM JSON analysis) ===\n",
        "import json, re\n",
        "from pathlib import Path\n",
        "\n",
        "def run_agent(user_text_or_ticker: str,\n",
        "              save_outputs: bool = True,\n",
        "              use_external_llm: bool = True,\n",
        "              provider: str = \"openai\",\n",
        "              model: str = \"gpt-4o-mini\",\n",
        "              api_key: str | None = None):\n",
        "    # 1) Resolve ticker\n",
        "    ticker = resolve_ticker(user_text_or_ticker)\n",
        "    if not ticker or not TICKER_RE.match(ticker):\n",
        "        raise ValueError(\"Could not resolve a valid ticker. Try 'NVDA' or 'analyze Nvidia'.\")\n",
        "    print(f\"Resolved ticker: {ticker}\")\n",
        "\n",
        "    # Optional: respect \"last two weeks\"\n",
        "    time_override_days = 14 if re.search(r\"last\\s+(two|2)\\s+weeks\", user_text_or_ticker, re.I) else None\n",
        "\n",
        "    # 2) Plan\n",
        "    plan = plan_research(ticker)\n",
        "    if time_override_days:\n",
        "        plan[\"time_window_days\"] = time_override_days\n",
        "    print(\"=== PLAN ===\")\n",
        "    print(json.dumps(plan, indent=2))\n",
        "\n",
        "    # 3–4) Chain + Routing\n",
        "    raw_docs = ingest(plan)\n",
        "    chunks   = preprocess(raw_docs)\n",
        "    labels   = classify(chunks)\n",
        "    facts    = extract_facts(chunks, labels)\n",
        "    findings = route_and_analyze(chunks, labels, plan[\"ticker\"])\n",
        "\n",
        "    # 5–6) Summarize → Evaluate → Optimize\n",
        "    brief = summarize_report(plan[\"ticker\"], findings, facts, chunks)\n",
        "    score, feedback = evaluate_brief(brief)\n",
        "    final_brief = optimize_brief_if_needed(brief, score, feedback, threshold=plan[\"optimize_if\"][\"score_lt\"])\n",
        "\n",
        "    print(\"\\n=== SCORE ===\", score)\n",
        "    print(\"=== FINAL BRIEF ===\\n\", final_brief)\n",
        "\n",
        "    remember_takeaway(plan[\"ticker\"], final_brief)\n",
        "\n",
        "    # 7) External LLM analysis JSON (OpenAI/Gemini)\n",
        "    external = None\n",
        "    if use_external_llm and \"llm_investment_analysis\" in globals():\n",
        "        try:\n",
        "            external = llm_investment_analysis(\n",
        "                ticker=plan[\"ticker\"],\n",
        "                findings=findings,\n",
        "                facts=facts,\n",
        "                chunks=chunks,\n",
        "                provider=provider,\n",
        "                model=model,\n",
        "                api_key=api_key\n",
        "            )\n",
        "            print(\"\\n=== EXTERNAL LLM ANALYSIS ===\")\n",
        "            print(json.dumps(external, indent=2))\n",
        "        except Exception as e:\n",
        "            print(f\"[warn] External LLM analysis failed: {e}\")\n",
        "\n",
        "    # 8) Exports\n",
        "    if save_outputs:\n",
        "        export_outputs(plan[\"ticker\"], final_brief, facts)\n",
        "        export_kpis_csv(plan[\"ticker\"], facts, findings, chunks)\n",
        "        if external is not None:\n",
        "            outp = Path(\"./outputs\"); outp.mkdir(exist_ok=True)\n",
        "            (outp / f\"{plan['ticker']}_analysis_llm.json\").write_text(json.dumps(external, indent=2))\n",
        "\n",
        "    return {\n",
        "        \"plan\": plan,\n",
        "        \"raw_docs\": raw_docs,\n",
        "        \"chunks\": chunks,\n",
        "        \"labels\": labels,\n",
        "        \"facts\": facts,\n",
        "        \"findings\": findings,\n",
        "        \"score\": score,\n",
        "        \"brief\": final_brief,\n",
        "        \"external\": external\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "Xi1fgZxmG2R3"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the cell below to run the agent. ask abou"
      ],
      "metadata": {
        "id": "fC6rfUkzN6kX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_ = run_agent(\"analyze teSLA\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr4CQt_ENZST",
        "outputId": "87a68939-373f-422e-9c01-a87a0f49c97b"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resolved ticker: TSLA\n",
            "=== PLAN ===\n",
            "{\n",
            "  \"ticker\": \"TSLA\",\n",
            "  \"time_window_days\": 90,\n",
            "  \"data_sources\": [\n",
            "    \"price\",\n",
            "    \"sec\",\n",
            "    \"news\",\n",
            "    \"wikipedia\",\n",
            "    \"fred\"\n",
            "  ],\n",
            "  \"workflow\": [\n",
            "    \"ingest\",\n",
            "    \"preprocess\",\n",
            "    \"classify\",\n",
            "    \"route\",\n",
            "    \"analyze\",\n",
            "    \"extract\",\n",
            "    \"summarize\",\n",
            "    \"evaluate\",\n",
            "    \"optimize\",\n",
            "    \"remember\"\n",
            "  ],\n",
            "  \"routing_rules\": {\n",
            "    \"earnings\": [\n",
            "      \"10-k\",\n",
            "      \"10-q\",\n",
            "      \"8-k\",\n",
            "      \"transcript\",\n",
            "      \"guidance\"\n",
            "    ],\n",
            "    \"news\": [\n",
            "      \"headline\",\n",
            "      \"press\",\n",
            "      \"acquires\",\n",
            "      \"launches\",\n",
            "      \"downgrade\",\n",
            "      \"upgrade\"\n",
            "    ],\n",
            "    \"market\": [\n",
            "      \"price\",\n",
            "      \"volume\",\n",
            "      \"yield\",\n",
            "      \"rate\",\n",
            "      \"macro\"\n",
            "    ],\n",
            "    \"other\": []\n",
            "  },\n",
            "  \"evaluation_rubric\": [\n",
            "    \"accuracy\",\n",
            "    \"evidence\",\n",
            "    \"coverage\",\n",
            "    \"clarity\",\n",
            "    \"actionability\"\n",
            "  ],\n",
            "  \"optimize_if\": {\n",
            "    \"score_lt\": 4.0,\n",
            "    \"missing_sections\": []\n",
            "  },\n",
            "  \"deliverables\": [\n",
            "    \"analyst_brief.md\",\n",
            "    \"facts.json\",\n",
            "    \"kpis.csv\"\n",
            "  ]\n",
            "}\n",
            "cik_resp: 200, {\"0\":{\"cik_str\":1045810,\"ticker\":\"NVDA\",\"title\":\"NVIDIA CORP\"},\"1\":{\"cik_str\":789019,\"ticker\":\"MSFT\"\n",
            "\n",
            "=== SCORE === 3.0\n",
            "=== FINAL BRIEF ===\n",
            " TSLA: China's stock price is 429.24.\n",
            "Saved memory note.\n",
            "\n",
            "=== EXTERNAL LLM ANALYSIS ===\n",
            "{\n",
            "  \"summary\": \"Tesla's current stock price is $429.24 amidst a turbulent market environment influenced by escalating trade tensions between China and the US. The latest filings indicate ongoing corporate governance activities, but no earnings-specific updates are available. Investor sentiment may be cautious as major Wall Street banks begin their earnings reports. Overall, the market's reaction to geopolitical developments could significantly impact TSLA's performance in the near term.\",\n",
            "  \"thesis\": \"Neutral; while TSLA remains a strong player in the EV market, external factors like trade tensions could hinder short-term performance.\",\n",
            "  \"risks\": [\n",
            "    \"Escalating US-China trade tensions affecting investor sentiment\",\n",
            "    \"Lack of earnings updates leading to uncertainty\",\n",
            "    \"Potential market volatility during earnings season\"\n",
            "  ],\n",
            "  \"catalysts\": [\n",
            "    \"Upcoming earnings reports from major Wall Street banks\",\n",
            "    \"Further developments in US-China trade negotiations\",\n",
            "    \"New product announcements or updates from Tesla\"\n",
            "  ],\n",
            "  \"data_points\": [\n",
            "    \"TSLA price is 429.24\",\n",
            "    \"Latest filings for TSLA: DEFA14A on 2025-10-14\",\n",
            "    \"China's trade actions impacting market sentiment\"\n",
            "  ],\n",
            "  \"sentiment\": \"neutral\",\n",
            "  \"confidence\": 3.0,\n",
            "  \"next_actions\": [\n",
            "    \"Monitor upcoming earnings reports for market reactions\",\n",
            "    \"Stay updated on US-China trade negotiations\",\n",
            "    \"Review Tesla's upcoming product announcements\"\n",
            "  ]\n",
            "}\n",
            "Saved KPIs -> outputs/TSLA_kpis.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Visualize the agent workflow as a directed graph\n",
        "# Shows main path + branches (external LLM) + memory loop.\n",
        "\n",
        "\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define nodes (your pipeline stages)\n",
        "nodes = [\n",
        "    \"plan\", \"ingest\", \"preprocess\", \"classify\", \"extract\",\n",
        "    \"route\", \"summarize\", \"evaluate\", \"optimize\", \"remember\", \"export\"\n",
        "]\n",
        "\n",
        "# Define edges (dataflow)\n",
        "edges = [\n",
        "    (\"plan\", \"ingest\"),\n",
        "    (\"ingest\", \"preprocess\"),\n",
        "    (\"preprocess\", \"classify\"),\n",
        "    (\"classify\", \"extract\"),\n",
        "    (\"extract\", \"route\"),\n",
        "    (\"route\", \"summarize\"),\n",
        "    (\"summarize\", \"evaluate\"),\n",
        "    (\"evaluate\", \"optimize\"),\n",
        "    (\"optimize\", \"remember\"),\n",
        "    (\"remember\", \"export\"),\n",
        "\n",
        "    # feedback loops\n",
        "    (\"remember\", \"plan\"),   # memory -> planner (learn across runs)\n",
        "    (\"evaluate\", \"summarize\"),  # evaluator-optimizer loop (refine brief)\n",
        "]\n",
        "\n",
        "# (Optional) external LLM branch (if you enable it in run_agent(use_external_llm=True))\n",
        "nodes += [\"external_llm_json\"]\n",
        "edges += [(\"route\", \"external_llm_json\"), (\"extract\", \"external_llm_json\")]\n",
        "\n",
        "G = nx.DiGraph()\n",
        "G.add_nodes_from(nodes)\n",
        "G.add_edges_from(edges)\n",
        "\n",
        "# Layout: layered left->right\n",
        "pos = nx.nx_agraph.graphviz_layout(G, prog=\"dot\") if nx.drawing.nx_agraph else nx.spring_layout(G, seed=42)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "nx.draw_networkx_nodes(G, pos, node_size=1800, node_color=\"#e8eef8\", edgecolors=\"#4a5b7a\")\n",
        "nx.draw_networkx_labels(G, pos, font_size=10)\n",
        "nx.draw_networkx_edges(G, pos, arrows=True, arrowstyle=\"-|>\", arrowsize=18, width=1.6)\n",
        "\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"AAI-520 Investment Research Agent — Workflow Graph\", fontsize=12)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "oVplH2SXNaCt",
        "outputId": "b55f5e84-b317-43fa-e3fa-86c58a2e06ab"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "requires pygraphviz http://pygraphviz.github.io/",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/networkx/drawing/nx_agraph.py\u001b[0m in \u001b[0;36mpygraphviz_layout\u001b[0;34m(G, prog, root, args)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mpygraphviz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pygraphviz'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4089543216.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Layout: layered left->right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnx_agraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraphviz_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dot\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrawing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnx_agraph\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspring_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/networkx/drawing/nx_agraph.py\u001b[0m in \u001b[0;36mgraphviz_layout\u001b[0;34m(G, prog, root, args)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0msee\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mgitlab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mgraphviz\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mgraphviz\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0missues\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1767\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmore\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \"\"\"\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpygraphviz_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/networkx/drawing/nx_agraph.py\u001b[0m in \u001b[0;36mpygraphviz_layout\u001b[0;34m(G, prog, root, args)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpygraphviz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"requires pygraphviz http://pygraphviz.github.io/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"-Groot={root}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: requires pygraphviz http://pygraphviz.github.io/",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install networkx"
      ],
      "metadata": {
        "id": "3YiF_eaSlLj-"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "-===========================================================================-\n",
        "\n",
        "-===========================================================================-\n",
        "\n",
        "-===========================================================================-\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1O3EjpjaGNwn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AH3XukKp2v4Q",
        "outputId": "549db3b7-0dee-4426-e3ea-01f8ed9bf797"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Could not find observation value for series 'GDP' on FRED site.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "get_fred_series_web('GDP')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "IdTf_zXh_wd0"
      },
      "outputs": [],
      "source": [
        "#For debugging\n",
        "#print(get_sec_filings(\"AAPL\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "X1a72SE52v4R"
      },
      "outputs": [],
      "source": [
        "query = 'Research TSLA'\n",
        "\n",
        "api_key = perplexity_key\n",
        "url = \"https://api.perplexity.ai/v1/search\"\n",
        "headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "params = {\"q\": query, \"num_results\": 3}\n",
        "response = requests.get(url, headers=headers, params=params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nRz8SZNx2v4R",
        "outputId": "600b4a9c-6dd4-4fbc-f2cd-38eabafb5d96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Error from Perplexity: 404 '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "perplexity_search('Research TSLA')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "aTeexEcD2v4R",
        "outputId": "3c6e10f3-232a-456b-9be7-93ad9dd0d9b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.get_stock_price(ticker: str) -> str>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>get_stock_price</b><br/>def get_stock_price(ticker: str) -&gt; str</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/tmp/ipython-input-1240038071.py</a>&lt;no docstring&gt;</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "yahoo_api_tool.func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TAj1nujP2v4R",
        "outputId": "4437a27a-ef53-47d8-d2a4-ab47bc35cd80"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I couldn't find any information on that.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "search_wikipedia(\"Research TSLA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "biMVj3ifg3ha"
      },
      "outputs": [],
      "source": [
        "#Create the tools list\n",
        "tools = [\n",
        "    yahoo_api_tool,\n",
        "    sec_api_tool,\n",
        "    news_api_tool,\n",
        "    tavily_news_tool,\n",
        "    wikipedia_tool,\n",
        "    sentiment_api_tool,\n",
        "    earnings_specialist,\n",
        "    sentiment_anaylsis_specialist,\n",
        "    wikipedia_tool,\n",
        "    perplexity_search_tool,\n",
        "    fred_api_tool\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "g2R9ogd52v4R",
        "outputId": "80765e71-5bde-4ab5-e20c-a3a6ff0f23b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'TSLA price is 429.24'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "#\n",
        "earnings_specialist.invoke({'messages': 'TSLA'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_EbjC2ok3m4"
      },
      "source": [
        "### Tools Agent Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "wyMhkBfhf3Hi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "2ecb4fc5-ca21-4eae-9737-cc87771048b4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "create_react_agent() got an unexpected keyword argument 'model'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2828982865.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#Define the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m tools_agent = create_react_agent(\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm_openai\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: create_react_agent() got an unexpected keyword argument 'model'"
          ]
        }
      ],
      "source": [
        "#Tools Agent Prompt & Function\n",
        "\n",
        "#Define the prompt, balaning using as many tools as possible without breaking recursion limits\n",
        "THOROUGH_ANALYSIS_PROMPT = \"\"\"\n",
        "You are a comprehensive financial research agent. You MUST use multiple tools to conduct deep analysis.\n",
        "\n",
        "MANDATORY RESEARCH STEPS - Complete ALL of these:\n",
        "\n",
        "1. **Company Overview**: Get basic company information, sector, and key metrics\n",
        "2. **Financial Data**: Retrieve latest quarterly/annual financials (revenue, profit, cash flow)\n",
        "3. **SEC Filings**: Download and analyze recent 10-K and 10-Q filings for context\n",
        "4. **Recent News**: Gather latest news, earnings reports, and analyst coverage\n",
        "5. **Market Context**: Check broader market conditions, sector performance\n",
        "6. **Peer Comparison**: Compare key metrics against 2-3 competitors\n",
        "7. **Technical Analysis**: Get recent price action, trends, and key levels\n",
        "8. **Analyst Sentiment**: Collect analyst ratings, price targets, and recommendations\n",
        "\n",
        "TOOL USAGE REQUIREMENTS:\n",
        "- Use at least 4 different tools for each analysis\n",
        "- Cross-reference information from multiple sources\n",
        "- If a tool fails, try alternative tools for the same data\n",
        "- Always explain your reasoning between tool calls\n",
        "\n",
        "STOPPING RULE: Once you have basic financials, recent news, and market context, conclude your analysis. Do not seek additional tools or data.\n",
        "\n",
        "**Your goal** is a concise investment overview, not exhaustive research.\n",
        "\n",
        "For {symbol}, provide a comprehensive investment analysis covering all aspects above.\n",
        "Be thorough - this analysis will inform major investment decisions.\n",
        "\"\"\"\n",
        "\n",
        "#Define the agent\n",
        "tools_agent = create_react_agent(\n",
        "    model=llm_openai,\n",
        "    tools=tools,\n",
        "    prompt=THOROUGH_ANALYSIS_PROMPT,\n",
        "    debug=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKVq1h3jfdap",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#For testing\n",
        "#Example:\n",
        "tools_output = tools_agent.invoke({\"messages\": [{'role': 'user', 'content': 'Research Tesla'}]})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "711Sy8xv2v4S"
      },
      "outputs": [],
      "source": [
        "print(tools_output['messages'][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlDIUBsi2v4S"
      },
      "outputs": [],
      "source": [
        "for msg in tools_output['messages']:\n",
        "    print(msg.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "987QsT47jjIH"
      },
      "source": [
        "## Self Reflection & Evaluation Agent\n",
        "Evaluates output quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKhXB6THg4Ua"
      },
      "outputs": [],
      "source": [
        "#Define Self Evaluation agent\n",
        "\n",
        "EVAL_PROMPT = \"\"\"\n",
        "You are an expert evaluator. Your primary job is to give feedback on the analysis below, NOT to overwrite or revise it.\n",
        "\n",
        "Instructions:\n",
        "\n",
        "- Always display the full analysis/summary input *exactly as received* at the start of your answer, clearly labeled.\n",
        "- Provide your commentary (improvement, completeness, feedback) **separately after the full input**.\n",
        "- If human feedback is supplied, include your response to it at the end **without changing the original summary**.\n",
        "\n",
        "FORMAT STRICTLY LIKE THIS:\n",
        "---\n",
        "Original Analysis:\n",
        "{input}\n",
        "\n",
        "Evaluator Commentary:\n",
        "[Your bullet points: Completeness, Succinctness, Accuracy, Clarity, Human Feedback summary, Suggestions, etc.]\n",
        "\n",
        "---\n",
        "\n",
        "Never rewrite or summarize the original analysis. Only provide clear, constructive evaluator commentary after reproducing the input in its original form.\n",
        "\n",
        "--- Human Feedback ---\n",
        "{human_feedback}\n",
        "\n",
        "\n",
        "--- Memory ---\n",
        "{memory_context}\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#Define agent\n",
        "evaluator_agent = create_react_agent(\n",
        "    model=llm_perplexity, # change this to perplexity\n",
        "    tools = [],\n",
        "    prompt=EVAL_PROMPT,\n",
        "    debug=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAfJm-Y22v4S"
      },
      "outputs": [],
      "source": [
        "user_input = \"\"\"\n",
        "    \"input\": (str) Analyze the Apple stock.\n",
        "    \"symbol\": (str) AAPL\n",
        "    \"company_name\": (str) Apple\n",
        "    \"date\": (str) 2025/10/02\n",
        "    \"agent_scratchpad\": (str) \"\",\n",
        "    \"research_notes\": (str) \"\"\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffDQvbfX2v4S"
      },
      "outputs": [],
      "source": [
        "context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "3GOAkUNB2v4S"
      },
      "outputs": [],
      "source": [
        "tools_output = tools_agent.invoke({\"messages\": [{'role': 'user', 'content': 'Research TSLA'}]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isQr231d2v4S"
      },
      "outputs": [],
      "source": [
        "for msg in tools_output['messages']:\n",
        "    print(msg.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dyl1vzqZ2v4S"
      },
      "outputs": [],
      "source": [
        "final_output = list(tools_output.values())[-1][-1].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlnEcnc32v4S"
      },
      "outputs": [],
      "source": [
        "human_feedback = 'you did great kid'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZ-FDRVT2v4T"
      },
      "outputs": [],
      "source": [
        "eval_payload_perplex = {\"messages\": [{\"role\": \"user\", \"content\": str(tools_output)},\n",
        "                                    {\"role\": \"user\", \"content\": human_feedback}\n",
        "                                    ]\n",
        "                       }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_GqlgnO2v4T"
      },
      "outputs": [],
      "source": [
        "from string import Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_0z0N2-2v4T"
      },
      "outputs": [],
      "source": [
        "tmpl = Template(EVAL_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wegJkug2v4T"
      },
      "outputs": [],
      "source": [
        "tmpl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7ZQk9Ug2v4T"
      },
      "outputs": [],
      "source": [
        "filled_prompt = tmpl.safe_substitute(input=final_output,\n",
        "                                     human_feedback=human_feedback or \"[no human feedback provided]\"\n",
        "                                    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0Rm9m1U2v4T"
      },
      "outputs": [],
      "source": [
        "eval_dic = {\n",
        "    \"input\": final_output,\n",
        "    \"human_feedback\": human_feedback\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JA_i3qcG2v4T"
      },
      "outputs": [],
      "source": [
        "print(final_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJmLUhxr2v4T"
      },
      "outputs": [],
      "source": [
        "evaluator_input = f\"\"\"\n",
        "{final_output}\n",
        "\n",
        "--- Human Feedback ---\n",
        "{human_feedback if human_feedback else \"no human feedback provided\"}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wjqq6mUm2v4T"
      },
      "outputs": [],
      "source": [
        "print(full_prompt_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ZdEW1tPr2v4T"
      },
      "outputs": [],
      "source": [
        "eval_output = evaluator_agent.invoke({'messages':full_prompt_text,\n",
        "                                     })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3r05Yg0_2v4T"
      },
      "outputs": [],
      "source": [
        "print(eval_output['messages'][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z48caz402v4T"
      },
      "outputs": [],
      "source": [
        "print(tools_output['messages'][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOBnNHeJl3r-"
      },
      "source": [
        "## Optimization Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3i3SKrv-EEm"
      },
      "outputs": [],
      "source": [
        "#Helper function to make printout look better\n",
        "def print_wrapped(text, width=80): #Limit width to 80 pixels\n",
        "    for line in text.splitlines():\n",
        "        print(textwrap.fill(line, width=width))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSdS1bOg4dXt"
      },
      "outputs": [],
      "source": [
        "#This function runs the agents in an optimization kind of loop, to iterate based on feedback\n",
        "def optimization_loop(tools_agent, evaluator_agent, user_input, session_memory):\n",
        "    #Step 1: Run tools agent\n",
        "    print(\"Conducting research...\")\n",
        "    tools_output = tools_agent.invoke(user_input)\n",
        "    final_output = list(tools_output.values())[-1][-1].content\n",
        "    print(\"\\n--- Analysis Summary ---\")\n",
        "    print_wrapped(str(final_output))\n",
        "\n",
        "    #Step 2: Ask user for feedback\n",
        "    human_feedback = input(\"\\nPlease enter your feedback (areas to improve, missing info, corrections):\\n\")\n",
        "\n",
        "    # grab memory if there is one.\n",
        "    stock_symbol = extract_symbol(final_output)\n",
        "    memory_context = query_memory(stock_symbol, session_memory)\n",
        "\n",
        "\n",
        "    evaluator_input = f\"\"\"\n",
        "                        {final_output}\n",
        "\n",
        "                        --- Human Feedback ---\n",
        "                        {human_feedback if human_feedback else \"no human feedback provided\"}\n",
        "\n",
        "                        --- Memory ---\n",
        "                        {memory_context}\n",
        "                        \"\"\"\n",
        "\n",
        "    #Step 3: Evaluate and revise summary using feedback\n",
        "    eval_payload = {\n",
        "        \"messages\": str(evaluator_input),\n",
        "    }\n",
        "    print(\"\\nRunning evaluator with feedback...\")\n",
        "    revised_output = evaluator_agent.invoke(eval_payload)\n",
        "    final_output = revised_output['messages'][-1].content\n",
        "    print(\"\\n--- Revised Summary ---\")\n",
        "    #print(revised_output)\n",
        "    print_wrapped(str(final_output))\n",
        "\n",
        "    #Optional: Loop for more feedback\n",
        "    while True:\n",
        "        more = input(\"\\nWould you like to refine further? (y/n): \")\n",
        "        if more.lower().startswith(\"y\"):\n",
        "            human_feedback = input(\"Enter any further feedback:\\n\")\n",
        "            eval_payload = {\n",
        "                            \"messages\": str(evaluator_input),\n",
        "                        }\n",
        "\n",
        "            revised_output = evaluator_agent.invoke(eval_payload)\n",
        "            final_output = revised_output['messages'][-1].content\n",
        "            print(\"\\n--- Revised Summary ---\")\n",
        "            print(final_output)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    user_question = as_text(user_input)\n",
        "    symbol = extract_symbol(user_question)\n",
        "    final_answer = as_text(final_output)   # Use your utility function\n",
        "\n",
        "    SESSION_MEMORY.remember(symbol, user_question, final_answer)\n",
        "\n",
        "    print(\"\\nWorkflow complete. Final output above.\")\n",
        "    return revised_output  #Return for pretty print\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNGphX3X6heE",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "#Print final analysis on stock in a pretty format\n",
        "def print_final_analysis(agent_output, title=\"Final Analysis Summary\"):\n",
        "    \"\"\"\n",
        "    Pretty-prints agent outputs including markdown, string, dict, or LangChain message formats.\n",
        "    Displays a title, renders bullet points and headings, and handles line breaks gracefully.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"{title}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    #Helper to render markdown-style output for terminal\n",
        "    def render_markdown(md):\n",
        "        #Render headings\n",
        "        md = re.sub(r\"^### (.+)$\", r\"\\n=== \\1 ===\\n\", md, flags=re.MULTILINE)\n",
        "        md = re.sub(r\"^## (.+)$\", r\"\\n== \\1 ==\\n\", md, flags=re.MULTILINE)\n",
        "        md = re.sub(r\"^# (.+)$\", r\"\\n= \\1 =\\n\", md, flags=re.MULTILINE)\n",
        "        #Replace double newlines with single blank line for separation\n",
        "        md = re.sub(r\"\\n{3,}\", \"\\n\\n\", md)\n",
        "        #Print with blank lines between paragraphs/bullets\n",
        "        for para in md.split('\\n\\n'):\n",
        "            print(para.strip())\n",
        "            print()\n",
        "\n",
        "    #Handle string output\n",
        "    if isinstance(agent_output, str):\n",
        "        render_markdown(agent_output)\n",
        "        return\n",
        "\n",
        "    #Handle dict outputs (common in LangChain)\n",
        "    if isinstance(agent_output, dict):\n",
        "        #Look for 'messages' (LangChain) or 'output'\n",
        "        if \"messages\" in agent_output:\n",
        "            messages = agent_output[\"messages\"]\n",
        "            for idx, msg in enumerate(messages):\n",
        "                content = getattr(msg, \"content\", str(msg))\n",
        "                if len(messages) > 1:\n",
        "                    print(f\"Message {idx+1}:\")\n",
        "                render_markdown(content)\n",
        "        elif \"output\" in agent_output and isinstance(agent_output[\"output\"], str):\n",
        "            render_markdown(agent_output[\"output\"])\n",
        "        else:\n",
        "            #Generic dict pretty-print\n",
        "            for k, v in agent_output.items():\n",
        "                print(f\"{k}:\")\n",
        "                render_markdown(str(v))\n",
        "        return\n",
        "\n",
        "    #Handle LangChain AIMessage or other objects with .content\n",
        "    content = getattr(agent_output, \"content\", None)\n",
        "    if content:\n",
        "        render_markdown(content)\n",
        "        return\n",
        "\n",
        "    #Fallback if above doesn't work\n",
        "    print(agent_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_CeRDyhjmUp"
      },
      "source": [
        "## Learning/Memory Agent\n",
        "Maintains memory across analysis runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_7UNagLGt60",
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "#MEMORY AGENT CELL 1\n",
        "#Session-scoped memory\n",
        "\n",
        "@dataclass\n",
        "class MemoryItem:\n",
        "    symbol: str\n",
        "    question: str\n",
        "    answer: str\n",
        "    created_at: str\n",
        "    meta: Dict[str, Any]\n",
        "\n",
        "class SessionMemory:\n",
        "    def __init__(self, max_items: int = 200, max_per_symbol: int = 10):\n",
        "        self._store: Dict[str, List[MemoryItem]] = {}\n",
        "        self.max_items = max_items\n",
        "        self.max_per_symbol = max_per_symbol\n",
        "\n",
        "    def remember(self, symbol: str, question: str, answer: str, **meta) -> None:\n",
        "        symbol = (symbol or \"GENERIC\").upper().strip()\n",
        "        item = MemoryItem(\n",
        "            symbol=symbol,\n",
        "            question=(question or \"\").strip(),\n",
        "            answer=(answer or \"\").strip(),\n",
        "            created_at=datetime.utcnow().isoformat(timespec=\"seconds\"),\n",
        "            meta=meta or {}\n",
        "        )\n",
        "        bucket = self._store.setdefault(symbol, [])\n",
        "        bucket.append(item)\n",
        "        if len(bucket) > self.max_per_symbol:\n",
        "            del bucket[0 : len(bucket) - self.max_per_symbol]\n",
        "        self._cap_global()\n",
        "\n",
        "    def recall(self, symbol: str, question: Optional[str] = None) -> Optional[str]:\n",
        "        symbol = (symbol or \"GENERIC\").upper().strip()\n",
        "        bucket = self._store.get(symbol, [])\n",
        "        if not bucket:\n",
        "            return None\n",
        "        if not question:\n",
        "            return bucket[-1].answer\n",
        "        q = (question or \"\").strip()\n",
        "        for item in reversed(bucket):\n",
        "            if item.question == q:\n",
        "                return item.answer\n",
        "        return None\n",
        "\n",
        "    def latest(self, symbol: str) -> Optional[MemoryItem]:\n",
        "        symbol = (symbol or \"GENERIC\").upper().strip()\n",
        "        bucket = self._store.get(symbol, [])\n",
        "        return bucket[-1] if bucket else None\n",
        "\n",
        "    def _cap_global(self):\n",
        "        all_items = []\n",
        "        for sym, bucket in self._store.items():\n",
        "            for it in bucket:\n",
        "                all_items.append((it.created_at, sym, it))\n",
        "        if len(all_items) <= self.max_items:\n",
        "            return\n",
        "        all_items.sort(key=lambda x: x[0])  # oldest first\n",
        "        to_drop = len(all_items) - self.max_items\n",
        "        cutoff = set(id(it) for _, _, it in all_items[:to_drop])\n",
        "        for sym in list(self._store.keys()):\n",
        "            self._store[sym] = [it for it in self._store[sym] if id(it) not in cutoff]\n",
        "\n",
        "\n",
        "\n",
        "def extract_symbol(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Grab a likely ticker from the user_input like 'Analyze the SPY stock ticker'.\n",
        "    Simple heuristic: first ALL-CAPS token 1-5 chars (e.g., AAPL, MSFT, SPY).\n",
        "    Falls back to 'GENERIC' if none found.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"GENERIC\"\n",
        "    candidates = re.findall(r\"\\b[A-Z]{1,5}\\b\", text)\n",
        "    # Light filter for common English words\n",
        "    stop = {\"THE\",\"AND\",\"FOR\",\"WITH\",\"FROM\",\"THIS\",\"THAT\",\"YOUR\",\"HAVE\",\"HOLD\"}\n",
        "    for c in candidates:\n",
        "        if c not in stop:\n",
        "            return c\n",
        "    return \"GENERIC\"\n",
        "\n",
        "def as_text(x: Any) -> str:\n",
        "    \"\"\"\n",
        "    Normalize whatever comes back from planner/tools/evaluator/optimizer into a string.\n",
        "    Works with LangChain AgentExecutor outputs (dict), AIMessage, or raw str.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # AIMessage / ChatMessage\n",
        "        if hasattr(x, \"content\"):\n",
        "            return str(x.content)\n",
        "        # Agent-like dicts\n",
        "        if isinstance(x, dict):\n",
        "            if \"output\" in x and isinstance(x[\"output\"], str):\n",
        "                return x[\"output\"]\n",
        "            if \"messages\" in x and isinstance(x[\"messages\"], list):\n",
        "                return \"\\n\\n\".join(\n",
        "                    (m.content if hasattr(m, \"content\") else str(m))\n",
        "                    for m in x[\"messages\"]\n",
        "                )\n",
        "        # plain string\n",
        "        if isinstance(x, str):\n",
        "            return x\n",
        "        return str(x)\n",
        "    except Exception:\n",
        "        return str(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPxekd7Rj2KW"
      },
      "source": [
        "## Execution Function, Set The Agent Free"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p-12fCY2v4U"
      },
      "outputs": [],
      "source": [
        "SESSION_MEMORY = SessionMemory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRPTUlph4xdI",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#Execution function\n",
        "final_analysis = optimization_loop(tools_agent, evaluator_agent, context, SESSION_MEMORY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpCJ-A-76-FD",
        "jupyter": {
          "source_hidden": true
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print_final_analysis(final_analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTu73fn72wD3",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#Ensure the memory is working as intended\n",
        "#After running the user query through your whole pipeline:\n",
        "user_question = user_input\n",
        "symbol = extract_symbol(user_input)\n",
        "final_answer = as_text(final_analysis)   # Use your utility function\n",
        "\n",
        "SESSION_MEMORY.remember(symbol, user_question, final_answer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqKXdKz62v4U"
      },
      "outputs": [],
      "source": [
        "def query_memory(stock_symbol, memory):\n",
        "    print('query memory stock symbol = ', stock_symbol)\n",
        "    if stock_symbol in memory._store:\n",
        "        recent_memory = memory._store[stock_symbol][-1].answer\n",
        "        return recent_memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU8f7LYK2v4U"
      },
      "outputs": [],
      "source": [
        "print(SESSION_MEMORY._store['AAPL'][0].answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXOR6HKY2v4U"
      },
      "outputs": [],
      "source": [
        "# Later, you can recall the latest answer for \"AAPL\":\n",
        "prev = SESSION_MEMORY.recall(\"AAPL\")\n",
        "if prev:\n",
        "    print(\"Previous answer for AAPL:\", prev)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "n_CeRDyhjmUp"
      ],
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}