{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kvnhooman/AAI520_Final-Project_Group5/blob/tommy-dev-1/Financial_Review_AI_Agents_v_10_7_2200.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z34tfGYWXK7X"
      },
      "source": [
        "# Title\n",
        "**Group 5**\n",
        "\n",
        "Final Project\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pss-SJpmYEdY"
      },
      "source": [
        "___\n",
        "## Outline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au9v6SfkXl5p"
      },
      "source": [
        "**Core Agent Functions**\n",
        "\n",
        "\n",
        "* Tool Use Agent: Integrates APIs (Yahoo Finance, SEC EDGAR, News APIs) and Specialists\n",
        "\n",
        "\n",
        "* Self-Reflection Agent: Evaluates output quality and iterates\n",
        "\n",
        "\n",
        "* Memory/Learning Agent: Maintains memory across analysis runs\n",
        "\n",
        "\n",
        "**Workflow Patterns**\n",
        "* Prompt Chaining: News ingestion → preprocessing → classification → extraction → summarization\n",
        "\n",
        "\n",
        "* Routing: Directs content to specialist analyzers (earnings, news, market)\n",
        "\n",
        "\n",
        "* Evaluator-Optimizer: Generates analysis → evaluates quality → refines using feedback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "p_jPSJnIXKke",
        "outputId": "57b91d11-5d8f-482d-bf00-9ef6fd27a381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.35-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.12-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.77)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.31)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.9)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
            "  Downloading langchain_core-0.3.78-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.11.0)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Collecting google-ai-generativelanguage<1,>=0.7 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.7.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting filetype<2,>=1.2 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.75.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (0.6.1)\n",
            "Downloading langchain_openai-0.3.35-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.31-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-2.1.12-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.7.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.78-py3-none-any.whl (449 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.6/449.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: filetype, requests, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-core, langchain-openai, google-ai-generativelanguage, langchain-google-genai, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.77\n",
            "    Uninstalling langchain-core-0.3.77:\n",
            "      Successfully uninstalled langchain-core-0.3.77\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 filetype-1.2.0 google-ai-generativelanguage-0.7.0 langchain-community-0.3.31 langchain-core-0.3.78 langchain-google-genai-2.1.12 langchain-openai-0.3.35 marshmallow-3.26.1 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "bc83852cc1e84c34a61530c60a08ea80"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.35.3)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Collecting openai\n",
            "  Downloading openai-2.2.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
            "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.183.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.10)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.75.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-2.2.0-py3-none-any.whl (998 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m999.0/999.0 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai, google-ai-generativelanguage\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.109.1\n",
            "    Uninstalling openai-1.109.1:\n",
            "      Successfully uninstalled openai-1.109.1\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.7.0\n",
            "    Uninstalling google-ai-generativelanguage-0.7.0:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-google-genai 2.1.12 requires google-ai-generativelanguage<1,>=0.7, but you have google-ai-generativelanguage 0.6.15 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-ai-generativelanguage-0.6.15 openai-2.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "20c1e079295e433f9db2676679cf4de4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.12/dist-packages (0.2.66)\n",
            "Collecting duckduckgo-search\n",
            "  Downloading duckduckgo_search-8.1.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.32.5)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.0.12)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.4.0)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.12/dist-packages (from yfinance) (3.18.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.13.5)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.13.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (5.29.5)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (8.3.0)\n",
            "Collecting primp>=0.15.0 (from duckduckgo-search)\n",
            "  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (5.4.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.15.0)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (2.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
            "Downloading duckduckgo_search-8.1.1-py3-none-any.whl (18 kB)\n",
            "Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: primp, duckduckgo-search\n",
            "Successfully installed duckduckgo-search-8.1.1 primp-0.15.0\n",
            "Collecting ddgs\n",
            "  Downloading ddgs-9.6.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from ddgs) (8.3.0)\n",
            "Requirement already satisfied: primp>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from ddgs) (0.15.0)\n",
            "Collecting lxml>=6.0.0 (from ddgs)\n",
            "  Downloading lxml-6.0.2-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.16.0)\n",
            "Requirement already satisfied: brotli in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.1.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.3.0)\n",
            "Collecting socksio==1.* (from httpx[brotli,http2,socks]>=0.28.1->ddgs)\n",
            "  Downloading socksio-1.0.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.15.0)\n",
            "Downloading ddgs-9.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading socksio-1.0.0-py3-none-any.whl (12 kB)\n",
            "Downloading lxml-6.0.2-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: socksio, lxml, ddgs\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 5.4.0\n",
            "    Uninstalling lxml-5.4.0:\n",
            "      Successfully uninstalled lxml-5.4.0\n",
            "Successfully installed ddgs-9.6.0 lxml-6.0.2 socksio-1.0.0\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.12.0\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.6.9-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.78)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.2-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.11.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.4.31)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.1)\n",
            "Downloading langgraph-0.6.9-py3-none-any.whl (156 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.3/156.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed langgraph-0.6.9 langgraph-checkpoint-2.1.2 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.9 ormsgpack-1.10.0\n",
            "Collecting restrictedpython\n",
            "  Downloading RestrictedPython-8.0-py3-none-any.whl.metadata (14 kB)\n",
            "Downloading RestrictedPython-8.0-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: restrictedpython\n",
            "Successfully installed restrictedpython-8.0\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (4.13.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (2.32.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.8.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->wikipedia) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->wikipedia) (4.15.0)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=b9ca45e585fce9ebedd4be531182429a7e0240fd91e833ec160263dd51cee3fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/47/7c/a9688349aa74d228ce0a9023229c6c0ac52ca2a40fe87679b8\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "# Install dependencis\n",
        "# Attribution: Geek for Geeks tutorial - https://www.geeksforgeeks.org/artificial-intelligence/introduction-to-langchain/\n",
        "\n",
        "# -- CORE LANGCHAIN AND PLUGINS --\n",
        "!pip install -U langchain langchain-openai langchain-community langchain-google-genai\n",
        "\n",
        "# -- LARGE MODEL PROVIDERS/SKILL ADAPTERS --\n",
        "!pip install -U google-generativeai huggingface_hub openai\n",
        "\n",
        "# -- COMMON TOOLING AND UTILITIES --\n",
        "!pip install -U python-dotenv yfinance duckduckgo-search\n",
        "\n",
        "# -- DUCKDUCKGO SEARCH API WRAPPER --\n",
        "!pip install -U ddgs\n",
        "\n",
        "# -- OPTIONAL: For advanced memory (semantic search/vector db) --\n",
        "!pip install -U faiss-cpu  # For in-memory vector DBs (Lightweight)\n",
        "# If you want persistent/production memory, add chromadb or qdrant-client\n",
        "\n",
        "# -- Install LangGraph for advanced agent orchestration --\n",
        "!pip install -U langgraph\n",
        "\n",
        "# -- (OPTIONAL) For running Python tool actions securely --\n",
        "!pip install -U restrictedpython\n",
        "\n",
        "# -- Wikipedia\n",
        "!pip install wikipedia\n",
        "\n",
        "# Restart the runtime after running this cell if prompted!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zeCeqrXeXdDA"
      },
      "outputs": [],
      "source": [
        "#Import key libraries\n",
        "import os\n",
        "import requests\n",
        "import re\n",
        "import textwrap\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain_community.chat_models import ChatPerplexity\n",
        "\n",
        "from langchain_openai import OpenAI, ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts.chat import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.agents import create_react_agent, AgentExecutor, initialize_agent, Tool, AgentType\n",
        "from langchain_core.tools import tool\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.tools import tool  #Newer import for @tool decorator\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "#Correct import path for YahooFinanceAPI\n",
        "import yfinance as yf\n",
        "from langchain_community.tools.yahoo_finance_news import YahooFinanceNewsTool\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "#Memory\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Any, Union"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SVdqL2i_ZTB0"
      },
      "outputs": [],
      "source": [
        "#Set up LLM API Calls\n",
        "gemini_key = userdata.get('GEMINI')\n",
        "hf_key = userdata.get('HF_TOKEN')\n",
        "openai_key = userdata.get('OPENAI')\n",
        "fin_news = userdata.get('FIN_API_KEY')\n",
        "tavily_key = userdata.get('TAVILY_API_KEY')\n",
        "perplexity_key = userdata.get('PERPLEXITY_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jlClLEzOAxjy"
      },
      "outputs": [],
      "source": [
        "# Setup Gemini to use in Agents\n",
        "llm_gemini = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    google_api_key=gemini_key\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "m95A2uEwHt4O"
      },
      "outputs": [],
      "source": [
        "llm_openai = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",  # \"gpt-3.5-turbo\", \"gpt-4-mini\", \"gpt-5-mini\" ...\n",
        "    openai_api_key=openai_key,  # Your OpenAI API key\n",
        "    temperature=0.0             # (optional) set as needed\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_perplexity = ChatPerplexity(\n",
        "    model=\"sonar-pro\", #Other model options\n",
        "    pplx_api_key = perplexity_key,\n",
        "    temperature=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "gKbuO_UUH3hO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-perplexity"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd9mMhbYLg6e",
        "outputId": "df8a183d-c3e7-4f6d-e030-97c5b669a926"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-perplexity in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.71 in /usr/local/lib/python3.12/dist-packages (from langchain-perplexity) (0.3.78)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.97.1 in /usr/local/lib/python3.12/dist-packages (from langchain-perplexity) (1.109.1)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (0.4.31)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (1.33)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (2.11.9)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.97.1->langchain-perplexity) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.97.1->langchain-perplexity) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.97.1->langchain-perplexity) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.97.1->langchain-perplexity) (0.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.97.1->langchain-perplexity) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.97.1->langchain-perplexity) (4.67.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.97.1->langchain-perplexity) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.97.1->langchain-perplexity) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.97.1->langchain-perplexity) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.97.1->langchain-perplexity) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<1.0.0,>=0.3.71->langchain-perplexity) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu6ALnCIHxr9",
        "outputId": "183b1042-7672-4400-cf16-cc9980cf0854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural networks process data through interconnected layers to learn patterns[1][2][3][6][7].\n"
          ]
        }
      ],
      "source": [
        "#Test OpenAI API\n",
        "#response = llm_openai.invoke(\"Explain how neural networks work in 10 words or less.\")\n",
        "#response = llm_gemini.invoke(\"Explain how neural networks work in 10 words or less.\")\n",
        "response = llm_perplexity.invoke(\"Explain how neural networks work in 10 words or less.\")\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User Input Context Extractor Agent"
      ],
      "metadata": {
        "id": "2iVDzqg0gaPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-----Simply outlining this here, we can build an ingestion agent or function if we have time\n",
        "user_input = \"\"\"\n",
        "    \"input\": (str) Analyze the Apple stock.\n",
        "    \"symbol\": (str) AAPL\n",
        "    \"company_name\": (str) Apple\n",
        "    \"date\": (str) 2025/10/07\n",
        "    \"agent_scratchpad\": (str) \"\",\n",
        "    \"research_notes\": (str) \"\"\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "XlJHg7QSJIhS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function on input\n",
        "def extract_context(user_input):\n",
        "    \"\"\"\n",
        "    Extracts key context fields from user input for agent API use.\n",
        "\n",
        "    Fields:\n",
        "      - input (str): raw user text\n",
        "      - symbol (str): detected uppercase ticker (2-5 chars)\n",
        "      - company_name (str): placeholder for future NLP extraction\n",
        "      - date (str): today's date in ISO format (YYYY-MM-DD)\n",
        "      - agent_scratchpad (str): empty, reserved for intermediate agent notes\n",
        "      - research_notes (str): empty, reserved for additional notes\n",
        "    \"\"\"\n",
        "    # Extract uppercase ticker symbol (2-5 chars)\n",
        "    match = re.search(r\"\\b([A-Z]{2,5})\\b\", user_input)\n",
        "    symbol = match.group(1) if match else \"\"\n",
        "\n",
        "    # Placeholder for company name extraction\n",
        "    company_name = \"\"  # Optional: implement NER for this\n",
        "\n",
        "    # Today's date in ISO format\n",
        "    today = datetime.now().date().isoformat()\n",
        "\n",
        "    # Build context dictionary\n",
        "    context = {\n",
        "        'input': user_input,\n",
        "        'symbol': symbol,\n",
        "        'company_name': company_name,\n",
        "        'date': today,\n",
        "        'agent_scratchpad': '',\n",
        "        'research_notes': ''\n",
        "    }\n",
        "    return context"
      ],
      "metadata": {
        "id": "AfRwYIsOghzz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Input extraction output\n",
        "context = extract_context(user_input)\n",
        "print(context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJQEJBhC3M3E",
        "outputId": "09c78b28-c1f2-4351-c095-2480d8e3b1f4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': '\\n    \"input\": (str) Analyze the Apple stock.\\n    \"symbol\": (str) AAPL\\n    \"company_name\": (str) Apple\\n    \"date\": (str) 2025/10/07\\n    \"agent_scratchpad\": (str) \"\",\\n    \"research_notes\": (str) \"\"\\n    ', 'symbol': 'AAPL', 'company_name': '', 'date': '2025-10-08', 'agent_scratchpad': '', 'research_notes': ''}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7IzZtQ0i_p0"
      },
      "source": [
        "## Tool Agent\n",
        "Integrates APIs (Yahoo Finance, SEC EDGAR, News APIs)\n",
        "\n",
        "List of Tools:\n",
        "- Web Search\n",
        "- API call - Yahoo Finance\n",
        "- to be updated..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz8YEnt9jfua"
      },
      "source": [
        "### Tool Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "OFCPTCNfkIit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d0f3d55-e38b-4310-bbae-8b3c21d14e9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "#Yahoo Finance Tool\n",
        "def get_stock_price(ticker: str) -> str:\n",
        "    try:\n",
        "        price = yf.Ticker(ticker).info['regularMarketPrice']\n",
        "        return f\"{ticker} price is {price}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching price for {ticker}: {e}\"\n",
        "\n",
        "\n",
        "yahoo_api_tool = Tool(\n",
        "    name=\"YahooFinanceAPI\",\n",
        "    func=get_stock_price,\n",
        "    description=\"Queries Yahoo Finance for stock price and financials\"\n",
        ")\n",
        "\n",
        "\n",
        "#SEC Filings\n",
        "def get_sec_filings(ticker: str) -> str:\n",
        "    cik_url = \"https://www.sec.gov/files/company_tickers.json\"\n",
        "    headers = {\"User-Agent\": \"tpool@sandiego.edu\"}  # Use your real email!\n",
        "    cik_resp = requests.get(cik_url, headers=headers)\n",
        "    print(f\"cik_resp: {cik_resp.status_code}, {cik_resp.text[:100]}\")\n",
        "\n",
        "\n",
        "    if cik_resp.status_code != 200:\n",
        "        return f\"SEC.gov rejected our request: {cik_resp.status_code}\\n{cik_resp.text[:200]}\"\n",
        "\n",
        "\n",
        "    try:\n",
        "        cik_data = cik_resp.json()\n",
        "        cik_lookup = {item['ticker']: item['cik_str'] for item in cik_data.values()}\n",
        "        cik = cik_lookup.get(ticker.upper())\n",
        "    except Exception as e:\n",
        "        return f\"Error parsing CIK data: {e}\"\n",
        "\n",
        "\n",
        "    if not cik:\n",
        "        return f\"CIK for {ticker} not found.\"\n",
        "\n",
        "\n",
        "    filings_url = f\"https://data.sec.gov/submissions/CIK{cik:0>10}.json\"\n",
        "    filings_resp = requests.get(filings_url, headers=headers)\n",
        "\n",
        "\n",
        "    try:\n",
        "        data = filings_resp.json() if filings_resp.status_code == 200 else {}\n",
        "        filings = data.get('filings', {}).get('recent', {})\n",
        "        if filings:\n",
        "            forms = filings.get('form', [])[:3]\n",
        "            filing_dates = filings.get('filingDate', [])[:3] #If successful, get filing dates\n",
        "            filing_links = filings.get('primaryDocument', [])[:3] #Also get three primary documents for agent\n",
        "            result = []\n",
        "            for f, d, l in zip(forms, filing_dates, filing_links):\n",
        "                result.append(f\"{f} on {d}: {l}\")\n",
        "            return f\"Latest filings for {ticker}:\\n\" + \"\\n\".join(result)\n",
        "        else:\n",
        "            return f\"No filings found for {ticker}.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error loading filings for {ticker}: {e}\"\n",
        "\n",
        "\n",
        "\n",
        "sec_api_tool = Tool(\n",
        "    name=\"SECEDGARAPI\",\n",
        "    func=get_sec_filings,\n",
        "    description=\"Retrieves SEC filings on stock symbol\"\n",
        ")\n",
        "\n",
        "#Finanical news lookup\n",
        "def get_fin_news(symbol: str) -> str:\n",
        "    api_key = fin_news\n",
        "    url = \"https://newsapi.org/v2/everything\"\n",
        "    params = {\n",
        "        \"q\": symbol,\n",
        "        \"apiKey\": api_key,\n",
        "        \"sortBy\": \"publishedAt\",\n",
        "        \"language\": \"en\"\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    if response.status_code != 200:\n",
        "        return f\"API error {response.status_code}: {response.text[:200]}\"\n",
        "    data = response.json()\n",
        "    articles = data.get(\"articles\", [])\n",
        "    if not articles:\n",
        "        return f\"No news found for {symbol}. Full message: {data.get('message', '')}\"\n",
        "    return \"\\n\".join([a[\"description\"] or a[\"title\"] for a in articles[:3]])\n",
        "\n",
        "\n",
        "\n",
        "news_api_tool = Tool(\n",
        "    name=\"NewsAPI\",\n",
        "    func=get_fin_news,  #Assumes you've defined this class\n",
        "    description=\"Finds recent financial news on stock symbol\"\n",
        ")\n",
        "\n",
        "#Tavily for websearching\n",
        "def get_fin_news_tavily(symbol: str) -> str:\n",
        "    api_key = tavily_key\n",
        "    url = \"https://api.tavily.com/search\"\n",
        "    # You can optimize the query for financial news by including keywords:\n",
        "    query = f\"{symbol} financial news\"\n",
        "    payload = {\n",
        "        \"query\": query,\n",
        "        \"api_key\": api_key,\n",
        "        \"max_results\": 3,  # You can set number of results as you prefer\n",
        "    }\n",
        "    response = requests.post(url, json=payload)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        results = data.get(\"results\", [])\n",
        "        # Each result contains title, link, snippet, etc.\n",
        "        return \"\\n\".join([r.get(\"title\", \"No title\") for r in results]) if results else \"No news found.\"\n",
        "    else:\n",
        "        return f\"Error from Tavily: {response.status_code} {response.text}\"\n",
        "\n",
        "\n",
        "tavily_news_tool = Tool(\n",
        "    name=\"TavilyNewsSearch\",\n",
        "    func=get_fin_news_tavily,\n",
        "    description=\"Searches web for recent financial news about a stock symbol using Tavily API.\"\n",
        ")\n",
        "\n",
        "\n",
        "#Wikipedia Search Tool\n",
        "from wikipedia import summary\n",
        "\n",
        "\n",
        "def search_wikipedia(query: str) -> str:\n",
        "    # If query looks like a ticker (e.g., all caps, 2-5 chars), get company name\n",
        "    if query.isupper() and 2 <= len(query) <= 5:\n",
        "        company_name = get_company_name(query)\n",
        "        search_term = f\"{company_name} stock\"\n",
        "    else:\n",
        "        search_term = query\n",
        "\n",
        "\n",
        "    try:\n",
        "        return summary(search_term, sentences=2)\n",
        "    except Exception:\n",
        "        # Fallback, try just company name (without \"stock\")\n",
        "        if search_term != query:\n",
        "            try:\n",
        "                return summary(company_name, sentences=2)\n",
        "            except Exception:\n",
        "                pass\n",
        "        return \"I couldn't find any information on that.\"\n",
        "\n",
        "\n",
        "wikipedia_tool = Tool(\n",
        "    name=\"WikipediaSearch\",\n",
        "    func=search_wikipedia,\n",
        "    description=\"Searches Wikipedia and returns a summary for a stock symbol or company name.\"\n",
        ")\n",
        "\n",
        "\n",
        "#Sentiment analysis tool\n",
        "class HuggingFaceSentimentTool:\n",
        "    def __init__(self):\n",
        "        self.classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "\n",
        "    def analyze(self, text):\n",
        "        result = self.classifier(text)[0]\n",
        "        # result['label'] is 'POSITIVE' or 'NEGATIVE'\n",
        "        return 1 if result['label'] == \"POSITIVE\" else -1\n",
        "\n",
        "\n",
        "sentiment_api_tool = Tool (\n",
        "    name=\"HuggingFaceSentiment\",\n",
        "    func=HuggingFaceSentimentTool().analyze,\n",
        "    description=\"Analyzes sentiment of text using HuggingFace\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "#Analyzer specialist tool\n",
        "class EarningsAnalyzer:\n",
        "    def __init__(self, yahoo_tool, sec_tool):\n",
        "        self.yahoo_tool = yahoo_tool\n",
        "        self.sec_tool = sec_tool\n",
        "\n",
        "\n",
        "    def analyze(self, symbol, messages=None):\n",
        "        #Try to find earnings info in past messages\n",
        "        yahoo_data = None\n",
        "        if messages:\n",
        "            for msg in reversed(messages):\n",
        "                # ToolMessage expected structure\n",
        "                # Check by tool name and relevant symbol\n",
        "                if (\n",
        "                    hasattr(msg, \"name\") and msg.name == \"YahooFinanceAPI\"\n",
        "                    and hasattr(msg, \"content\") and symbol in msg.content\n",
        "                ):\n",
        "                    yahoo_data = msg.content  # or parse accordingly\n",
        "                    break\n",
        "        #If not found, call API\n",
        "        if yahoo_data is None:\n",
        "            yahoo_data = self.yahoo_tool.fetch_earnings(symbol)\n",
        "\n",
        "earnings_specialist = Tool (\n",
        "    name=\"EarningsAnalyzer\",\n",
        "    func=EarningsAnalyzer(yahoo_api_tool, sec_api_tool).analyze,\n",
        "    description=\"Analyzes earnings of a stock symbol\"\n",
        ")\n",
        "\n",
        "\n",
        "class SentimentAnalyzer:\n",
        "    def __init__(self, sentiment_tool, news_tool):\n",
        "        self.sentiment_tool = sentiment_tool  # expects .analyze(text)\n",
        "        self.news_tool = news_tool            # expects .get_news(symbol)\n",
        "\n",
        "\n",
        "    def analyze(self, symbol, messages=None):\n",
        "        #Find any recent news/sentiment in agent messages\n",
        "        headlines = []\n",
        "        if messages:\n",
        "            for msg in reversed(messages):\n",
        "                # If you have ToolMessage for news and symbol in content\n",
        "                if hasattr(msg, \"name\") and msg.name in [\"NewsAPI\", \"TavilyNewsSearch\"]:\n",
        "                    if hasattr(msg, \"content\") and symbol in msg.content:\n",
        "                        # Try splitting headlines (change this parsing as needed)\n",
        "                        headlines += msg.content.split(\"\\n\")\n",
        "                # If you cache sentiment scores/results, you can parse those too!\n",
        "\n",
        "\n",
        "        #If no headlines found, call news API/tool\n",
        "        if not headlines:\n",
        "            news_items = self.news_tool.get_news(symbol)\n",
        "            headlines = [item['headline'] for item in news_items]\n",
        "\n",
        "\n",
        "        #Get sentiment scores for each headline\n",
        "        if headlines:\n",
        "            scores = [self.sentiment_tool.analyze(h) for h in headlines]\n",
        "            if scores:\n",
        "                mean_score = sum(scores) / len(scores)\n",
        "                recommendation = self._interpret_score(mean_score)\n",
        "                return (f\"Average news sentiment for {symbol}: {mean_score:.2f}\\n\"\n",
        "                        f\"Recommendation: {recommendation}\")\n",
        "            else:\n",
        "                return \"No sentiment scores could be computed.\"\n",
        "        else:\n",
        "            return \"No recent news headlines found.\"\n",
        "\n",
        "\n",
        "    def _interpret_score(self, score):\n",
        "        #Customize thresholds to your model's scoring scale\n",
        "        if score > 0.2:\n",
        "            return \"Buy\"\n",
        "        elif score > -0.2:\n",
        "            return \"Hold\"\n",
        "        else:\n",
        "            return \"Sell\"\n",
        "\n",
        "\n",
        "sentiment_anaylsis_specialist = Tool(\n",
        "    name=\"SentimentAnalyzer\",\n",
        "    func=lambda symbol, messages: SentimentAnalyzer(sentiment_api_tool, news_api_tool).analyze(symbol, messages=messages),\n",
        "    description=\"Aggregates news sentiment for a stock and suggests buy/hold/sell.\"\n",
        ")\n",
        "\n",
        "\n",
        "#Perplexity search tool\n",
        "def perplexity_search(query: str) -> str:\n",
        "    api_key = perplexity_key\n",
        "    url = \"https://api.perplexity.ai/v1/search\"\n",
        "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "    params = {\"q\": query, \"num_results\": 3}\n",
        "    response = requests.get(url, headers=headers, params=params)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        return \"\\n\".join([item.get(\"snippet\", \"No snippet\") for item in data.get(\"results\", [])])\n",
        "    else:\n",
        "        return f\"Error from Perplexity: {response.status_code} {response.text}\"\n",
        "\n",
        "perplexity_search_tool = Tool(\n",
        "    name=\"PerplexitySearch\",\n",
        "    func=perplexity_search,\n",
        "    description=\"Searches the web using Perplexity AI engine.\"\n",
        ")\n",
        "\n",
        "#FRED Scrape\n",
        "def get_fred_series_web(series_id: str) -> str:\n",
        "    url = f\"https://fred.stlouisfed.org/series/{series_id}\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code != 200:\n",
        "            return f\"Failed to load FRED page [{response.status_code}]\"\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        # Try to extract the latest value from the series chart metadata\n",
        "        latest = soup.select_one(\".series-meta-observation-value\")\n",
        "        date = soup.select_one(\".series-meta-observation-date\")\n",
        "        if latest and date:\n",
        "            latest_val = latest.text.strip()\n",
        "            latest_date = date.text.strip()\n",
        "            return f\"{series_id} latest value: {latest_val} ({latest_date})\"\n",
        "        # Fallback: Try alternative selectors or extraction\n",
        "        # For some series, the value is in an element with class=\"meta-value\"\n",
        "        alt_latest = soup.find(\"span\", class_=\"meta-value\")\n",
        "        if alt_latest:\n",
        "            return f\"{series_id} recent value: {alt_latest.text.strip()}\"\n",
        "        return f\"Could not find observation value for series '{series_id}' on FRED site.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching FRED data: {e}\"\n",
        "\n",
        "fred_api_tool = Tool(\n",
        "    name=\"FREDScrapeTool\",\n",
        "    func=get_fred_series_web,\n",
        "    description=\"Scrapes the open FRED site for latest data value of a given series (e.g., GDP, CPI, etc.)\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For debugging\n",
        "#print(get_sec_filings(\"AAPL\"))"
      ],
      "metadata": {
        "id": "IdTf_zXh_wd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "biMVj3ifg3ha"
      },
      "outputs": [],
      "source": [
        "#Create the tools list\n",
        "tools = [\n",
        "    yahoo_api_tool,\n",
        "    sec_api_tool,\n",
        "    news_api_tool,\n",
        "    tavily_news_tool,\n",
        "    wikipedia_tool,\n",
        "    sentiment_api_tool,\n",
        "    earnings_specialist,\n",
        "    sentiment_anaylsis_specialist,\n",
        "    wikipedia_tool,\n",
        "    perplexity_search_tool,\n",
        "    fred_api_tool\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_EbjC2ok3m4"
      },
      "source": [
        "### Tools Agent Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "wyMhkBfhf3Hi"
      },
      "outputs": [],
      "source": [
        "#Tools Agent Prompt & Function\n",
        "\n",
        "#Define the prompt, balaning using as many tools as possible without breaking recursion limits\n",
        "THOROUGH_ANALYSIS_PROMPT = \"\"\"\n",
        "You are a comprehensive financial research agent. You MUST use multiple tools to conduct deep analysis.\n",
        "\n",
        "MANDATORY RESEARCH STEPS - Complete ALL of these:\n",
        "\n",
        "1. **Company Overview**: Get basic company information, sector, and key metrics\n",
        "2. **Financial Data**: Retrieve latest quarterly/annual financials (revenue, profit, cash flow)\n",
        "3. **SEC Filings**: Download and analyze recent 10-K and 10-Q filings for context\n",
        "4. **Recent News**: Gather latest news, earnings reports, and analyst coverage\n",
        "5. **Market Context**: Check broader market conditions, sector performance\n",
        "6. **Peer Comparison**: Compare key metrics against 2-3 competitors\n",
        "7. **Technical Analysis**: Get recent price action, trends, and key levels\n",
        "8. **Analyst Sentiment**: Collect analyst ratings, price targets, and recommendations\n",
        "\n",
        "TOOL USAGE REQUIREMENTS:\n",
        "- Use at least 4 different tools for each analysis\n",
        "- Cross-reference information from multiple sources\n",
        "- If a tool fails, try alternative tools for the same data\n",
        "- Always explain your reasoning between tool calls\n",
        "\n",
        "STOPPING RULE: Once you have basic financials, recent news, and market context, conclude your analysis. Do not seek additional tools or data.\n",
        "\n",
        "**Your goal** is a concise investment overview, not exhaustive research.\n",
        "\n",
        "For {symbol}, provide a comprehensive investment analysis covering all aspects above.\n",
        "Be thorough - this analysis will inform major investment decisions.\n",
        "\"\"\"\n",
        "\n",
        "#Define the agent\n",
        "tools_agent = create_react_agent(\n",
        "    model=llm_openai,\n",
        "    tools=tools,\n",
        "    prompt=THOROUGH_ANALYSIS_PROMPT,\n",
        "    debug=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For testing\n",
        "#Example:\n",
        "#tools_output = tools_agent.invoke({\"messages\": [{'role': 'user', 'content': 'Research TSLA'}]})\n",
        "#print(result)"
      ],
      "metadata": {
        "id": "iKVq1h3jfdap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "987QsT47jjIH"
      },
      "source": [
        "## Self Reflection & Evaluation Agent\n",
        "Evaluates output quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "hKhXB6THg4Ua"
      },
      "outputs": [],
      "source": [
        "#Define Self Evaluation agent\n",
        "\n",
        "#Define prompt\n",
        "EVAL_PROMPT = \"\"\"\n",
        "You are an expert evaluator. Your primary job is to give feedback on the analysis below, NOT to overwrite or revise it.\n",
        "\n",
        "Instructions:\n",
        "\n",
        "- Always display the full analysis/summary input *exactly as received* at the start of your answer, clearly labeled.\n",
        "- Provide your commentary (improvement, completeness, feedback) **separately after the full input**.\n",
        "- If human feedback is supplied, include your response to it at the end **without changing the original summary**.\n",
        "\n",
        "FORMAT STRICTLY LIKE THIS:\n",
        "---\n",
        "Original Analysis:\n",
        "{input}\n",
        "\n",
        "Evaluator Commentary:\n",
        "[Your bullet points: Completeness, Succinctness, Accuracy, Clarity, Human Feedback summary, Suggestions, etc.]\n",
        "\n",
        "---\n",
        "\n",
        "Never rewrite or summarize the original analysis. Only provide clear, constructive evaluator commentary after reproducing the input in its original form.\n",
        "\n",
        "--- Human Feedback ---\n",
        "{human_feedback}\n",
        "\"\"\"\n",
        "\n",
        "#Define agent\n",
        "evaluator_agent = create_react_agent(\n",
        "    model=llm_openai,\n",
        "    tools = [],\n",
        "    prompt=EVAL_PROMPT\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOBnNHeJl3r-"
      },
      "source": [
        "## Optimization Loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Helper function to make printout look better\n",
        "def print_wrapped(text, width=80): #Limit width to 80 pixels\n",
        "    for line in text.splitlines():\n",
        "        print(textwrap.fill(line, width=width))"
      ],
      "metadata": {
        "id": "v3i3SKrv-EEm"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This function runs the agents in an optimization kind of loop, to iterate based on feedback\n",
        "def optimization_loop(tools_agent, evaluator_agent, user_input):\n",
        "    #Step 1: Run tools agent\n",
        "    print(\"Conducting research...\")\n",
        "    tools_output = tools_agent.invoke(user_input)\n",
        "    print(\"\\n--- Analysis Summary ---\")\n",
        "    #print(tools_output)\n",
        "    print_wrapped(str(tools_output))\n",
        "\n",
        "    #Step 2: Ask user for feedback\n",
        "    human_feedback = input(\"\\nPlease enter your feedback (areas to improve, missing info, corrections):\\n\")\n",
        "\n",
        "    #Step 3: Evaluate and revise summary using feedback\n",
        "    eval_payload = {\n",
        "        \"input\": str(tools_output),\n",
        "        \"human_feedback\": human_feedback\n",
        "    }\n",
        "    print(\"\\nRunning evaluator with feedback...\")\n",
        "    revised_output = evaluator_agent.invoke(eval_payload)\n",
        "    print(\"\\n--- Revised Summary ---\")\n",
        "    #print(revised_output)\n",
        "    print_wrapped(str(revised_output))\n",
        "\n",
        "    #Optional: Loop for more feedback\n",
        "    while True:\n",
        "        more = input(\"\\nWould you like to refine further? (y/n): \")\n",
        "        if more.lower().startswith(\"y\"):\n",
        "            human_feedback = input(\"Enter any further feedback:\\n\")\n",
        "            eval_payload[\"human_feedback\"] = human_feedback\n",
        "            revised_output = evaluator_agent.invoke(eval_payload)\n",
        "            print(\"\\n--- Revised Summary ---\")\n",
        "            print(revised_output)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    print(\"\\nWorkflow complete. Final output above.\")\n",
        "    return revised_output  #Return for pretty print\n"
      ],
      "metadata": {
        "id": "RSdS1bOg4dXt"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print final analysis on stock in a pretty format\n",
        "def print_final_analysis(agent_output, title=\"Final Analysis Summary\"):\n",
        "    \"\"\"\n",
        "    Pretty-prints agent outputs including markdown, string, dict, or LangChain message formats.\n",
        "    Displays a title, renders bullet points and headings, and handles line breaks gracefully.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"{title}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    #Helper to render markdown-style output for terminal\n",
        "    def render_markdown(md):\n",
        "        #Render headings\n",
        "        md = re.sub(r\"^### (.+)$\", r\"\\n=== \\1 ===\\n\", md, flags=re.MULTILINE)\n",
        "        md = re.sub(r\"^## (.+)$\", r\"\\n== \\1 ==\\n\", md, flags=re.MULTILINE)\n",
        "        md = re.sub(r\"^# (.+)$\", r\"\\n= \\1 =\\n\", md, flags=re.MULTILINE)\n",
        "        #Replace double newlines with single blank line for separation\n",
        "        md = re.sub(r\"\\n{3,}\", \"\\n\\n\", md)\n",
        "        #Print with blank lines between paragraphs/bullets\n",
        "        for para in md.split('\\n\\n'):\n",
        "            print(para.strip())\n",
        "            print()\n",
        "\n",
        "    #Handle string output\n",
        "    if isinstance(agent_output, str):\n",
        "        render_markdown(agent_output)\n",
        "        return\n",
        "\n",
        "    #Handle dict outputs (common in LangChain)\n",
        "    if isinstance(agent_output, dict):\n",
        "        #Look for 'messages' (LangChain) or 'output'\n",
        "        if \"messages\" in agent_output:\n",
        "            messages = agent_output[\"messages\"]\n",
        "            for idx, msg in enumerate(messages):\n",
        "                content = getattr(msg, \"content\", str(msg))\n",
        "                if len(messages) > 1:\n",
        "                    print(f\"Message {idx+1}:\")\n",
        "                render_markdown(content)\n",
        "        elif \"output\" in agent_output and isinstance(agent_output[\"output\"], str):\n",
        "            render_markdown(agent_output[\"output\"])\n",
        "        else:\n",
        "            #Generic dict pretty-print\n",
        "            for k, v in agent_output.items():\n",
        "                print(f\"{k}:\")\n",
        "                render_markdown(str(v))\n",
        "        return\n",
        "\n",
        "    #Handle LangChain AIMessage or other objects with .content\n",
        "    content = getattr(agent_output, \"content\", None)\n",
        "    if content:\n",
        "        render_markdown(content)\n",
        "        return\n",
        "\n",
        "    #Fallback if above doesn't work\n",
        "    print(agent_output)\n"
      ],
      "metadata": {
        "id": "uNGphX3X6heE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_CeRDyhjmUp"
      },
      "source": [
        "## Learning/Memory Agent\n",
        "Maintains memory across analysis runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "l_7UNagLGt60"
      },
      "outputs": [],
      "source": [
        "#MEMORY AGENT CELL 1\n",
        "#Session-scoped memory\n",
        "\n",
        "@dataclass\n",
        "class MemoryItem:\n",
        "    symbol: str\n",
        "    question: str\n",
        "    answer: str\n",
        "    created_at: str\n",
        "    meta: Dict[str, Any]\n",
        "\n",
        "class SessionMemory:\n",
        "    def __init__(self, max_items: int = 200, max_per_symbol: int = 10):\n",
        "        self._store: Dict[str, List[MemoryItem]] = {}\n",
        "        self.max_items = max_items\n",
        "        self.max_per_symbol = max_per_symbol\n",
        "\n",
        "    def remember(self, symbol: str, question: str, answer: str, **meta) -> None:\n",
        "        symbol = (symbol or \"GENERIC\").upper().strip()\n",
        "        item = MemoryItem(\n",
        "            symbol=symbol,\n",
        "            question=(question or \"\").strip(),\n",
        "            answer=(answer or \"\").strip(),\n",
        "            created_at=datetime.utcnow().isoformat(timespec=\"seconds\"),\n",
        "            meta=meta or {}\n",
        "        )\n",
        "        bucket = self._store.setdefault(symbol, [])\n",
        "        bucket.append(item)\n",
        "        if len(bucket) > self.max_per_symbol:\n",
        "            del bucket[0 : len(bucket) - self.max_per_symbol]\n",
        "        self._cap_global()\n",
        "\n",
        "    def recall(self, symbol: str, question: Optional[str] = None) -> Optional[str]:\n",
        "        symbol = (symbol or \"GENERIC\").upper().strip()\n",
        "        bucket = self._store.get(symbol, [])\n",
        "        if not bucket:\n",
        "            return None\n",
        "        if not question:\n",
        "            return bucket[-1].answer\n",
        "        q = (question or \"\").strip()\n",
        "        for item in reversed(bucket):\n",
        "            if item.question == q:\n",
        "                return item.answer\n",
        "        return None\n",
        "\n",
        "    def latest(self, symbol: str) -> Optional[MemoryItem]:\n",
        "        symbol = (symbol or \"GENERIC\").upper().strip()\n",
        "        bucket = self._store.get(symbol, [])\n",
        "        return bucket[-1] if bucket else None\n",
        "\n",
        "    def _cap_global(self):\n",
        "        all_items = []\n",
        "        for sym, bucket in self._store.items():\n",
        "            for it in bucket:\n",
        "                all_items.append((it.created_at, sym, it))\n",
        "        if len(all_items) <= self.max_items:\n",
        "            return\n",
        "        all_items.sort(key=lambda x: x[0])  # oldest first\n",
        "        to_drop = len(all_items) - self.max_items\n",
        "        cutoff = set(id(it) for _, _, it in all_items[:to_drop])\n",
        "        for sym in list(self._store.keys()):\n",
        "            self._store[sym] = [it for it in self._store[sym] if id(it) not in cutoff]\n",
        "\n",
        "SESSION_MEMORY = SessionMemory()\n",
        "\n",
        "def extract_symbol(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Grab a likely ticker from the user_input like 'Analyze the SPY stock ticker'.\n",
        "    Simple heuristic: first ALL-CAPS token 1-5 chars (e.g., AAPL, MSFT, SPY).\n",
        "    Falls back to 'GENERIC' if none found.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"GENERIC\"\n",
        "    candidates = re.findall(r\"\\b[A-Z]{1,5}\\b\", text)\n",
        "    # Light filter for common English words\n",
        "    stop = {\"THE\",\"AND\",\"FOR\",\"WITH\",\"FROM\",\"THIS\",\"THAT\",\"YOUR\",\"HAVE\",\"HOLD\"}\n",
        "    for c in candidates:\n",
        "        if c not in stop:\n",
        "            return c\n",
        "    return \"GENERIC\"\n",
        "\n",
        "def as_text(x: Any) -> str:\n",
        "    \"\"\"\n",
        "    Normalize whatever comes back from planner/tools/evaluator/optimizer into a string.\n",
        "    Works with LangChain AgentExecutor outputs (dict), AIMessage, or raw str.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # AIMessage / ChatMessage\n",
        "        if hasattr(x, \"content\"):\n",
        "            return str(x.content)\n",
        "        # Agent-like dicts\n",
        "        if isinstance(x, dict):\n",
        "            if \"output\" in x and isinstance(x[\"output\"], str):\n",
        "                return x[\"output\"]\n",
        "            if \"messages\" in x and isinstance(x[\"messages\"], list):\n",
        "                return \"\\n\\n\".join(\n",
        "                    (m.content if hasattr(m, \"content\") else str(m))\n",
        "                    for m in x[\"messages\"]\n",
        "                )\n",
        "        # plain string\n",
        "        if isinstance(x, str):\n",
        "            return x\n",
        "        return str(x)\n",
        "    except Exception:\n",
        "        return str(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPxekd7Rj2KW"
      },
      "source": [
        "## Execution Function, Set The Agent Free"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Execution function\n",
        "final_analysis = optimization_loop(tools_agent, evaluator_agent, context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRPTUlph4xdI",
        "outputId": "9c188919-ceca-4ce1-cd21-fcbc983ba902"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conducting research...\n",
            "\u001b[1m[updates]\u001b[0m {'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_wnk9j9e2ZcoFdxzU7V9PWsgU', 'function': {'arguments': '{\"__arg1\": \"AAPL\"}', 'name': 'YahooFinanceAPI'}, 'type': 'function'}, {'id': 'call_Qf7xDqQJySsFIBphNWL8LkqJ', 'function': {'arguments': '{\"__arg1\": \"AAPL\"}', 'name': 'SECEDGARAPI'}, 'type': 'function'}, {'id': 'call_f1Dpdvs8c6u8acFTpK2QFUKu', 'function': {'arguments': '{\"__arg1\": \"AAPL\"}', 'name': 'NewsAPI'}, 'type': 'function'}, {'id': 'call_ZSHt7loDaFeepoXMj2JySGFv', 'function': {'arguments': '{\"__arg1\": \"Apple Inc.\"}', 'name': 'WikipediaSearch'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 682, 'total_tokens': 773, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-COGu7D6muPu8mzMU9qFc85NCZlkI4', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--d5aa1a60-2a12-43ff-8451-0e1f117b2feb-0', tool_calls=[{'name': 'YahooFinanceAPI', 'args': {'__arg1': 'AAPL'}, 'id': 'call_wnk9j9e2ZcoFdxzU7V9PWsgU', 'type': 'tool_call'}, {'name': 'SECEDGARAPI', 'args': {'__arg1': 'AAPL'}, 'id': 'call_Qf7xDqQJySsFIBphNWL8LkqJ', 'type': 'tool_call'}, {'name': 'NewsAPI', 'args': {'__arg1': 'AAPL'}, 'id': 'call_f1Dpdvs8c6u8acFTpK2QFUKu', 'type': 'tool_call'}, {'name': 'WikipediaSearch', 'args': {'__arg1': 'Apple Inc.'}, 'id': 'call_ZSHt7loDaFeepoXMj2JySGFv', 'type': 'tool_call'}], usage_metadata={'input_tokens': 682, 'output_tokens': 91, 'total_tokens': 773, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
            "\u001b[1m[values]\u001b[0m {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_wnk9j9e2ZcoFdxzU7V9PWsgU', 'function': {'arguments': '{\"__arg1\": \"AAPL\"}', 'name': 'YahooFinanceAPI'}, 'type': 'function'}, {'id': 'call_Qf7xDqQJySsFIBphNWL8LkqJ', 'function': {'arguments': '{\"__arg1\": \"AAPL\"}', 'name': 'SECEDGARAPI'}, 'type': 'function'}, {'id': 'call_f1Dpdvs8c6u8acFTpK2QFUKu', 'function': {'arguments': '{\"__arg1\": \"AAPL\"}', 'name': 'NewsAPI'}, 'type': 'function'}, {'id': 'call_ZSHt7loDaFeepoXMj2JySGFv', 'function': {'arguments': '{\"__arg1\": \"Apple Inc.\"}', 'name': 'WikipediaSearch'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 682, 'total_tokens': 773, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-COGu7D6muPu8mzMU9qFc85NCZlkI4', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--d5aa1a60-2a12-43ff-8451-0e1f117b2feb-0', tool_calls=[{'name': 'YahooFinanceAPI', 'args': {'__arg1': 'AAPL'}, 'id': 'call_wnk9j9e2ZcoFdxzU7V9PWsgU', 'type': 'tool_call'}, {'name': 'SECEDGARAPI', 'args': {'__arg1': 'AAPL'}, 'id': 'call_Qf7xDqQJySsFIBphNWL8LkqJ', 'type': 'tool_call'}, {'name': 'NewsAPI', 'args': {'__arg1': 'AAPL'}, 'id': 'call_f1Dpdvs8c6u8acFTpK2QFUKu', 'type': 'tool_call'}, {'name': 'WikipediaSearch', 'args': {'__arg1': 'Apple Inc.'}, 'id': 'call_ZSHt7loDaFeepoXMj2JySGFv', 'type': 'tool_call'}], usage_metadata={'input_tokens': 682, 'output_tokens': 91, 'total_tokens': 773, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
            "cik_resp: 200, {\"0\":{\"cik_str\":1045810,\"ticker\":\"NVDA\",\"title\":\"NVIDIA CORP\"},\"1\":{\"cik_str\":789019,\"ticker\":\"MSFT\"\n",
            "\u001b[1m[updates]\u001b[0m {'tools': {'messages': [ToolMessage(content='Latest filings for AAPL:\\n4 on 2025-10-03: xslF345X05/wk-form4_1759530830.xml\\n4 on 2025-10-03: xslF345X05/wk-form4_1759530744.xml\\n4 on 2025-10-03: xslF345X05/wk-form4_1759530690.xml', name='SECEDGARAPI', tool_call_id='call_Qf7xDqQJySsFIBphNWL8LkqJ')]}}\n",
            "\u001b[1m[updates]\u001b[0m {'tools': {'messages': [ToolMessage(content='AAPL price is 256.48', name='YahooFinanceAPI', tool_call_id='call_wnk9j9e2ZcoFdxzU7V9PWsgU')]}}\n",
            "\u001b[1m[updates]\u001b[0m {'tools': {'messages': [ToolMessage(content='NVIDIA, Palantir Technologies, Apple, Meta Platforms, Microsoft, Alphabet, and Advanced Micro Devices are the seven Technology stocks to watch today, according to MarketBeat’s stock screener tool. Technology stocks are shares of publicly traded companies whos…\\nAmazonプライム感謝祭セールで、Elgatoのカスタムキーデバイス「Stream Deck Scissor Keys」やLEDパネルライト「Key Light Neo」などがタイムセールとなっています。詳細は以下か […]\\nThe post Amazonプライム感謝祭セールで、Elgatoのカスタムキーデバイス「Stream Deck Scissor Keys」やLEDパネルライト「Key Light Neo」などがタイムセール中。 first appeared on AAPL Ch..\\n米SpigenがiMac G3とPower Mac G4 CubeをイメージしたM4チップ搭載のMac mini用スタンドケース「Mac mini Case Stand Classic C1」を発売しています。詳細は以 […]\\nThe post 米Spigen、iMac G3とPower Mac G4 CubeをイメージしたM4チップ搭載のMac mini用スタンドケース「Mac mini Case Stand Classic C1」を発売。 first appeared on AAPL Ch..', name='NewsAPI', tool_call_id='call_f1Dpdvs8c6u8acFTpK2QFUKu')]}}\n",
            "\u001b[1m[updates]\u001b[0m {'tools': {'messages': [ToolMessage(content=\"I couldn't find any information on that.\", name='WikipediaSearch', tool_call_id='call_ZSHt7loDaFeepoXMj2JySGFv')]}}\n",
            "\u001b[1m[values]\u001b[0m {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_wnk9j9e2ZcoFdxzU7V9PWsgU', 'function': {'arguments': '{\"__arg1\": \"AAPL\"}', 'name': 'YahooFinanceAPI'}, 'type': 'function'}, {'id': 'call_Qf7xDqQJySsFIBphNWL8LkqJ', 'function': {'arguments': '{\"__arg1\": \"AAPL\"}', 'name': 'SECEDGARAPI'}, 'type': 'function'}, {'id': 'call_f1Dpdvs8c6u8acFTpK2QFUKu', 'function': {'arguments': '{\"__arg1\": \"AAPL\"}', 'name': 'NewsAPI'}, 'type': 'function'}, {'id': 'call_ZSHt7loDaFeepoXMj2JySGFv', 'function': {'arguments': '{\"__arg1\": \"Apple Inc.\"}', 'name': 'WikipediaSearch'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 682, 'total_tokens': 773, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-COGu7D6muPu8mzMU9qFc85NCZlkI4', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--d5aa1a60-2a12-43ff-8451-0e1f117b2feb-0', tool_calls=[{'name': 'YahooFinanceAPI', 'args': {'__arg1': 'AAPL'}, 'id': 'call_wnk9j9e2ZcoFdxzU7V9PWsgU', 'type': 'tool_call'}, {'name': 'SECEDGARAPI', 'args': {'__arg1': 'AAPL'}, 'id': 'call_Qf7xDqQJySsFIBphNWL8LkqJ', 'type': 'tool_call'}, {'name': 'NewsAPI', 'args': {'__arg1': 'AAPL'}, 'id': 'call_f1Dpdvs8c6u8acFTpK2QFUKu', 'type': 'tool_call'}, {'name': 'WikipediaSearch', 'args': {'__arg1': 'Apple Inc.'}, 'id': 'call_ZSHt7loDaFeepoXMj2JySGFv', 'type': 'tool_call'}], usage_metadata={'input_tokens': 682, 'output_tokens': 91, 'total_tokens': 773, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='AAPL price is 256.48', name='YahooFinanceAPI', id='b244f2d7-9518-4047-aeab-8034e783dd9a', tool_call_id='call_wnk9j9e2ZcoFdxzU7V9PWsgU'), ToolMessage(content='Latest filings for AAPL:\\n4 on 2025-10-03: xslF345X05/wk-form4_1759530830.xml\\n4 on 2025-10-03: xslF345X05/wk-form4_1759530744.xml\\n4 on 2025-10-03: xslF345X05/wk-form4_1759530690.xml', name='SECEDGARAPI', id='203c2f4d-aba6-474f-98f6-d3411a067100', tool_call_id='call_Qf7xDqQJySsFIBphNWL8LkqJ'), ToolMessage(content='NVIDIA, Palantir Technologies, Apple, Meta Platforms, Microsoft, Alphabet, and Advanced Micro Devices are the seven Technology stocks to watch today, according to MarketBeat’s stock screener tool. Technology stocks are shares of publicly traded companies whos…\\nAmazonプライム感謝祭セールで、Elgatoのカスタムキーデバイス「Stream Deck Scissor Keys」やLEDパネルライト「Key Light Neo」などがタイムセールとなっています。詳細は以下か […]\\nThe post Amazonプライム感謝祭セールで、Elgatoのカスタムキーデバイス「Stream Deck Scissor Keys」やLEDパネルライト「Key Light Neo」などがタイムセール中。 first appeared on AAPL Ch..\\n米SpigenがiMac G3とPower Mac G4 CubeをイメージしたM4チップ搭載のMac mini用スタンドケース「Mac mini Case Stand Classic C1」を発売しています。詳細は以 […]\\nThe post 米Spigen、iMac G3とPower Mac G4 CubeをイメージしたM4チップ搭載のMac mini用スタンドケース「Mac mini Case Stand Classic C1」を発売。 first appeared on AAPL Ch..', name='NewsAPI', id='3491ad0f-b574-4b23-9cfc-7ec84fe1639b', tool_call_id='call_f1Dpdvs8c6u8acFTpK2QFUKu'), ToolMessage(content=\"I couldn't find any information on that.\", name='WikipediaSearch', id='38bb6392-e0a2-4dd8-8283-f132bbcdb7e9', tool_call_id='call_ZSHt7loDaFeepoXMj2JySGFv')]}\n",
            "\u001b[1m[updates]\u001b[0m {'agent': {'messages': [AIMessage(content='I have gathered some information on Apple Inc. (AAPL):\\n\\n1. **Company Overview**:\\n   - Stock Price: $256.48\\n   - Sector: Technology\\n\\n2. **Financial Data**:\\n   - Revenue, Profit, and Cash Flow: Not available in the initial response\\n\\n3. **SEC Filings**:\\n   - Latest filings for AAPL on 2025-10-03: \\n     - xslF345X05/wk-form4_1759530830.xml\\n     - xslF345X05/wk-form4_1759530744.xml\\n     - xslF345X05/wk-form4_1759530690.xml\\n\\n4. **Recent News**:\\n   - NVIDIA, Palantir Technologies, Apple, Meta Platforms, Microsoft, Alphabet, and Advanced Micro Devices are the seven Technology stocks to watch today.\\n   - Various product announcements and releases related to Amazon and Elgato.\\n   - No specific recent news related to Apple Inc.\\n\\n5. **Market Context**:\\n   - No specific market context information provided.\\n\\n6. **Peer Comparison**:\\n   - No peer comparison information provided.\\n\\n7. **Technical Analysis**:\\n   - No technical analysis information provided.\\n\\n8. **Analyst Sentiment**:\\n   - No analyst sentiment information provided.\\n\\nFor a more comprehensive analysis, additional tools and research may be required to gather financial data, market context, peer comparison, technical analysis, and analyst sentiment.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 301, 'prompt_tokens': 1637, 'total_tokens': 1938, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-COGu9IKi8EhPmgFIccnHYssFQgOcR', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--bd1c327b-76cf-46c0-af75-a4ff4dd94183-0', usage_metadata={'input_tokens': 1637, 'output_tokens': 301, 'total_tokens': 1938, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
            "\u001b[1m[values]\u001b[0m {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_wnk9j9e2ZcoFdxzU7V9PWsgU', 'function': {'arguments': '{\"__arg1\": \"AAPL\"}', 'name': 'YahooFinanceAPI'}, 'type': 'function'}, {'id': 'call_Qf7xDqQJySsFIBphNWL8LkqJ', 'function': {'arguments': '{\"__arg1\": \"AAPL\"}', 'name': 'SECEDGARAPI'}, 'type': 'function'}, {'id': 'call_f1Dpdvs8c6u8acFTpK2QFUKu', 'function': {'arguments': '{\"__arg1\": \"AAPL\"}', 'name': 'NewsAPI'}, 'type': 'function'}, {'id': 'call_ZSHt7loDaFeepoXMj2JySGFv', 'function': {'arguments': '{\"__arg1\": \"Apple Inc.\"}', 'name': 'WikipediaSearch'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 682, 'total_tokens': 773, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-COGu7D6muPu8mzMU9qFc85NCZlkI4', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--d5aa1a60-2a12-43ff-8451-0e1f117b2feb-0', tool_calls=[{'name': 'YahooFinanceAPI', 'args': {'__arg1': 'AAPL'}, 'id': 'call_wnk9j9e2ZcoFdxzU7V9PWsgU', 'type': 'tool_call'}, {'name': 'SECEDGARAPI', 'args': {'__arg1': 'AAPL'}, 'id': 'call_Qf7xDqQJySsFIBphNWL8LkqJ', 'type': 'tool_call'}, {'name': 'NewsAPI', 'args': {'__arg1': 'AAPL'}, 'id': 'call_f1Dpdvs8c6u8acFTpK2QFUKu', 'type': 'tool_call'}, {'name': 'WikipediaSearch', 'args': {'__arg1': 'Apple Inc.'}, 'id': 'call_ZSHt7loDaFeepoXMj2JySGFv', 'type': 'tool_call'}], usage_metadata={'input_tokens': 682, 'output_tokens': 91, 'total_tokens': 773, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='AAPL price is 256.48', name='YahooFinanceAPI', id='b244f2d7-9518-4047-aeab-8034e783dd9a', tool_call_id='call_wnk9j9e2ZcoFdxzU7V9PWsgU'), ToolMessage(content='Latest filings for AAPL:\\n4 on 2025-10-03: xslF345X05/wk-form4_1759530830.xml\\n4 on 2025-10-03: xslF345X05/wk-form4_1759530744.xml\\n4 on 2025-10-03: xslF345X05/wk-form4_1759530690.xml', name='SECEDGARAPI', id='203c2f4d-aba6-474f-98f6-d3411a067100', tool_call_id='call_Qf7xDqQJySsFIBphNWL8LkqJ'), ToolMessage(content='NVIDIA, Palantir Technologies, Apple, Meta Platforms, Microsoft, Alphabet, and Advanced Micro Devices are the seven Technology stocks to watch today, according to MarketBeat’s stock screener tool. Technology stocks are shares of publicly traded companies whos…\\nAmazonプライム感謝祭セールで、Elgatoのカスタムキーデバイス「Stream Deck Scissor Keys」やLEDパネルライト「Key Light Neo」などがタイムセールとなっています。詳細は以下か […]\\nThe post Amazonプライム感謝祭セールで、Elgatoのカスタムキーデバイス「Stream Deck Scissor Keys」やLEDパネルライト「Key Light Neo」などがタイムセール中。 first appeared on AAPL Ch..\\n米SpigenがiMac G3とPower Mac G4 CubeをイメージしたM4チップ搭載のMac mini用スタンドケース「Mac mini Case Stand Classic C1」を発売しています。詳細は以 […]\\nThe post 米Spigen、iMac G3とPower Mac G4 CubeをイメージしたM4チップ搭載のMac mini用スタンドケース「Mac mini Case Stand Classic C1」を発売。 first appeared on AAPL Ch..', name='NewsAPI', id='3491ad0f-b574-4b23-9cfc-7ec84fe1639b', tool_call_id='call_f1Dpdvs8c6u8acFTpK2QFUKu'), ToolMessage(content=\"I couldn't find any information on that.\", name='WikipediaSearch', id='38bb6392-e0a2-4dd8-8283-f132bbcdb7e9', tool_call_id='call_ZSHt7loDaFeepoXMj2JySGFv'), AIMessage(content='I have gathered some information on Apple Inc. (AAPL):\\n\\n1. **Company Overview**:\\n   - Stock Price: $256.48\\n   - Sector: Technology\\n\\n2. **Financial Data**:\\n   - Revenue, Profit, and Cash Flow: Not available in the initial response\\n\\n3. **SEC Filings**:\\n   - Latest filings for AAPL on 2025-10-03: \\n     - xslF345X05/wk-form4_1759530830.xml\\n     - xslF345X05/wk-form4_1759530744.xml\\n     - xslF345X05/wk-form4_1759530690.xml\\n\\n4. **Recent News**:\\n   - NVIDIA, Palantir Technologies, Apple, Meta Platforms, Microsoft, Alphabet, and Advanced Micro Devices are the seven Technology stocks to watch today.\\n   - Various product announcements and releases related to Amazon and Elgato.\\n   - No specific recent news related to Apple Inc.\\n\\n5. **Market Context**:\\n   - No specific market context information provided.\\n\\n6. **Peer Comparison**:\\n   - No peer comparison information provided.\\n\\n7. **Technical Analysis**:\\n   - No technical analysis information provided.\\n\\n8. **Analyst Sentiment**:\\n   - No analyst sentiment information provided.\\n\\nFor a more comprehensive analysis, additional tools and research may be required to gather financial data, market context, peer comparison, technical analysis, and analyst sentiment.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 301, 'prompt_tokens': 1637, 'total_tokens': 1938, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-COGu9IKi8EhPmgFIccnHYssFQgOcR', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--bd1c327b-76cf-46c0-af75-a4ff4dd94183-0', usage_metadata={'input_tokens': 1637, 'output_tokens': 301, 'total_tokens': 1938, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
            "\n",
            "--- Analysis Summary ---\n",
            "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id':\n",
            "'call_wnk9j9e2ZcoFdxzU7V9PWsgU', 'function': {'arguments': '{\"__arg1\": \"AAPL\"}',\n",
            "'name': 'YahooFinanceAPI'}, 'type': 'function'}, {'id':\n",
            "'call_Qf7xDqQJySsFIBphNWL8LkqJ', 'function': {'arguments': '{\"__arg1\": \"AAPL\"}',\n",
            "'name': 'SECEDGARAPI'}, 'type': 'function'}, {'id':\n",
            "'call_f1Dpdvs8c6u8acFTpK2QFUKu', 'function': {'arguments': '{\"__arg1\": \"AAPL\"}',\n",
            "'name': 'NewsAPI'}, 'type': 'function'}, {'id': 'call_ZSHt7loDaFeepoXMj2JySGFv',\n",
            "'function': {'arguments': '{\"__arg1\": \"Apple Inc.\"}', 'name':\n",
            "'WikipediaSearch'}, 'type': 'function'}], 'refusal': None},\n",
            "response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens':\n",
            "682, 'total_tokens': 773, 'completion_tokens_details':\n",
            "{'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0,\n",
            "'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0,\n",
            "'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint':\n",
            "None, 'id': 'chatcmpl-COGu7D6muPu8mzMU9qFc85NCZlkI4', 'service_tier': 'default',\n",
            "'finish_reason': 'tool_calls', 'logprobs': None}, id='run--\n",
            "d5aa1a60-2a12-43ff-8451-0e1f117b2feb-0', tool_calls=[{'name': 'YahooFinanceAPI',\n",
            "'args': {'__arg1': 'AAPL'}, 'id': 'call_wnk9j9e2ZcoFdxzU7V9PWsgU', 'type':\n",
            "'tool_call'}, {'name': 'SECEDGARAPI', 'args': {'__arg1': 'AAPL'}, 'id':\n",
            "'call_Qf7xDqQJySsFIBphNWL8LkqJ', 'type': 'tool_call'}, {'name': 'NewsAPI',\n",
            "'args': {'__arg1': 'AAPL'}, 'id': 'call_f1Dpdvs8c6u8acFTpK2QFUKu', 'type':\n",
            "'tool_call'}, {'name': 'WikipediaSearch', 'args': {'__arg1': 'Apple Inc.'},\n",
            "'id': 'call_ZSHt7loDaFeepoXMj2JySGFv', 'type': 'tool_call'}],\n",
            "usage_metadata={'input_tokens': 682, 'output_tokens': 91, 'total_tokens': 773,\n",
            "'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details':\n",
            "{'audio': 0, 'reasoning': 0}}), ToolMessage(content='AAPL price is 256.48',\n",
            "name='YahooFinanceAPI', id='b244f2d7-9518-4047-aeab-8034e783dd9a',\n",
            "tool_call_id='call_wnk9j9e2ZcoFdxzU7V9PWsgU'), ToolMessage(content='Latest\n",
            "filings for AAPL:\\n4 on 2025-10-03: xslF345X05/wk-form4_1759530830.xml\\n4 on\n",
            "2025-10-03: xslF345X05/wk-form4_1759530744.xml\\n4 on 2025-10-03: xslF345X05/wk-\n",
            "form4_1759530690.xml', name='SECEDGARAPI',\n",
            "id='203c2f4d-aba6-474f-98f6-d3411a067100',\n",
            "tool_call_id='call_Qf7xDqQJySsFIBphNWL8LkqJ'), ToolMessage(content='NVIDIA,\n",
            "Palantir Technologies, Apple, Meta Platforms, Microsoft, Alphabet, and Advanced\n",
            "Micro Devices are the seven Technology stocks to watch today, according to\n",
            "MarketBeat’s stock screener tool. Technology stocks are shares of publicly\n",
            "traded companies whos…\\nAmazonプライム感謝祭セールで、Elgatoのカスタムキーデバイス「Stream Deck Scissor\n",
            "Keys」やLEDパネルライト「Key Light Neo」などがタイムセールとなっています。詳細は以下か […]\\nThe post\n",
            "Amazonプライム感謝祭セールで、Elgatoのカスタムキーデバイス「Stream Deck Scissor Keys」やLEDパネルライト「Key\n",
            "Light Neo」などがタイムセール中。 first appeared on AAPL Ch..\\n米SpigenがiMac G3とPower Mac G4\n",
            "CubeをイメージしたM4チップ搭載のMac mini用スタンドケース「Mac mini Case Stand Classic C1」を発売しています。詳細は以\n",
            "[…]\\nThe post 米Spigen、iMac G3とPower Mac G4 CubeをイメージしたM4チップ搭載のMac\n",
            "mini用スタンドケース「Mac mini Case Stand Classic C1」を発売。 first appeared on AAPL Ch..',\n",
            "name='NewsAPI', id='3491ad0f-b574-4b23-9cfc-7ec84fe1639b',\n",
            "tool_call_id='call_f1Dpdvs8c6u8acFTpK2QFUKu'), ToolMessage(content=\"I couldn't\n",
            "find any information on that.\", name='WikipediaSearch',\n",
            "id='38bb6392-e0a2-4dd8-8283-f132bbcdb7e9',\n",
            "tool_call_id='call_ZSHt7loDaFeepoXMj2JySGFv'), AIMessage(content='I have\n",
            "gathered some information on Apple Inc. (AAPL):\\n\\n1. **Company Overview**:\\n\n",
            "- Stock Price: $256.48\\n   - Sector: Technology\\n\\n2. **Financial Data**:\\n   -\n",
            "Revenue, Profit, and Cash Flow: Not available in the initial response\\n\\n3.\n",
            "**SEC Filings**:\\n   - Latest filings for AAPL on 2025-10-03: \\n     -\n",
            "xslF345X05/wk-form4_1759530830.xml\\n     - xslF345X05/wk-form4_1759530744.xml\\n\n",
            "- xslF345X05/wk-form4_1759530690.xml\\n\\n4. **Recent News**:\\n   - NVIDIA,\n",
            "Palantir Technologies, Apple, Meta Platforms, Microsoft, Alphabet, and Advanced\n",
            "Micro Devices are the seven Technology stocks to watch today.\\n   - Various\n",
            "product announcements and releases related to Amazon and Elgato.\\n   - No\n",
            "specific recent news related to Apple Inc.\\n\\n5. **Market Context**:\\n   - No\n",
            "specific market context information provided.\\n\\n6. **Peer Comparison**:\\n   -\n",
            "No peer comparison information provided.\\n\\n7. **Technical Analysis**:\\n   - No\n",
            "technical analysis information provided.\\n\\n8. **Analyst Sentiment**:\\n   - No\n",
            "analyst sentiment information provided.\\n\\nFor a more comprehensive analysis,\n",
            "additional tools and research may be required to gather financial data, market\n",
            "context, peer comparison, technical analysis, and analyst sentiment.',\n",
            "additional_kwargs={'refusal': None}, response_metadata={'token_usage':\n",
            "{'completion_tokens': 301, 'prompt_tokens': 1637, 'total_tokens': 1938,\n",
            "'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens':\n",
            "0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0},\n",
            "'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name':\n",
            "'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-\n",
            "COGu9IKi8EhPmgFIccnHYssFQgOcR', 'service_tier': 'default', 'finish_reason':\n",
            "'stop', 'logprobs': None}, id='run--bd1c327b-76cf-46c0-af75-a4ff4dd94183-0',\n",
            "usage_metadata={'input_tokens': 1637, 'output_tokens': 301, 'total_tokens':\n",
            "1938, 'input_token_details': {'audio': 0, 'cache_read': 0},\n",
            "'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
            "\n",
            "Please enter your feedback (areas to improve, missing info, corrections):\n",
            "This looks good!\n",
            "\n",
            "Running evaluator with feedback...\n",
            "\n",
            "--- Revised Summary ---\n",
            "{'messages': [AIMessage(content='\\nOriginal Analysis:\\nThe data shows that there\n",
            "is a positive correlation between the number of hours spent studying and exam\n",
            "scores. This is evident from the scatter plot where students who studied more\n",
            "hours tended to score higher on exams. The correlation coefficient between study\n",
            "hours and exam scores is 0.75, indicating a strong positive correlation between\n",
            "the two variables.\\n\\nEvaluator Commentary:\\n- The analysis is clear and\n",
            "concise, providing a summary of the relationship between study hours and exam\n",
            "scores.\\n- It accurately mentions the positive correlation between study hours\n",
            "and exam scores, supported by the correlation coefficient of 0.75.\\n- It would\n",
            "be beneficial to include additional statistical tests or regression analysis to\n",
            "further strengthen the findings and provide more in-depth insights into the\n",
            "relationship.\\n- Providing information on the sample size and any potential\n",
            "limitations in the data collection process would enhance the\n",
            "analysis.\\n\\n---\\n\\nHuman Feedback:\\nThe analysis could benefit from including\n",
            "the impact of other factors on exam scores, such as sleep quality and study\n",
            "environment.\\n', additional_kwargs={'refusal': None},\n",
            "response_metadata={'token_usage': {'completion_tokens': 194, 'prompt_tokens':\n",
            "181, 'total_tokens': 375, 'completion_tokens_details':\n",
            "{'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0,\n",
            "'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0,\n",
            "'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint':\n",
            "None, 'id': 'chatcmpl-COGujodaLxpv3lLBtZj05z0gcyDzW', 'service_tier': 'default',\n",
            "'finish_reason': 'stop', 'logprobs': None}, id='run--\n",
            "08f600a9-55b4-4b5f-843e-0dbf46a7db2d-0', usage_metadata={'input_tokens': 181,\n",
            "'output_tokens': 194, 'total_tokens': 375, 'input_token_details': {'audio': 0,\n",
            "'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
            "\n",
            "Would you like to refine further? (y/n): n\n",
            "\n",
            "Workflow complete. Final output above.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_final_analysis(final_analysis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpCJ-A-76-FD",
        "outputId": "a905b2a7-0948-4830-858f-22dbecd8587b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Final Analysis Summary\n",
            "============================================================\n",
            "Original Analysis:\n",
            "The data shows that there is a positive correlation between the number of hours spent studying and exam scores. This is evident from the scatter plot where students who studied more hours tended to score higher on exams. The correlation coefficient between study hours and exam scores is 0.75, indicating a strong positive correlation between the two variables.\n",
            "\n",
            "Evaluator Commentary:\n",
            "- The analysis is clear and concise, providing a summary of the relationship between study hours and exam scores.\n",
            "- It accurately mentions the positive correlation between study hours and exam scores, supported by the correlation coefficient of 0.75.\n",
            "- It would be beneficial to include additional statistical tests or regression analysis to further strengthen the findings and provide more in-depth insights into the relationship.\n",
            "- Providing information on the sample size and any potential limitations in the data collection process would enhance the analysis.\n",
            "\n",
            "---\n",
            "\n",
            "Human Feedback:\n",
            "The analysis could benefit from including the impact of other factors on exam scores, such as sleep quality and study environment.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ensure the memory is working as intended\n",
        "#After running the user query through your whole pipeline:\n",
        "user_question = user_input\n",
        "symbol = extract_symbol(user_input)\n",
        "final_answer = as_text(final_analysis)   # Use your utility function\n",
        "\n",
        "SESSION_MEMORY.remember(symbol, user_question, final_answer)\n",
        "\n",
        "# Later, you can recall the latest answer for \"AAPL\":\n",
        "prev = SESSION_MEMORY.recall(\"AAPL\")\n",
        "if prev:\n",
        "    print(\"Previous answer for AAPL:\", prev)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTu73fn72wD3",
        "outputId": "091c5792-4bd8-49f0-8e9e-ed6a0913475e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Previous answer for AAPL: Original Analysis:\n",
            "The data shows that there is a positive correlation between the number of hours spent studying and exam scores. This is evident from the scatter plot where students who studied more hours tended to score higher on exams. The correlation coefficient between study hours and exam scores is 0.75, indicating a strong positive correlation between the two variables.\n",
            "\n",
            "Evaluator Commentary:\n",
            "- The analysis is clear and concise, providing a summary of the relationship between study hours and exam scores.\n",
            "- It accurately mentions the positive correlation between study hours and exam scores, supported by the correlation coefficient of 0.75.\n",
            "- It would be beneficial to include additional statistical tests or regression analysis to further strengthen the findings and provide more in-depth insights into the relationship.\n",
            "- Providing information on the sample size and any potential limitations in the data collection process would enhance the analysis.\n",
            "\n",
            "---\n",
            "\n",
            "Human Feedback:\n",
            "The analysis could benefit from including the impact of other factors on exam scores, such as sleep quality and study environment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3075633432.py:24: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  created_at=datetime.utcnow().isoformat(timespec=\"seconds\"),\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "n_CeRDyhjmUp"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}